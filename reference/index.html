
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Google Cloud Vertex AI SDK.">
      
      
      
        <link rel="canonical" href="https://github.com/googleapis/python-aiplatform/reference/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.25">
    
    
      
        <title>Reference - Vertex SDK</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Vertex SDK" class="md-header__button md-logo" aria-label="Vertex SDK" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Vertex SDK
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Reference
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/googleapis/python-aiplatform" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    googleapis/python-aiplatform
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Overview

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../vertexai/generative_models/" class="md-tabs__link">
        
  
    
  
  Generative models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../vertexai/language_models/" class="md-tabs__link">
        
  
    
  
  Language models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../vertexai/vision_models/" class="md-tabs__link">
        
  
    
  
  Vision models

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Vertex SDK" class="md-nav__button md-logo" aria-label="Vertex SDK" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Vertex SDK
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/googleapis/python-aiplatform" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    googleapis/python-aiplatform
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../vertexai/generative_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generative models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../vertexai/language_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../vertexai/vision_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision models
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models" class="md-nav__link">
    <span class="md-ellipsis">
      generative_models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.Candidate" class="md-nav__link">
    <span class="md-ellipsis">
      Candidate
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.ChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      ChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.ChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.ChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.Content" class="md-nav__link">
    <span class="md-ellipsis">
      Content
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.FunctionDeclaration" class="md-nav__link">
    <span class="md-ellipsis">
      FunctionDeclaration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FunctionDeclaration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.FunctionDeclaration.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.GenerationConfig" class="md-nav__link">
    <span class="md-ellipsis">
      GenerationConfig
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GenerationConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.GenerationConfig.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.GenerationResponse" class="md-nav__link">
    <span class="md-ellipsis">
      GenerationResponse
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.Image" class="md-nav__link">
    <span class="md-ellipsis">
      Image
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Image">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.Image.data" class="md-nav__link">
    <span class="md-ellipsis">
      data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.Image.from_bytes" class="md-nav__link">
    <span class="md-ellipsis">
      from_bytes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.Image.load_from_file" class="md-nav__link">
    <span class="md-ellipsis">
      load_from_file
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.Part" class="md-nav__link">
    <span class="md-ellipsis">
      Part
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.SafetySetting" class="md-nav__link">
    <span class="md-ellipsis">
      SafetySetting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SafetySetting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.SafetySetting.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.Tool" class="md-nav__link">
    <span class="md-ellipsis">
      Tool
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.generative_models.grounding" class="md-nav__link">
    <span class="md-ellipsis">
      grounding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="grounding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.grounding.GoogleSearchRetrieval" class="md-nav__link">
    <span class="md-ellipsis">
      GoogleSearchRetrieval
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GoogleSearchRetrieval">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.generative_models.grounding.GoogleSearchRetrieval.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models" class="md-nav__link">
    <span class="md-ellipsis">
      language_models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage" class="md-nav__link">
    <span class="md-ellipsis">
      ChatMessage
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      ChatModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      ChatSession
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.start_chat" class="md-nav__link">
    <span class="md-ellipsis">
      start_chat
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair" class="md-nav__link">
    <span class="md-ellipsis">
      InputOutputTextPair
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbedding
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbeddingInput
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse" class="md-nav__link">
    <span class="md-ellipsis">
      TextGenerationResponse
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextGenerationResponse">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="md-nav__link">
    <span class="md-ellipsis">
      raw_prediction_response
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="reference">Reference</h1>


<div class="doc doc-object doc-module">



<a id="vertexai.generative_models"></a>
    <div class="doc doc-contents first">

      <p>Classes for working with the Gemini models.</p>



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.Candidate" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Candidate</span>


</h2>


    <div class="doc doc-contents ">


      <p>A response candidate generated by the model.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span>
<span class="normal">1730</span>
<span class="normal">1731</span>
<span class="normal">1732</span>
<span class="normal">1733</span>
<span class="normal">1734</span>
<span class="normal">1735</span>
<span class="normal">1736</span>
<span class="normal">1737</span>
<span class="normal">1738</span>
<span class="normal">1739</span>
<span class="normal">1740</span>
<span class="normal">1741</span>
<span class="normal">1742</span>
<span class="normal">1743</span>
<span class="normal">1744</span>
<span class="normal">1745</span>
<span class="normal">1746</span>
<span class="normal">1747</span>
<span class="normal">1748</span>
<span class="normal">1749</span>
<span class="normal">1750</span>
<span class="normal">1751</span>
<span class="normal">1752</span>
<span class="normal">1753</span>
<span class="normal">1754</span>
<span class="normal">1755</span>
<span class="normal">1756</span>
<span class="normal">1757</span>
<span class="normal">1758</span>
<span class="normal">1759</span>
<span class="normal">1760</span>
<span class="normal">1761</span>
<span class="normal">1762</span>
<span class="normal">1763</span>
<span class="normal">1764</span>
<span class="normal">1765</span>
<span class="normal">1766</span>
<span class="normal">1767</span>
<span class="normal">1768</span>
<span class="normal">1769</span>
<span class="normal">1770</span>
<span class="normal">1771</span>
<span class="normal">1772</span>
<span class="normal">1773</span>
<span class="normal">1774</span>
<span class="normal">1775</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Candidate</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A response candidate generated by the model.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">raw_candidate</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Candidate</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span> <span class="o">=</span> <span class="n">raw_candidate</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_gapic</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">raw_candidate</span><span class="p">:</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Candidate</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Candidate&quot;</span><span class="p">:</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
        <span class="n">candidate</span><span class="o">.</span><span class="n">_raw_candidate</span> <span class="o">=</span> <span class="n">raw_candidate</span>
        <span class="k">return</span> <span class="n">candidate</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">candidate_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;Candidate&quot;</span><span class="p">:</span>
        <span class="n">raw_candidate</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Candidate</span><span class="p">()</span>
        <span class="n">json_format</span><span class="o">.</span><span class="n">ParseDict</span><span class="p">(</span><span class="n">candidate_dict</span><span class="p">,</span> <span class="n">raw_candidate</span><span class="o">.</span><span class="n">_pb</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_candidate</span><span class="o">=</span><span class="n">raw_candidate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">content</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Content&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Content</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">raw_content</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="n">content</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">finish_reason</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Candidate</span><span class="o">.</span><span class="n">FinishReason</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="n">finish_reason</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">finish_message</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="n">finish_message</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="n">index</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">safety_ratings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">gapic_content_types</span><span class="o">.</span><span class="n">SafetyRating</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="n">safety_ratings</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">citation_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">CitationMetadata</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="n">citation_metadata</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">grounding_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">GroundingMetadata</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_candidate</span><span class="o">.</span><span class="n">grounding_metadata</span>

    <span class="c1"># GenerationPart properties</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">text</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">text</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">AttributeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Enrich the error message with the whole Candidate.</span>
            <span class="c1"># The Content object does not have full information.</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot get the Candidate text.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;Candidate:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">_dict_to_pretty_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">function_calls</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">FunctionCall</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">content</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">parts</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">part</span><span class="o">.</span><span class="n">function_call</span>
            <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">parts</span>
            <span class="k">if</span> <span class="n">part</span> <span class="ow">and</span> <span class="n">part</span><span class="o">.</span><span class="n">function_call</span>
        <span class="p">]</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.ChatSession" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatSession</span>


</h2>


    <div class="doc doc-contents ">


      <p>Chat session holds the chat history.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 745</span>
<span class="normal"> 746</span>
<span class="normal"> 747</span>
<span class="normal"> 748</span>
<span class="normal"> 749</span>
<span class="normal"> 750</span>
<span class="normal"> 751</span>
<span class="normal"> 752</span>
<span class="normal"> 753</span>
<span class="normal"> 754</span>
<span class="normal"> 755</span>
<span class="normal"> 756</span>
<span class="normal"> 757</span>
<span class="normal"> 758</span>
<span class="normal"> 759</span>
<span class="normal"> 760</span>
<span class="normal"> 761</span>
<span class="normal"> 762</span>
<span class="normal"> 763</span>
<span class="normal"> 764</span>
<span class="normal"> 765</span>
<span class="normal"> 766</span>
<span class="normal"> 767</span>
<span class="normal"> 768</span>
<span class="normal"> 769</span>
<span class="normal"> 770</span>
<span class="normal"> 771</span>
<span class="normal"> 772</span>
<span class="normal"> 773</span>
<span class="normal"> 774</span>
<span class="normal"> 775</span>
<span class="normal"> 776</span>
<span class="normal"> 777</span>
<span class="normal"> 778</span>
<span class="normal"> 779</span>
<span class="normal"> 780</span>
<span class="normal"> 781</span>
<span class="normal"> 782</span>
<span class="normal"> 783</span>
<span class="normal"> 784</span>
<span class="normal"> 785</span>
<span class="normal"> 786</span>
<span class="normal"> 787</span>
<span class="normal"> 788</span>
<span class="normal"> 789</span>
<span class="normal"> 790</span>
<span class="normal"> 791</span>
<span class="normal"> 792</span>
<span class="normal"> 793</span>
<span class="normal"> 794</span>
<span class="normal"> 795</span>
<span class="normal"> 796</span>
<span class="normal"> 797</span>
<span class="normal"> 798</span>
<span class="normal"> 799</span>
<span class="normal"> 800</span>
<span class="normal"> 801</span>
<span class="normal"> 802</span>
<span class="normal"> 803</span>
<span class="normal"> 804</span>
<span class="normal"> 805</span>
<span class="normal"> 806</span>
<span class="normal"> 807</span>
<span class="normal"> 808</span>
<span class="normal"> 809</span>
<span class="normal"> 810</span>
<span class="normal"> 811</span>
<span class="normal"> 812</span>
<span class="normal"> 813</span>
<span class="normal"> 814</span>
<span class="normal"> 815</span>
<span class="normal"> 816</span>
<span class="normal"> 817</span>
<span class="normal"> 818</span>
<span class="normal"> 819</span>
<span class="normal"> 820</span>
<span class="normal"> 821</span>
<span class="normal"> 822</span>
<span class="normal"> 823</span>
<span class="normal"> 824</span>
<span class="normal"> 825</span>
<span class="normal"> 826</span>
<span class="normal"> 827</span>
<span class="normal"> 828</span>
<span class="normal"> 829</span>
<span class="normal"> 830</span>
<span class="normal"> 831</span>
<span class="normal"> 832</span>
<span class="normal"> 833</span>
<span class="normal"> 834</span>
<span class="normal"> 835</span>
<span class="normal"> 836</span>
<span class="normal"> 837</span>
<span class="normal"> 838</span>
<span class="normal"> 839</span>
<span class="normal"> 840</span>
<span class="normal"> 841</span>
<span class="normal"> 842</span>
<span class="normal"> 843</span>
<span class="normal"> 844</span>
<span class="normal"> 845</span>
<span class="normal"> 846</span>
<span class="normal"> 847</span>
<span class="normal"> 848</span>
<span class="normal"> 849</span>
<span class="normal"> 850</span>
<span class="normal"> 851</span>
<span class="normal"> 852</span>
<span class="normal"> 853</span>
<span class="normal"> 854</span>
<span class="normal"> 855</span>
<span class="normal"> 856</span>
<span class="normal"> 857</span>
<span class="normal"> 858</span>
<span class="normal"> 859</span>
<span class="normal"> 860</span>
<span class="normal"> 861</span>
<span class="normal"> 862</span>
<span class="normal"> 863</span>
<span class="normal"> 864</span>
<span class="normal"> 865</span>
<span class="normal"> 866</span>
<span class="normal"> 867</span>
<span class="normal"> 868</span>
<span class="normal"> 869</span>
<span class="normal"> 870</span>
<span class="normal"> 871</span>
<span class="normal"> 872</span>
<span class="normal"> 873</span>
<span class="normal"> 874</span>
<span class="normal"> 875</span>
<span class="normal"> 876</span>
<span class="normal"> 877</span>
<span class="normal"> 878</span>
<span class="normal"> 879</span>
<span class="normal"> 880</span>
<span class="normal"> 881</span>
<span class="normal"> 882</span>
<span class="normal"> 883</span>
<span class="normal"> 884</span>
<span class="normal"> 885</span>
<span class="normal"> 886</span>
<span class="normal"> 887</span>
<span class="normal"> 888</span>
<span class="normal"> 889</span>
<span class="normal"> 890</span>
<span class="normal"> 891</span>
<span class="normal"> 892</span>
<span class="normal"> 893</span>
<span class="normal"> 894</span>
<span class="normal"> 895</span>
<span class="normal"> 896</span>
<span class="normal"> 897</span>
<span class="normal"> 898</span>
<span class="normal"> 899</span>
<span class="normal"> 900</span>
<span class="normal"> 901</span>
<span class="normal"> 902</span>
<span class="normal"> 903</span>
<span class="normal"> 904</span>
<span class="normal"> 905</span>
<span class="normal"> 906</span>
<span class="normal"> 907</span>
<span class="normal"> 908</span>
<span class="normal"> 909</span>
<span class="normal"> 910</span>
<span class="normal"> 911</span>
<span class="normal"> 912</span>
<span class="normal"> 913</span>
<span class="normal"> 914</span>
<span class="normal"> 915</span>
<span class="normal"> 916</span>
<span class="normal"> 917</span>
<span class="normal"> 918</span>
<span class="normal"> 919</span>
<span class="normal"> 920</span>
<span class="normal"> 921</span>
<span class="normal"> 922</span>
<span class="normal"> 923</span>
<span class="normal"> 924</span>
<span class="normal"> 925</span>
<span class="normal"> 926</span>
<span class="normal"> 927</span>
<span class="normal"> 928</span>
<span class="normal"> 929</span>
<span class="normal"> 930</span>
<span class="normal"> 931</span>
<span class="normal"> 932</span>
<span class="normal"> 933</span>
<span class="normal"> 934</span>
<span class="normal"> 935</span>
<span class="normal"> 936</span>
<span class="normal"> 937</span>
<span class="normal"> 938</span>
<span class="normal"> 939</span>
<span class="normal"> 940</span>
<span class="normal"> 941</span>
<span class="normal"> 942</span>
<span class="normal"> 943</span>
<span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span>
<span class="normal">1108</span>
<span class="normal">1109</span>
<span class="normal">1110</span>
<span class="normal">1111</span>
<span class="normal">1112</span>
<span class="normal">1113</span>
<span class="normal">1114</span>
<span class="normal">1115</span>
<span class="normal">1116</span>
<span class="normal">1117</span>
<span class="normal">1118</span>
<span class="normal">1119</span>
<span class="normal">1120</span>
<span class="normal">1121</span>
<span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ChatSession</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Chat session holds the chat history.&quot;&quot;&quot;</span>

    <span class="n">_USER_ROLE</span> <span class="o">=</span> <span class="s2">&quot;user&quot;</span>
    <span class="n">_MODEL_ROLE</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">_GenerativeModel</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Content&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">response_validation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">history</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">Content</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">history</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;history must be a list of Content objects.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span> <span class="o">=</span> <span class="n">history</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span> <span class="o">=</span> <span class="n">_validate_response</span> <span class="k">if</span> <span class="n">response_validation</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="c1"># _responder is currently only set by PreviewChatSession</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_responder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;AutomaticFunctionCallingResponder&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">history</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;Content&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_history</span>

    <span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generates content.</span>

<span class="sd">        Args:</span>
<span class="sd">            content: Content to send to the model.</span>
<span class="sd">                Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">                Supports</span>
<span class="sd">                * str, Image, Part,</span>
<span class="sd">                * List[Union[str, Image, Part]],</span>
<span class="sd">            generation_config: Parameters for the generation.</span>
<span class="sd">            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">            tools: A list of tools (functions) that the model can try calling.</span>
<span class="sd">            stream: Whether to stream the response.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single GenerationResponse object if stream == False</span>
<span class="sd">            A stream of GenerationResponse objects if stream == True</span>

<span class="sd">        Raises:</span>
<span class="sd">            ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message_streaming</span><span class="p">(</span>
                <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message</span><span class="p">(</span>
                <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">Awaitable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">],</span>
        <span class="n">Awaitable</span><span class="p">[</span><span class="n">AsyncIterable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">]],</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generates content asynchronously.</span>

<span class="sd">        Args:</span>
<span class="sd">            content: Content to send to the model.</span>
<span class="sd">                Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">                Supports</span>
<span class="sd">                * str, Image, Part,</span>
<span class="sd">                * List[Union[str, Image, Part]],</span>
<span class="sd">            generation_config: Parameters for the generation.</span>
<span class="sd">            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">            tools: A list of tools (functions) that the model can try calling.</span>
<span class="sd">            stream: Whether to stream the response.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An awaitable for a single GenerationResponse object if stream == False</span>
<span class="sd">            An awaitable for a stream of GenerationResponse objects if stream == True</span>

<span class="sd">        Raises:</span>
<span class="sd">            ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message_streaming_async</span><span class="p">(</span>
                <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message_async</span><span class="p">(</span>
                <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_send_message</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;GenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generates content.</span>

<span class="sd">        Args:</span>
<span class="sd">            content: Content to send to the model.</span>
<span class="sd">                Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">                Supports</span>
<span class="sd">                * str, Image, Part,</span>
<span class="sd">                * List[Union[str, Image, Part]],</span>
<span class="sd">            generation_config: Parameters for the generation.</span>
<span class="sd">            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">            tools: A list of tools (functions) that the model can try calling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A single GenerationResponse object</span>

<span class="sd">        Raises:</span>
<span class="sd">            ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Preparing the message history to send</span>
        <span class="n">request_message</span> <span class="o">=</span> <span class="n">Content</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">_to_content</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_USER_ROLE</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">history_delta</span> <span class="o">=</span> <span class="p">[</span><span class="n">request_message</span><span class="p">]</span>

        <span class="n">message_responder</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_responder</span><span class="o">.</span><span class="n">_create_responder_for_message</span><span class="p">(</span>
                <span class="n">tools</span><span class="o">=</span><span class="n">tools</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_tools</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_responder</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">request_history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_history</span> <span class="o">+</span> <span class="n">history_delta</span>
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_generate_content</span><span class="p">(</span>
                <span class="n">contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
                <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
                <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
                <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># By default we&#39;re not adding incomplete interactions to history.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span><span class="p">(</span>
                    <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                    <span class="n">request_contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
                    <span class="n">response_chunks</span><span class="o">=</span><span class="p">[</span><span class="n">response</span><span class="p">],</span>
                <span class="p">)</span>

            <span class="c1"># Adding the request and the first response candidate to history</span>
            <span class="n">response_message</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
            <span class="c1"># Response role is NOT set by the model.</span>
            <span class="n">response_message</span><span class="o">.</span><span class="n">role</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MODEL_ROLE</span>
            <span class="n">history_delta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response_message</span><span class="p">)</span>

            <span class="n">auto_responder_content</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">message_responder</span><span class="o">.</span><span class="n">respond_to_model_response</span><span class="p">(</span><span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">message_responder</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">auto_responder_content</span><span class="p">:</span>
                <span class="n">auto_responder_content</span><span class="o">.</span><span class="n">role</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_USER_ROLE</span>
                <span class="n">history_delta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">auto_responder_content</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">history_delta</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_send_message_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;GenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generates content asynchronously.</span>

<span class="sd">        Args:</span>
<span class="sd">            content: Content to send to the model.</span>
<span class="sd">                Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">                Supports</span>
<span class="sd">                * str, Image, Part,</span>
<span class="sd">                * List[Union[str, Image, Part]],</span>
<span class="sd">            generation_config: Parameters for the generation.</span>
<span class="sd">            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">            tools: A list of tools (functions) that the model can try calling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An awaitable for a single GenerationResponse object</span>

<span class="sd">        Raises:</span>
<span class="sd">            ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Preparing the message history to send</span>
        <span class="n">request_message</span> <span class="o">=</span> <span class="n">Content</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">_to_content</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_USER_ROLE</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">request_history</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="p">)</span>
        <span class="n">request_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">request_message</span><span class="p">)</span>

        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_generate_content_async</span><span class="p">(</span>
            <span class="n">contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># By default we&#39;re not adding incomplete interactions to history.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span><span class="p">(</span>
                <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="n">request_contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
                <span class="n">response_chunks</span><span class="o">=</span><span class="p">[</span><span class="n">response</span><span class="p">],</span>
            <span class="p">)</span>

        <span class="c1"># Adding the request and the first response candidate to history</span>
        <span class="n">response_message</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
        <span class="c1"># Response role is NOT set by the model.</span>
        <span class="n">response_message</span><span class="o">.</span><span class="n">role</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MODEL_ROLE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">request_message</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response_message</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="k">def</span> <span class="nf">_send_message_streaming</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generates content.</span>

<span class="sd">        Args:</span>
<span class="sd">            content: Content to send to the model.</span>
<span class="sd">                Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">                Supports</span>
<span class="sd">                * str, Image, Part,</span>
<span class="sd">                * List[Union[str, Image, Part]],</span>
<span class="sd">            generation_config: Parameters for the generation.</span>
<span class="sd">            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">            tools: A list of tools (functions) that the model can try calling.</span>

<span class="sd">        Yields:</span>
<span class="sd">            A stream of GenerationResponse objects</span>

<span class="sd">        Raises:</span>
<span class="sd">            ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Preparing the message history to send</span>
        <span class="n">request_message</span> <span class="o">=</span> <span class="n">Content</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">_to_content</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_USER_ROLE</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">request_history</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="p">)</span>
        <span class="n">request_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">request_message</span><span class="p">)</span>

        <span class="n">stream</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_generate_content_streaming</span><span class="p">(</span>
            <span class="n">contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">full_response</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
            <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
            <span class="c1"># By default we&#39;re not adding incomplete interactions to history.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span><span class="p">(</span>
                    <span class="n">response</span><span class="o">=</span><span class="n">chunk</span><span class="p">,</span>
                    <span class="n">request_contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
                    <span class="n">response_chunks</span><span class="o">=</span><span class="n">chunks</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">full_response</span><span class="p">:</span>
                <span class="n">_append_response</span><span class="p">(</span><span class="n">full_response</span><span class="p">,</span> <span class="n">chunk</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">full_response</span> <span class="o">=</span> <span class="n">chunk</span>
            <span class="k">yield</span> <span class="n">chunk</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">full_response</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Adding the request and the first response candidate to history</span>
        <span class="n">response_message</span> <span class="o">=</span> <span class="n">full_response</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
        <span class="c1"># Response role is NOT set by the model.</span>
        <span class="n">response_message</span><span class="o">.</span><span class="n">role</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MODEL_ROLE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">request_message</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response_message</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_send_message_streaming_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generates content asynchronously.</span>

<span class="sd">        Args:</span>
<span class="sd">            content: Content to send to the model.</span>
<span class="sd">                Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">                Supports</span>
<span class="sd">                * str, Image, Part,</span>
<span class="sd">                * List[Union[str, Image, Part]],</span>
<span class="sd">            generation_config: Parameters for the generation.</span>
<span class="sd">            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">            tools: A list of tools (functions) that the model can try calling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An awaitable for a stream of GenerationResponse objects</span>

<span class="sd">        Raises:</span>
<span class="sd">            ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Preparing the message history to send</span>
        <span class="n">request_message</span> <span class="o">=</span> <span class="n">Content</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">_to_content</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">content</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_USER_ROLE</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">request_history</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="p">)</span>
        <span class="n">request_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">request_message</span><span class="p">)</span>

        <span class="n">stream</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_generate_content_streaming_async</span><span class="p">(</span>
            <span class="n">contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">async</span> <span class="k">def</span> <span class="nf">async_generator</span><span class="p">():</span>
            <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">full_response</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
                <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
                <span class="c1"># By default we&#39;re not adding incomplete interactions to history.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_response_validator</span><span class="p">(</span>
                        <span class="n">response</span><span class="o">=</span><span class="n">chunk</span><span class="p">,</span>
                        <span class="n">request_contents</span><span class="o">=</span><span class="n">request_history</span><span class="p">,</span>
                        <span class="n">response_chunks</span><span class="o">=</span><span class="n">chunks</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">full_response</span><span class="p">:</span>
                    <span class="n">_append_response</span><span class="p">(</span><span class="n">full_response</span><span class="p">,</span> <span class="n">chunk</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">full_response</span> <span class="o">=</span> <span class="n">chunk</span>
                <span class="k">yield</span> <span class="n">chunk</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">full_response</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="c1"># Adding the request and the first response candidate to history</span>
            <span class="n">response_message</span> <span class="o">=</span> <span class="n">full_response</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
            <span class="c1"># Response role is NOT set by the model.</span>
            <span class="n">response_message</span><span class="o">.</span><span class="n">role</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_MODEL_ROLE</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">request_message</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response_message</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">async_generator</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.generative_models.ChatSession.send_message" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message</span><span class="p">(</span>
    <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">GenerationConfigType</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
    <span class="n">GenerationResponse</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="n">GenerationResponse</span><span class="p">]</span>
<span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Generates content.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>content</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Content to send to the model.
Supports a value that can be converted to a Part or a list of such values.
Supports
* str, Image, Part,
* List[Union[str, Image, Part]],</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="vertexai.generative_models._generative_models.PartsType">PartsType</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>generation_config</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Parameters for the generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.generative_models._generative_models.GenerationConfigType">GenerationConfigType</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safety_settings</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.generative_models._generative_models.SafetySettingsType">SafetySettingsType</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tools</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of tools (functions) that the model can try calling.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.Tool" href="../vertexai/generative_models/#vertexai.generative_models.Tool">Tool</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stream</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to stream the response.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>, <span title="typing.Iterable">Iterable</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>]]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A single GenerationResponse object if stream == False</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>, <span title="typing.Iterable">Iterable</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>]]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A stream of GenerationResponse objects if stream == True</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code><span title="vertexai.generative_models._generative_models.ResponseValidationError">ResponseValidationError</span></code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the response was blocked or is incomplete.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates content.</span>

<span class="sd">    Args:</span>
<span class="sd">        content: Content to send to the model.</span>
<span class="sd">            Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">            Supports</span>
<span class="sd">            * str, Image, Part,</span>
<span class="sd">            * List[Union[str, Image, Part]],</span>
<span class="sd">        generation_config: Parameters for the generation.</span>
<span class="sd">        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">        tools: A list of tools (functions) that the model can try calling.</span>
<span class="sd">        stream: Whether to stream the response.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A single GenerationResponse object if stream == False</span>
<span class="sd">        A stream of GenerationResponse objects if stream == True</span>

<span class="sd">    Raises:</span>
<span class="sd">        ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message_streaming</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.generative_models.ChatSession.send_message_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_async</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_async</span><span class="p">(</span>
    <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">GenerationConfigType</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
    <span class="n">Awaitable</span><span class="p">[</span><span class="n">GenerationResponse</span><span class="p">],</span>
    <span class="n">Awaitable</span><span class="p">[</span><span class="n">AsyncIterable</span><span class="p">[</span><span class="n">GenerationResponse</span><span class="p">]],</span>
<span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Generates content asynchronously.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>content</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Content to send to the model.
Supports a value that can be converted to a Part or a list of such values.
Supports
* str, Image, Part,
* List[Union[str, Image, Part]],</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="vertexai.generative_models._generative_models.PartsType">PartsType</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>generation_config</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Parameters for the generation.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.generative_models._generative_models.GenerationConfigType">GenerationConfigType</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>safety_settings</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.generative_models._generative_models.SafetySettingsType">SafetySettingsType</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tools</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of tools (functions) that the model can try calling.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.Tool" href="../vertexai/generative_models/#vertexai.generative_models.Tool">Tool</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stream</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to stream the response.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>False</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="typing.Awaitable">Awaitable</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>], <span title="typing.Awaitable">Awaitable</span>[<span title="typing.AsyncIterable">AsyncIterable</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>]]]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>An awaitable for a single GenerationResponse object if stream == False</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Union">Union</span>[<span title="typing.Awaitable">Awaitable</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>], <span title="typing.Awaitable">Awaitable</span>[<span title="typing.AsyncIterable">AsyncIterable</span>[<a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.GenerationResponse" href="../vertexai/generative_models/#vertexai.generative_models.GenerationResponse">GenerationResponse</a>]]]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>An awaitable for a stream of GenerationResponse objects if stream == True</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code><span title="vertexai.generative_models._generative_models.ResponseValidationError">ResponseValidationError</span></code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the response was blocked or is incomplete.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">content</span><span class="p">:</span> <span class="n">PartsType</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">generation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GenerationConfigType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">safety_settings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySettingsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tools</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="s2">&quot;Tool&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stream</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
    <span class="n">Awaitable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">],</span>
    <span class="n">Awaitable</span><span class="p">[</span><span class="n">AsyncIterable</span><span class="p">[</span><span class="s2">&quot;GenerationResponse&quot;</span><span class="p">]],</span>
<span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates content asynchronously.</span>

<span class="sd">    Args:</span>
<span class="sd">        content: Content to send to the model.</span>
<span class="sd">            Supports a value that can be converted to a Part or a list of such values.</span>
<span class="sd">            Supports</span>
<span class="sd">            * str, Image, Part,</span>
<span class="sd">            * List[Union[str, Image, Part]],</span>
<span class="sd">        generation_config: Parameters for the generation.</span>
<span class="sd">        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</span>
<span class="sd">        tools: A list of tools (functions) that the model can try calling.</span>
<span class="sd">        stream: Whether to stream the response.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An awaitable for a single GenerationResponse object if stream == False</span>
<span class="sd">        An awaitable for a stream of GenerationResponse objects if stream == True</span>

<span class="sd">    Raises:</span>
<span class="sd">        ResponseValidationError: If the response was blocked or is incomplete.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">stream</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message_streaming_async</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_send_message_async</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">content</span><span class="p">,</span>
            <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">,</span>
            <span class="n">safety_settings</span><span class="o">=</span><span class="n">safety_settings</span><span class="p">,</span>
            <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.Content" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Content</span>


</h2>


    <div class="doc doc-contents ">


      <p>The multi-part content of a message.</p>


<details class="usage" open>
  <summary>Usage</summary>
  <div class="language-text highlight"><pre><span></span><code>response = model.generate_content(contents=[
    Content(role=&quot;user&quot;, parts=[Part.from_text(&quot;Why is sky blue?&quot;)])
])
</code></pre></div>
</details>
              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1778</span>
<span class="normal">1779</span>
<span class="normal">1780</span>
<span class="normal">1781</span>
<span class="normal">1782</span>
<span class="normal">1783</span>
<span class="normal">1784</span>
<span class="normal">1785</span>
<span class="normal">1786</span>
<span class="normal">1787</span>
<span class="normal">1788</span>
<span class="normal">1789</span>
<span class="normal">1790</span>
<span class="normal">1791</span>
<span class="normal">1792</span>
<span class="normal">1793</span>
<span class="normal">1794</span>
<span class="normal">1795</span>
<span class="normal">1796</span>
<span class="normal">1797</span>
<span class="normal">1798</span>
<span class="normal">1799</span>
<span class="normal">1800</span>
<span class="normal">1801</span>
<span class="normal">1802</span>
<span class="normal">1803</span>
<span class="normal">1804</span>
<span class="normal">1805</span>
<span class="normal">1806</span>
<span class="normal">1807</span>
<span class="normal">1808</span>
<span class="normal">1809</span>
<span class="normal">1810</span>
<span class="normal">1811</span>
<span class="normal">1812</span>
<span class="normal">1813</span>
<span class="normal">1814</span>
<span class="normal">1815</span>
<span class="normal">1816</span>
<span class="normal">1817</span>
<span class="normal">1818</span>
<span class="normal">1819</span>
<span class="normal">1820</span>
<span class="normal">1821</span>
<span class="normal">1822</span>
<span class="normal">1823</span>
<span class="normal">1824</span>
<span class="normal">1825</span>
<span class="normal">1826</span>
<span class="normal">1827</span>
<span class="normal">1828</span>
<span class="normal">1829</span>
<span class="normal">1830</span>
<span class="normal">1831</span>
<span class="normal">1832</span>
<span class="normal">1833</span>
<span class="normal">1834</span>
<span class="normal">1835</span>
<span class="normal">1836</span>
<span class="normal">1837</span>
<span class="normal">1838</span>
<span class="normal">1839</span>
<span class="normal">1840</span>
<span class="normal">1841</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Content</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The multi-part content of a message.</span>

<span class="sd">    Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        response = model.generate_content(contents=[</span>
<span class="sd">            Content(role=&quot;user&quot;, parts=[Part.from_text(&quot;Why is sky blue?&quot;)])</span>
<span class="sd">        ])</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">parts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;Part&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">role</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">raw_parts</span> <span class="o">=</span> <span class="p">[</span><span class="n">part</span><span class="o">.</span><span class="n">_raw_part</span> <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">parts</span> <span class="ow">or</span> <span class="p">[]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_content</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Content</span><span class="p">(</span><span class="n">parts</span><span class="o">=</span><span class="n">raw_parts</span><span class="p">,</span> <span class="n">role</span><span class="o">=</span><span class="n">role</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_gapic</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">raw_content</span><span class="p">:</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Content</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Content&quot;</span><span class="p">:</span>
        <span class="n">content</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
        <span class="n">content</span><span class="o">.</span><span class="n">_raw_content</span> <span class="o">=</span> <span class="n">raw_content</span>
        <span class="k">return</span> <span class="n">content</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">content_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;Content&quot;</span><span class="p">:</span>
        <span class="n">raw_content</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Content</span><span class="p">()</span>
        <span class="n">json_format</span><span class="o">.</span><span class="n">ParseDict</span><span class="p">(</span><span class="n">content_dict</span><span class="p">,</span> <span class="n">raw_content</span><span class="o">.</span><span class="n">_pb</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_content</span><span class="o">=</span><span class="n">raw_content</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_content</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_content</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;Part&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">Part</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_part</span><span class="o">=</span><span class="n">raw_part</span><span class="p">)</span> <span class="k">for</span> <span class="n">raw_part</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_content</span><span class="o">.</span><span class="n">parts</span>
        <span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">role</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_content</span><span class="o">.</span><span class="n">role</span>

    <span class="nd">@role</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">role</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">role</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_content</span><span class="o">.</span><span class="n">role</span> <span class="o">=</span> <span class="n">role</span>

    <span class="c1"># GenerationPart properties</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">text</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Multiple content parts are not supported.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">parts</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Response candidate content has no parts (and thus no text).&quot;</span>
                <span class="s2">&quot; The candidate is likely blocked by the safety filters.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;Content:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">_dict_to_pretty_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.FunctionDeclaration" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">FunctionDeclaration</span>


</h2>


    <div class="doc doc-contents ">


      <p>A representation of a function declaration.</p>


<details class="usage" open>
  <summary>Usage</summary>
  <p>Create function declaration and tool:
<div class="language-text highlight"><pre><span></span><code>get_current_weather_func = generative_models.FunctionDeclaration(
    name=&quot;get_current_weather&quot;,
    description=&quot;Get the current weather in a given location&quot;,
    parameters={
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
            &quot;location&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;
            },
            &quot;unit&quot;: {
                &quot;type&quot;: &quot;string&quot;,
                &quot;enum&quot;: [
                    &quot;celsius&quot;,
                    &quot;fahrenheit&quot;,
                ]
            }
        },
        &quot;required&quot;: [
            &quot;location&quot;
        ]
    },
)
weather_tool = generative_models.Tool(
    function_declarations=[get_current_weather_func],
)
</code></pre></div>
Use tool in <code class="language-python highlight"><span class="n">GenerativeModel</span><span class="o">.</span><span class="n">generate_content</span></code>:
<div class="language-text highlight"><pre><span></span><code>model = GenerativeModel(&quot;gemini-pro&quot;)
print(model.generate_content(
    &quot;What is the weather like in Boston?&quot;,
    # You can specify tools when creating a model to avoid having to send them with every request.
    tools=[weather_tool],
))
</code></pre></div>
Use tool in chat:
<div class="language-text highlight"><pre><span></span><code>model = GenerativeModel(
    &quot;gemini-pro&quot;,
    # You can specify tools when creating a model to avoid having to send them with every request.
    tools=[weather_tool],
)
chat = model.start_chat()
print(chat.send_message(&quot;What is the weather like in Boston?&quot;))
print(chat.send_message(
    Part.from_function_response(
        name=&quot;get_current_weather&quot;,
        response={
            &quot;content&quot;: {&quot;weather_there&quot;: &quot;super nice&quot;},
        }
    ),
))
</code></pre></div></p>
</details>
              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FunctionDeclaration</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A representation of a function declaration.</span>

<span class="sd">    Usage:</span>
<span class="sd">        Create function declaration and tool:</span>
<span class="sd">        ```</span>
<span class="sd">        get_current_weather_func = generative_models.FunctionDeclaration(</span>
<span class="sd">            name=&quot;get_current_weather&quot;,</span>
<span class="sd">            description=&quot;Get the current weather in a given location&quot;,</span>
<span class="sd">            parameters={</span>
<span class="sd">                &quot;type&quot;: &quot;object&quot;,</span>
<span class="sd">                &quot;properties&quot;: {</span>
<span class="sd">                    &quot;location&quot;: {</span>
<span class="sd">                        &quot;type&quot;: &quot;string&quot;,</span>
<span class="sd">                        &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;</span>
<span class="sd">                    },</span>
<span class="sd">                    &quot;unit&quot;: {</span>
<span class="sd">                        &quot;type&quot;: &quot;string&quot;,</span>
<span class="sd">                        &quot;enum&quot;: [</span>
<span class="sd">                            &quot;celsius&quot;,</span>
<span class="sd">                            &quot;fahrenheit&quot;,</span>
<span class="sd">                        ]</span>
<span class="sd">                    }</span>
<span class="sd">                },</span>
<span class="sd">                &quot;required&quot;: [</span>
<span class="sd">                    &quot;location&quot;</span>
<span class="sd">                ]</span>
<span class="sd">            },</span>
<span class="sd">        )</span>
<span class="sd">        weather_tool = generative_models.Tool(</span>
<span class="sd">            function_declarations=[get_current_weather_func],</span>
<span class="sd">        )</span>
<span class="sd">        ```</span>
<span class="sd">        Use tool in `GenerativeModel.generate_content`:</span>
<span class="sd">        ```</span>
<span class="sd">        model = GenerativeModel(&quot;gemini-pro&quot;)</span>
<span class="sd">        print(model.generate_content(</span>
<span class="sd">            &quot;What is the weather like in Boston?&quot;,</span>
<span class="sd">            # You can specify tools when creating a model to avoid having to send them with every request.</span>
<span class="sd">            tools=[weather_tool],</span>
<span class="sd">        ))</span>
<span class="sd">        ```</span>
<span class="sd">        Use tool in chat:</span>
<span class="sd">        ```</span>
<span class="sd">        model = GenerativeModel(</span>
<span class="sd">            &quot;gemini-pro&quot;,</span>
<span class="sd">            # You can specify tools when creating a model to avoid having to send them with every request.</span>
<span class="sd">            tools=[weather_tool],</span>
<span class="sd">        )</span>
<span class="sd">        chat = model.start_chat()</span>
<span class="sd">        print(chat.send_message(&quot;What is the weather like in Boston?&quot;))</span>
<span class="sd">        print(chat.send_message(</span>
<span class="sd">            Part.from_function_response(</span>
<span class="sd">                name=&quot;get_current_weather&quot;,</span>
<span class="sd">                response={</span>
<span class="sd">                    &quot;content&quot;: {&quot;weather_there&quot;: &quot;super nice&quot;},</span>
<span class="sd">                }</span>
<span class="sd">            ),</span>
<span class="sd">        ))</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">description</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Constructs a FunctionDeclaration.</span>

<span class="sd">        Args:</span>
<span class="sd">            name: The name of the function that the model can call.</span>
<span class="sd">            parameters: Describes the parameters to this function in JSON Schema Object format.</span>
<span class="sd">            description: Description and purpose of the function.</span>
<span class="sd">                Model uses it to decide how and whether to call the function.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">gapic_schema_dict</span> <span class="o">=</span> <span class="n">_convert_schema_dict_to_gapic</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
        <span class="n">raw_schema</span> <span class="o">=</span> <span class="n">aiplatform_types</span><span class="o">.</span><span class="n">Schema</span><span class="p">(</span><span class="n">gapic_schema_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_function_declaration</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">FunctionDeclaration</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="n">description</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">raw_schema</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_func</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;CallableFunctionDeclaration&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">CallableFunctionDeclaration</span><span class="o">.</span><span class="n">from_func</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_function_declaration</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_function_declaration</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.generative_models.FunctionDeclaration.__init__" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">__init__</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="fm">__init__</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="nf">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">description</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The name of the function that the model can call.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>parameters</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Describes the parameters to this function in JSON Schema Object format.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>description</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Description and purpose of the function.
Model uses it to decide how and whether to call the function.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">description</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Constructs a FunctionDeclaration.</span>

<span class="sd">    Args:</span>
<span class="sd">        name: The name of the function that the model can call.</span>
<span class="sd">        parameters: Describes the parameters to this function in JSON Schema Object format.</span>
<span class="sd">        description: Description and purpose of the function.</span>
<span class="sd">            Model uses it to decide how and whether to call the function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gapic_schema_dict</span> <span class="o">=</span> <span class="n">_convert_schema_dict_to_gapic</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    <span class="n">raw_schema</span> <span class="o">=</span> <span class="n">aiplatform_types</span><span class="o">.</span><span class="n">Schema</span><span class="p">(</span><span class="n">gapic_schema_dict</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_raw_function_declaration</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">FunctionDeclaration</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="n">description</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">raw_schema</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.GenerationConfig" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">GenerationConfig</span>


</h2>


    <div class="doc doc-contents ">


      <p>Parameters for the generation.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">GenerationConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parameters for the generation.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">response_mime_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">response_schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Constructs a GenerationConfig object.</span>

<span class="sd">        Args:</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0.0, 1.0]</span>
<span class="sd">            top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]</span>
<span class="sd">            top_k: If specified, top-k sampling will be used.</span>
<span class="sd">            candidate_count: Number of candidates to generate.</span>
<span class="sd">            max_output_tokens: The maximum number of output tokens to generate per message.</span>
<span class="sd">            stop_sequences: A list of stop sequences.</span>
<span class="sd">            presence_penalty: Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">                thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</span>
<span class="sd">            frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">                text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</span>
<span class="sd">            response_mime_type: Output response mimetype of the generated</span>
<span class="sd">                candidate text. Supported mimetypes:</span>

<span class="sd">                -  ``text/plain``: (default) Text output.</span>
<span class="sd">                -  ``application/json``: JSON response in the candidates.</span>

<span class="sd">                The model needs to be prompted to output the appropriate</span>
<span class="sd">                response type, otherwise the behavior is undefined.</span>
<span class="sd">            response_schema: Output response schema of the genreated candidate text. Only valid when</span>
<span class="sd">                response_mime_type is application/json.</span>

<span class="sd">        Usage:</span>
<span class="sd">            ```</span>
<span class="sd">            response = model.generate_content(</span>
<span class="sd">                &quot;Why is sky blue?&quot;,</span>
<span class="sd">                generation_config=GenerationConfig(</span>
<span class="sd">                    temperature=0.1,</span>
<span class="sd">                    top_p=0.95,</span>
<span class="sd">                    top_k=20,</span>
<span class="sd">                    candidate_count=1,</span>
<span class="sd">                    max_output_tokens=100,</span>
<span class="sd">                    stop_sequences=[&quot;\n\n\n&quot;],</span>
<span class="sd">                )</span>
<span class="sd">            )</span>
<span class="sd">            ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">response_schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">raw_schema</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gapic_schema_dict</span> <span class="o">=</span> <span class="n">_convert_schema_dict_to_gapic</span><span class="p">(</span><span class="n">response_schema</span><span class="p">)</span>
            <span class="n">raw_schema</span> <span class="o">=</span> <span class="n">aiplatform_types</span><span class="o">.</span><span class="n">Schema</span><span class="p">(</span><span class="n">gapic_schema_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_generation_config</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">GenerationConfig</span><span class="p">(</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
            <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
            <span class="n">response_mime_type</span><span class="o">=</span><span class="n">response_mime_type</span><span class="p">,</span>
            <span class="n">response_schema</span><span class="o">=</span><span class="n">raw_schema</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_gapic</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">raw_generation_config</span><span class="p">:</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">GenerationConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;GenerationConfig&quot;</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
        <span class="n">response</span><span class="o">.</span><span class="n">_raw_generation_config</span> <span class="o">=</span> <span class="n">raw_generation_config</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">generation_config_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;GenerationConfig&quot;</span><span class="p">:</span>
        <span class="n">raw_generation_config</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">GenerationConfig</span><span class="p">(</span>
            <span class="n">generation_config_dict</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_generation_config</span><span class="o">=</span><span class="n">raw_generation_config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_generation_config</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_generation_config</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.generative_models.GenerationConfig.__init__" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">__init__</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="fm">__init__</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="nf">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">response_mime_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">response_schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0.0, 1.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified, top-k sampling will be used.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to generate.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The maximum number of output tokens to generate per message.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of stop sequences.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>presence_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that have appeared in the generated text,
thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>frequency_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that repeatedly appear in the generated
text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>response_mime_type</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Output response mimetype of the generated
candidate text. Supported mimetypes:</p>
<ul>
<li><code class="language-python highlight"><span class="n">text</span><span class="o">/</span><span class="n">plain</span></code>: (default) Text output.</li>
<li><code class="language-python highlight"><span class="n">application</span><span class="o">/</span><span class="n">json</span></code>: JSON response in the candidates.</li>
</ul>
<p>The model needs to be prompted to output the appropriate
response type, otherwise the behavior is undefined.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>response_schema</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Output response schema of the genreated candidate text. Only valid when
response_mime_type is application/json.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<details class="usage" open>
  <summary>Usage</summary>
  <div class="language-text highlight"><pre><span></span><code>response = model.generate_content(
    &quot;Why is sky blue?&quot;,
    generation_config=GenerationConfig(
        temperature=0.1,
        top_p=0.95,
        top_k=20,
        candidate_count=1,
        max_output_tokens=100,
        stop_sequences=[&quot;\n\n\n&quot;],
    )
)
</code></pre></div>
</details>
            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span>
<span class="normal">1204</span>
<span class="normal">1205</span>
<span class="normal">1206</span>
<span class="normal">1207</span>
<span class="normal">1208</span>
<span class="normal">1209</span>
<span class="normal">1210</span>
<span class="normal">1211</span>
<span class="normal">1212</span>
<span class="normal">1213</span>
<span class="normal">1214</span>
<span class="normal">1215</span>
<span class="normal">1216</span>
<span class="normal">1217</span>
<span class="normal">1218</span>
<span class="normal">1219</span>
<span class="normal">1220</span>
<span class="normal">1221</span>
<span class="normal">1222</span>
<span class="normal">1223</span>
<span class="normal">1224</span>
<span class="normal">1225</span>
<span class="normal">1226</span>
<span class="normal">1227</span>
<span class="normal">1228</span>
<span class="normal">1229</span>
<span class="normal">1230</span>
<span class="normal">1231</span>
<span class="normal">1232</span>
<span class="normal">1233</span>
<span class="normal">1234</span>
<span class="normal">1235</span>
<span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">response_mime_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">response_schema</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Constructs a GenerationConfig object.</span>

<span class="sd">    Args:</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]</span>
<span class="sd">        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]</span>
<span class="sd">        top_k: If specified, top-k sampling will be used.</span>
<span class="sd">        candidate_count: Number of candidates to generate.</span>
<span class="sd">        max_output_tokens: The maximum number of output tokens to generate per message.</span>
<span class="sd">        stop_sequences: A list of stop sequences.</span>
<span class="sd">        presence_penalty: Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">            thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</span>
<span class="sd">        frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">            text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</span>
<span class="sd">        response_mime_type: Output response mimetype of the generated</span>
<span class="sd">            candidate text. Supported mimetypes:</span>

<span class="sd">            -  ``text/plain``: (default) Text output.</span>
<span class="sd">            -  ``application/json``: JSON response in the candidates.</span>

<span class="sd">            The model needs to be prompted to output the appropriate</span>
<span class="sd">            response type, otherwise the behavior is undefined.</span>
<span class="sd">        response_schema: Output response schema of the genreated candidate text. Only valid when</span>
<span class="sd">            response_mime_type is application/json.</span>

<span class="sd">    Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        response = model.generate_content(</span>
<span class="sd">            &quot;Why is sky blue?&quot;,</span>
<span class="sd">            generation_config=GenerationConfig(</span>
<span class="sd">                temperature=0.1,</span>
<span class="sd">                top_p=0.95,</span>
<span class="sd">                top_k=20,</span>
<span class="sd">                candidate_count=1,</span>
<span class="sd">                max_output_tokens=100,</span>
<span class="sd">                stop_sequences=[&quot;\n\n\n&quot;],</span>
<span class="sd">            )</span>
<span class="sd">        )</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">response_schema</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">raw_schema</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gapic_schema_dict</span> <span class="o">=</span> <span class="n">_convert_schema_dict_to_gapic</span><span class="p">(</span><span class="n">response_schema</span><span class="p">)</span>
        <span class="n">raw_schema</span> <span class="o">=</span> <span class="n">aiplatform_types</span><span class="o">.</span><span class="n">Schema</span><span class="p">(</span><span class="n">gapic_schema_dict</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_raw_generation_config</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">GenerationConfig</span><span class="p">(</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
        <span class="n">response_mime_type</span><span class="o">=</span><span class="n">response_mime_type</span><span class="p">,</span>
        <span class="n">response_schema</span><span class="o">=</span><span class="n">raw_schema</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.GenerationResponse" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">GenerationResponse</span>


</h2>


    <div class="doc doc-contents ">


      <p>The response from the model.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">GenerationResponse</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The response from the model.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">raw_response</span> <span class="o">=</span> <span class="n">gapic_prediction_service_types</span><span class="o">.</span><span class="n">GenerateContentResponse</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_response</span> <span class="o">=</span> <span class="n">raw_response</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_gapic</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">raw_response</span><span class="p">:</span> <span class="n">gapic_prediction_service_types</span><span class="o">.</span><span class="n">GenerateContentResponse</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;GenerationResponse&quot;</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
        <span class="n">response</span><span class="o">.</span><span class="n">_raw_response</span> <span class="o">=</span> <span class="n">raw_response</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">response_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;GenerationResponse&quot;</span><span class="p">:</span>
        <span class="n">raw_response</span> <span class="o">=</span> <span class="n">gapic_prediction_service_types</span><span class="o">.</span><span class="n">GenerateContentResponse</span><span class="p">()</span>
        <span class="n">json_format</span><span class="o">.</span><span class="n">ParseDict</span><span class="p">(</span><span class="n">response_dict</span><span class="p">,</span> <span class="n">raw_response</span><span class="o">.</span><span class="n">_pb</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_response</span><span class="o">=</span><span class="n">raw_response</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_response</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_response</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">candidates</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;Candidate&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">Candidate</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_candidate</span><span class="o">=</span><span class="n">raw_candidate</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">raw_candidate</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_response</span><span class="o">.</span><span class="n">candidates</span>
        <span class="p">]</span>

    <span class="c1"># GenerationPart properties</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">text</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">candidates</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The response has multiple candidates.&quot;</span>
                <span class="s2">&quot; Use `response.candidate[i].text` to get text of a particular candidate.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">candidates</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Response has no candidates (and thus no text).&quot;</span>
                <span class="s2">&quot; The response is likely blocked by the safety filters.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;Response:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">_dict_to_pretty_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
            <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">AttributeError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Enrich the error message with the whole Response.</span>
            <span class="c1"># The Candidate object does not have full information.</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot get the response text.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;Response:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">_dict_to_pretty_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
            <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">prompt_feedback</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_prediction_service_types</span><span class="o">.</span><span class="n">GenerateContentResponse</span><span class="o">.</span><span class="n">PromptFeedback</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_response</span><span class="o">.</span><span class="n">prompt_feedback</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">usage_metadata</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_prediction_service_types</span><span class="o">.</span><span class="n">GenerateContentResponse</span><span class="o">.</span><span class="n">UsageMetadata</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_response</span><span class="o">.</span><span class="n">usage_metadata</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.Image" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Image</span>


</h2>


    <div class="doc doc-contents ">


      <p>The image that can be sent to a generative model.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span>
<span class="normal">2323</span>
<span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span>
<span class="normal">2329</span>
<span class="normal">2330</span>
<span class="normal">2331</span>
<span class="normal">2332</span>
<span class="normal">2333</span>
<span class="normal">2334</span>
<span class="normal">2335</span>
<span class="normal">2336</span>
<span class="normal">2337</span>
<span class="normal">2338</span>
<span class="normal">2339</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Image</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The image that can be sent to a generative model.&quot;&quot;&quot;</span>

    <span class="n">_image_bytes</span><span class="p">:</span> <span class="nb">bytes</span>
    <span class="n">_loaded_image</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;PIL_Image.Image&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">load_from_file</span><span class="p">(</span><span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Image&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads image from file.</span>

<span class="sd">        Args:</span>
<span class="sd">            location: Local path from where to load the image.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loaded image as an `Image` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">image_bytes</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">location</span><span class="p">)</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">()</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">()</span>
        <span class="n">image</span><span class="o">.</span><span class="n">_image_bytes</span> <span class="o">=</span> <span class="n">image_bytes</span>
        <span class="k">return</span> <span class="n">image</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Image&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads image from image bytes.</span>

<span class="sd">        Args:</span>
<span class="sd">            data: Image bytes.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Loaded image as an `Image` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">()</span>
        <span class="n">image</span><span class="o">.</span><span class="n">_image_bytes</span> <span class="o">=</span> <span class="n">data</span>
        <span class="k">return</span> <span class="n">image</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_pil_image</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PIL_Image.Image&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_image</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">PIL_Image</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;The PIL module is not available. Please install the Pillow package.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_image</span> <span class="o">=</span> <span class="n">PIL_Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_image_bytes</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_image</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_mime_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the MIME type of the image.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">PIL_Image</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_FORMAT_TO_MIME_TYPE</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_pil_image</span><span class="o">.</span><span class="n">format</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Fall back to jpeg</span>
            <span class="k">return</span> <span class="s2">&quot;image/jpeg&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bytes</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the image data.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_image_bytes</span>

    <span class="k">def</span> <span class="nf">_repr_png_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pil_image</span><span class="o">.</span><span class="n">_repr_png_</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="vertexai.generative_models.Image.data" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">data</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">data</span><span class="p">:</span> <span class="nb">bytes</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Returns the image data.</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="vertexai.generative_models.Image.from_bytes" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">from_bytes</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">from_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Image</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Loads image from image bytes.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>data</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Image bytes.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bytes</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.Image" href="../vertexai/generative_models/#vertexai.generative_models.Image">Image</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Loaded image as an <code class="language-python highlight"><span class="n">Image</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">from_bytes</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Image&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads image from image bytes.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: Image bytes.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loaded image as an `Image` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">()</span>
    <span class="n">image</span><span class="o">.</span><span class="n">_image_bytes</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.generative_models.Image.load_from_file" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">load_from_file</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">load_from_file</span><span class="p">(</span><span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Image</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Loads image from file.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>location</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Local path from where to load the image.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.generative_models._generative_models.Image" href="../vertexai/generative_models/#vertexai.generative_models.Image">Image</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Loaded image as an <code class="language-python highlight"><span class="n">Image</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">load_from_file</span><span class="p">(</span><span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Image&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads image from file.</span>

<span class="sd">    Args:</span>
<span class="sd">        location: Local path from where to load the image.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Loaded image as an `Image` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">image_bytes</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">location</span><span class="p">)</span><span class="o">.</span><span class="n">read_bytes</span><span class="p">()</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">()</span>
    <span class="n">image</span><span class="o">.</span><span class="n">_image_bytes</span> <span class="o">=</span> <span class="n">image_bytes</span>
    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.Part" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Part</span>


</h2>


    <div class="doc doc-contents ">


      <p>A part of a multi-part Content message.</p>


<details class="usage" open>
  <summary>Usage</summary>
  <div class="language-text highlight"><pre><span></span><code>text_part = Part.from_text(&quot;Why is sky blue?&quot;)
image_part = Part.from_image(Image.load_from_file(&quot;image.jpg&quot;))
video_part = Part.from_uri(uri=&quot;gs://.../video.mp4&quot;, mime_type=&quot;video/mp4&quot;)
function_response_part = Part.from_function_response(
    name=&quot;get_current_weather&quot;,
    response={
        &quot;content&quot;: {&quot;weather_there&quot;: &quot;super nice&quot;},
    }
)

response1 = model.generate_content([text_part, image_part])
response2 = model.generate_content(video_part)
response3 = chat.send_message(function_response_part)
</code></pre></div>
</details>
              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1844</span>
<span class="normal">1845</span>
<span class="normal">1846</span>
<span class="normal">1847</span>
<span class="normal">1848</span>
<span class="normal">1849</span>
<span class="normal">1850</span>
<span class="normal">1851</span>
<span class="normal">1852</span>
<span class="normal">1853</span>
<span class="normal">1854</span>
<span class="normal">1855</span>
<span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span>
<span class="normal">1911</span>
<span class="normal">1912</span>
<span class="normal">1913</span>
<span class="normal">1914</span>
<span class="normal">1915</span>
<span class="normal">1916</span>
<span class="normal">1917</span>
<span class="normal">1918</span>
<span class="normal">1919</span>
<span class="normal">1920</span>
<span class="normal">1921</span>
<span class="normal">1922</span>
<span class="normal">1923</span>
<span class="normal">1924</span>
<span class="normal">1925</span>
<span class="normal">1926</span>
<span class="normal">1927</span>
<span class="normal">1928</span>
<span class="normal">1929</span>
<span class="normal">1930</span>
<span class="normal">1931</span>
<span class="normal">1932</span>
<span class="normal">1933</span>
<span class="normal">1934</span>
<span class="normal">1935</span>
<span class="normal">1936</span>
<span class="normal">1937</span>
<span class="normal">1938</span>
<span class="normal">1939</span>
<span class="normal">1940</span>
<span class="normal">1941</span>
<span class="normal">1942</span>
<span class="normal">1943</span>
<span class="normal">1944</span>
<span class="normal">1945</span>
<span class="normal">1946</span>
<span class="normal">1947</span>
<span class="normal">1948</span>
<span class="normal">1949</span>
<span class="normal">1950</span>
<span class="normal">1951</span>
<span class="normal">1952</span>
<span class="normal">1953</span>
<span class="normal">1954</span>
<span class="normal">1955</span>
<span class="normal">1956</span>
<span class="normal">1957</span>
<span class="normal">1958</span>
<span class="normal">1959</span>
<span class="normal">1960</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Part</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A part of a multi-part Content message.</span>

<span class="sd">    Usage:</span>
<span class="sd">        ```</span>
<span class="sd">        text_part = Part.from_text(&quot;Why is sky blue?&quot;)</span>
<span class="sd">        image_part = Part.from_image(Image.load_from_file(&quot;image.jpg&quot;))</span>
<span class="sd">        video_part = Part.from_uri(uri=&quot;gs://.../video.mp4&quot;, mime_type=&quot;video/mp4&quot;)</span>
<span class="sd">        function_response_part = Part.from_function_response(</span>
<span class="sd">            name=&quot;get_current_weather&quot;,</span>
<span class="sd">            response={</span>
<span class="sd">                &quot;content&quot;: {&quot;weather_there&quot;: &quot;super nice&quot;},</span>
<span class="sd">            }</span>
<span class="sd">        )</span>

<span class="sd">        response1 = model.generate_content([text_part, image_part])</span>
<span class="sd">        response2 = model.generate_content(video_part)</span>
<span class="sd">        response3 = chat.send_message(function_response_part)</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">raw_part</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Part</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span> <span class="o">=</span> <span class="n">raw_part</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_gapic</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">raw_part</span><span class="p">:</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Part</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Part&quot;</span><span class="p">:</span>
        <span class="n">part</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">()</span>
        <span class="n">part</span><span class="o">.</span><span class="n">_raw_part</span> <span class="o">=</span> <span class="n">raw_part</span>
        <span class="k">return</span> <span class="n">part</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">part_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;Part&quot;</span><span class="p">:</span>
        <span class="n">raw_part</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Part</span><span class="p">()</span>
        <span class="n">json_format</span><span class="o">.</span><span class="n">ParseDict</span><span class="p">(</span><span class="n">part_dict</span><span class="p">,</span> <span class="n">raw_part</span><span class="o">.</span><span class="n">_pb</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_part</span><span class="o">=</span><span class="n">raw_part</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_data</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">mime_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Part&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Part</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">raw_part</span><span class="o">=</span><span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Part</span><span class="p">(</span>
                <span class="n">inline_data</span><span class="o">=</span><span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Blob</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">mime_type</span><span class="o">=</span><span class="n">mime_type</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_uri</span><span class="p">(</span><span class="n">uri</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">mime_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Part&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Part</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">raw_part</span><span class="o">=</span><span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Part</span><span class="p">(</span>
                <span class="n">file_data</span><span class="o">=</span><span class="n">gapic_content_types</span><span class="o">.</span><span class="n">FileData</span><span class="p">(</span>
                    <span class="n">file_uri</span><span class="o">=</span><span class="n">uri</span><span class="p">,</span> <span class="n">mime_type</span><span class="o">=</span><span class="n">mime_type</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Part&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Part</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_part</span><span class="o">=</span><span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Part</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_image</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="s2">&quot;Image&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Part&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Part</span><span class="o">.</span><span class="n">from_data</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">mime_type</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">_mime_type</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_function_response</span><span class="p">(</span><span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">response</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;Part&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Part</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span>
            <span class="n">raw_part</span><span class="o">=</span><span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Part</span><span class="p">(</span>
                <span class="n">function_response</span><span class="o">=</span><span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">FunctionResponse</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
                    <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">text</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;text&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Response candidate content part has no text.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;Part:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">_dict_to_pretty_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">text</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mime_type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="n">part_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">_pb</span><span class="o">.</span><span class="n">WhichOneof</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">part_type</span> <span class="o">==</span> <span class="s2">&quot;inline_data&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">inline_data</span><span class="o">.</span><span class="n">mime_type</span>
        <span class="k">if</span> <span class="n">part_type</span> <span class="o">==</span> <span class="s2">&quot;file_data&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">file_data</span><span class="o">.</span><span class="n">mime_type</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Part has no mime_type.</span><span class="se">\n</span><span class="s2">Part:</span><span class="se">\n</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">inline_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">Blob</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">inline_data</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">file_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">FileData</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">file_data</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">function_call</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">FunctionCall</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">function_call</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">function_response</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">FunctionResponse</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">function_response</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_image</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Image&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Image</span><span class="o">.</span><span class="n">from_bytes</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_part</span><span class="o">.</span><span class="n">inline_data</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.SafetySetting" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">SafetySetting</span>


</h2>


    <div class="doc doc-contents ">


      <p>Parameters for the generation.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1963</span>
<span class="normal">1964</span>
<span class="normal">1965</span>
<span class="normal">1966</span>
<span class="normal">1967</span>
<span class="normal">1968</span>
<span class="normal">1969</span>
<span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span>
<span class="normal">1991</span>
<span class="normal">1992</span>
<span class="normal">1993</span>
<span class="normal">1994</span>
<span class="normal">1995</span>
<span class="normal">1996</span>
<span class="normal">1997</span>
<span class="normal">1998</span>
<span class="normal">1999</span>
<span class="normal">2000</span>
<span class="normal">2001</span>
<span class="normal">2002</span>
<span class="normal">2003</span>
<span class="normal">2004</span>
<span class="normal">2005</span>
<span class="normal">2006</span>
<span class="normal">2007</span>
<span class="normal">2008</span>
<span class="normal">2009</span>
<span class="normal">2010</span>
<span class="normal">2011</span>
<span class="normal">2012</span>
<span class="normal">2013</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SafetySetting</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Parameters for the generation.&quot;&quot;&quot;</span>

    <span class="n">HarmCategory</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">HarmCategory</span>
    <span class="n">HarmBlockMethod</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">SafetySetting</span><span class="o">.</span><span class="n">HarmBlockMethod</span>
    <span class="n">HarmBlockThreshold</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">SafetySetting</span><span class="o">.</span><span class="n">HarmBlockThreshold</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">category</span><span class="p">:</span> <span class="s2">&quot;SafetySetting.HarmCategory&quot;</span><span class="p">,</span>
        <span class="n">threshold</span><span class="p">:</span> <span class="s2">&quot;SafetySetting.HarmBlockThreshold&quot;</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;SafetySetting.HarmBlockMethod&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Safety settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            category: Harm category.</span>
<span class="sd">            threshold: The harm block threshold.</span>
<span class="sd">            method: Specify if the threshold is used for probability or severity</span>
<span class="sd">                score. If not specified, the threshold is used for probability</span>
<span class="sd">                score.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_safety_setting</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">SafetySetting</span><span class="p">(</span>
            <span class="n">category</span><span class="o">=</span><span class="n">category</span><span class="p">,</span>
            <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span>
            <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_gapic</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">raw_safety_setting</span><span class="p">:</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">SafetySetting</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SafetySetting&quot;</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">category</span><span class="o">=</span><span class="n">raw_safety_setting</span><span class="o">.</span><span class="n">category</span><span class="p">,</span>
            <span class="n">threshold</span><span class="o">=</span><span class="n">raw_safety_setting</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">_raw_safety_setting</span> <span class="o">=</span> <span class="n">raw_safety_setting</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">safety_setting_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;SafetySetting&quot;</span><span class="p">:</span>
        <span class="n">raw_safety_setting</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">SafetySetting</span><span class="p">(</span><span class="n">safety_setting_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_safety_setting</span><span class="o">=</span><span class="n">raw_safety_setting</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_safety_setting</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_safety_setting</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.generative_models.SafetySetting.__init__" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">__init__</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="fm">__init__</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="nf">category</span><span class="p">:</span> <span class="n">SafetySetting</span><span class="o">.</span><span class="n">HarmCategory</span><span class="p">,</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="n">SafetySetting</span><span class="o">.</span><span class="n">HarmBlockThreshold</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SafetySetting</span><span class="o">.</span><span class="n">HarmBlockMethod</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">



<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>category</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Harm category.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="vertexai.generative_models._generative_models.SafetySetting.HarmCategory">HarmCategory</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>threshold</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The harm block threshold.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="vertexai.generative_models._generative_models.SafetySetting.HarmBlockThreshold">HarmBlockThreshold</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>method</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Specify if the threshold is used for probability or severity
score. If not specified, the threshold is used for probability
score.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.generative_models._generative_models.SafetySetting.HarmBlockMethod">HarmBlockMethod</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1970</span>
<span class="normal">1971</span>
<span class="normal">1972</span>
<span class="normal">1973</span>
<span class="normal">1974</span>
<span class="normal">1975</span>
<span class="normal">1976</span>
<span class="normal">1977</span>
<span class="normal">1978</span>
<span class="normal">1979</span>
<span class="normal">1980</span>
<span class="normal">1981</span>
<span class="normal">1982</span>
<span class="normal">1983</span>
<span class="normal">1984</span>
<span class="normal">1985</span>
<span class="normal">1986</span>
<span class="normal">1987</span>
<span class="normal">1988</span>
<span class="normal">1989</span>
<span class="normal">1990</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">category</span><span class="p">:</span> <span class="s2">&quot;SafetySetting.HarmCategory&quot;</span><span class="p">,</span>
    <span class="n">threshold</span><span class="p">:</span> <span class="s2">&quot;SafetySetting.HarmBlockThreshold&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;SafetySetting.HarmBlockMethod&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Safety settings.</span>

<span class="sd">    Args:</span>
<span class="sd">        category: Harm category.</span>
<span class="sd">        threshold: The harm block threshold.</span>
<span class="sd">        method: Specify if the threshold is used for probability or severity</span>
<span class="sd">            score. If not specified, the threshold is used for probability</span>
<span class="sd">            score.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_raw_safety_setting</span> <span class="o">=</span> <span class="n">gapic_content_types</span><span class="o">.</span><span class="n">SafetySetting</span><span class="p">(</span>
        <span class="n">category</span><span class="o">=</span><span class="n">category</span><span class="p">,</span>
        <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span>
        <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.Tool" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Tool</span>


</h2>


    <div class="doc doc-contents ">


      <p>A collection of functions that the model may use to generate response.</p>


<details class="usage" open>
  <summary>Usage</summary>
  <p>Create tool from function declarations:
<div class="language-text highlight"><pre><span></span><code>get_current_weather_func = generative_models.FunctionDeclaration(...)
weather_tool = generative_models.Tool(
    function_declarations=[get_current_weather_func],
)
</code></pre></div>
Use tool in <code class="language-python highlight"><span class="n">GenerativeModel</span><span class="o">.</span><span class="n">generate_content</span></code>:
<div class="language-text highlight"><pre><span></span><code>model = GenerativeModel(&quot;gemini-pro&quot;)
print(model.generate_content(
    &quot;What is the weather like in Boston?&quot;,
    # You can specify tools when creating a model to avoid having to send them with every request.
    tools=[weather_tool],
))
</code></pre></div>
Use tool in chat:
<div class="language-text highlight"><pre><span></span><code>model = GenerativeModel(
    &quot;gemini-pro&quot;,
    # You can specify tools when creating a model to avoid having to send them with every request.
    tools=[weather_tool],
)
chat = model.start_chat()
print(chat.send_message(&quot;What is the weather like in Boston?&quot;))
print(chat.send_message(
    Part.from_function_response(
        name=&quot;get_current_weather&quot;,
        response={
            &quot;content&quot;: {&quot;weather_there&quot;: &quot;super nice&quot;},
        }
    ),
))
</code></pre></div></p>
</details>
              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Tool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A collection of functions that the model may use to generate response.</span>

<span class="sd">    Usage:</span>
<span class="sd">        Create tool from function declarations:</span>
<span class="sd">        ```</span>
<span class="sd">        get_current_weather_func = generative_models.FunctionDeclaration(...)</span>
<span class="sd">        weather_tool = generative_models.Tool(</span>
<span class="sd">            function_declarations=[get_current_weather_func],</span>
<span class="sd">        )</span>
<span class="sd">        ```</span>
<span class="sd">        Use tool in `GenerativeModel.generate_content`:</span>
<span class="sd">        ```</span>
<span class="sd">        model = GenerativeModel(&quot;gemini-pro&quot;)</span>
<span class="sd">        print(model.generate_content(</span>
<span class="sd">            &quot;What is the weather like in Boston?&quot;,</span>
<span class="sd">            # You can specify tools when creating a model to avoid having to send them with every request.</span>
<span class="sd">            tools=[weather_tool],</span>
<span class="sd">        ))</span>
<span class="sd">        ```</span>
<span class="sd">        Use tool in chat:</span>
<span class="sd">        ```</span>
<span class="sd">        model = GenerativeModel(</span>
<span class="sd">            &quot;gemini-pro&quot;,</span>
<span class="sd">            # You can specify tools when creating a model to avoid having to send them with every request.</span>
<span class="sd">            tools=[weather_tool],</span>
<span class="sd">        )</span>
<span class="sd">        chat = model.start_chat()</span>
<span class="sd">        print(chat.send_message(&quot;What is the weather like in Boston?&quot;))</span>
<span class="sd">        print(chat.send_message(</span>
<span class="sd">            Part.from_function_response(</span>
<span class="sd">                name=&quot;get_current_weather&quot;,</span>
<span class="sd">                response={</span>
<span class="sd">                    &quot;content&quot;: {&quot;weather_there&quot;: &quot;super nice&quot;},</span>
<span class="sd">                }</span>
<span class="sd">            ),</span>
<span class="sd">        ))</span>
<span class="sd">        ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_raw_tool</span><span class="p">:</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">Tool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">function_declarations</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;FunctionDeclaration&quot;</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="n">gapic_function_declarations</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">function_declaration</span><span class="o">.</span><span class="n">_raw_function_declaration</span>
            <span class="k">for</span> <span class="n">function_declaration</span> <span class="ow">in</span> <span class="n">function_declarations</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_tool</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">Tool</span><span class="p">(</span>
            <span class="n">function_declarations</span><span class="o">=</span><span class="n">gapic_function_declarations</span>
        <span class="p">)</span>
        <span class="n">callable_functions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">function_declaration</span><span class="o">.</span><span class="n">_raw_function_declaration</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">function_declaration</span>
            <span class="k">for</span> <span class="n">function_declaration</span> <span class="ow">in</span> <span class="n">function_declarations</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">function_declaration</span><span class="p">,</span> <span class="n">CallableFunctionDeclaration</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_callable_functions</span> <span class="o">=</span> <span class="n">callable_functions</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_function_declarations</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">function_declarations</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;FunctionDeclaration&quot;</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tool&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tool</span><span class="p">(</span><span class="n">function_declarations</span><span class="o">=</span><span class="n">function_declarations</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_retrieval</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">retrieval</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="s2">&quot;grounding.Retrieval&quot;</span><span class="p">,</span> <span class="s2">&quot;rag.Retrieval&quot;</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tool&quot;</span><span class="p">:</span>
        <span class="n">raw_tool</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">Tool</span><span class="p">(</span><span class="n">retrieval</span><span class="o">=</span><span class="n">retrieval</span><span class="o">.</span><span class="n">_raw_retrieval</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_tool</span><span class="o">=</span><span class="n">raw_tool</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_google_search_retrieval</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">google_search_retrieval</span><span class="p">:</span> <span class="s2">&quot;grounding.GoogleSearchRetrieval&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tool&quot;</span><span class="p">:</span>
        <span class="n">raw_tool</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">Tool</span><span class="p">(</span>
            <span class="n">google_search_retrieval</span><span class="o">=</span><span class="n">google_search_retrieval</span><span class="o">.</span><span class="n">_raw_google_search_retrieval</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_tool</span><span class="o">=</span><span class="n">raw_tool</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_gapic</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">raw_tool</span><span class="p">:</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">Tool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tool&quot;</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">([])</span>
        <span class="n">response</span><span class="o">.</span><span class="n">_raw_tool</span> <span class="o">=</span> <span class="n">raw_tool</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_dict</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">tool_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tool&quot;</span><span class="p">:</span>
        <span class="n">tool_dict</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">tool_dict</span><span class="p">)</span>
        <span class="n">function_declarations</span> <span class="o">=</span> <span class="n">tool_dict</span><span class="p">[</span><span class="s2">&quot;function_declarations&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">function_declaration</span> <span class="ow">in</span> <span class="n">function_declarations</span><span class="p">:</span>
            <span class="n">function_declaration</span><span class="p">[</span><span class="s2">&quot;parameters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_convert_schema_dict_to_gapic</span><span class="p">(</span>
                <span class="n">function_declaration</span><span class="p">[</span><span class="s2">&quot;parameters&quot;</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="n">raw_tool</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">Tool</span><span class="p">(</span><span class="n">tool_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_gapic</span><span class="p">(</span><span class="n">raw_tool</span><span class="o">=</span><span class="n">raw_tool</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_proto_to_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_raw_tool</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_tool</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.generative_models.grounding" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">grounding</span>


</h2>


    <div class="doc doc-contents ">


      <p>Grounding namespace.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2016</span>
<span class="normal">2017</span>
<span class="normal">2018</span>
<span class="normal">2019</span>
<span class="normal">2020</span>
<span class="normal">2021</span>
<span class="normal">2022</span>
<span class="normal">2023</span>
<span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">grounding</span><span class="p">:</span>  <span class="c1"># pylint: disable=invalid-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Grounding namespace.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.generative_models&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;This class must not be instantiated.&quot;</span><span class="p">)</span>

    <span class="k">class</span> <span class="nc">GoogleSearchRetrieval</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Tool to retrieve public web data for grounding, powered by</span>
<span class="sd">        Google Search.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Initializes a Google Search Retrieval tool.&quot;&quot;&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_raw_google_search_retrieval</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">GoogleSearchRetrieval</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="vertexai.generative_models.grounding.GoogleSearchRetrieval" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">GoogleSearchRetrieval</span>


</h3>


    <div class="doc doc-contents ">


      <p>Tool to retrieve public web data for grounding, powered by
Google Search.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2024</span>
<span class="normal">2025</span>
<span class="normal">2026</span>
<span class="normal">2027</span>
<span class="normal">2028</span>
<span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">GoogleSearchRetrieval</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Tool to retrieve public web data for grounding, powered by</span>
<span class="sd">    Google Search.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes a Google Search Retrieval tool.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_raw_google_search_retrieval</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">GoogleSearchRetrieval</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="vertexai.generative_models.grounding.GoogleSearchRetrieval.__init__" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">__init__</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div>

    <div class="doc doc-contents ">

            <details class="quote">
              <summary>Source code in <code>vertexai\generative_models\_generative_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2029</span>
<span class="normal">2030</span>
<span class="normal">2031</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes a Google Search Retrieval tool.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_raw_google_search_retrieval</span> <span class="o">=</span> <span class="n">gapic_tool_types</span><span class="o">.</span><span class="n">GoogleSearchRetrieval</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="vertexai.language_models"></a>
    <div class="doc doc-contents first">

      <p>Classes for working with language models.</p>



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.ChatMessage" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatMessage</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>A chat message.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.ChatMessage.content">content</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Content of the message.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.ChatMessage.author">author</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Author of the message.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ChatMessage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A chat message.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        content: Content of the message.</span>
<span class="sd">        author: Author of the message.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">content</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">author</span><span class="p">:</span> <span class="nb">str</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.ChatModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatModel</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatModelBase">_ChatModelBase</span></code>, <code><span title="vertexai.language_models._language_models._TunableChatModelMixin">_TunableChatModelMixin</span></code>, <code><span title="vertexai.language_models._language_models._RlhfTunableModelMixin">_RlhfTunableModelMixin</span></code></p>


      <p>ChatModel represents a language model that is capable of chat.</p>
<p>Examples::</p>
<div class="language-text highlight"><pre><span></span><code>chat_model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)

chat = chat_model.start_chat(
    context=&quot;My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.&quot;,
    examples=[
        InputOutputTextPair(
            input_text=&quot;Who do you work for?&quot;,
            output_text=&quot;I work for Ned.&quot;,
        ),
        InputOutputTextPair(
            input_text=&quot;What do I like?&quot;,
            output_text=&quot;Ned likes watching movies.&quot;,
        ),
    ],
    temperature=0.3,
)

chat.send_message(&quot;Do you know any cool events this weekend?&quot;)
</code></pre></div>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ChatModel</span><span class="p">(</span><span class="n">_ChatModelBase</span><span class="p">,</span> <span class="n">_TunableChatModelMixin</span><span class="p">,</span> <span class="n">_RlhfTunableModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatModel represents a language model that is capable of chat.</span>

<span class="sd">    Examples::</span>

<span class="sd">        chat_model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)</span>

<span class="sd">        chat = chat_model.start_chat(</span>
<span class="sd">            context=&quot;My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.&quot;,</span>
<span class="sd">            examples=[</span>
<span class="sd">                InputOutputTextPair(</span>
<span class="sd">                    input_text=&quot;Who do you work for?&quot;,</span>
<span class="sd">                    output_text=&quot;I work for Ned.&quot;,</span>
<span class="sd">                ),</span>
<span class="sd">                InputOutputTextPair(</span>
<span class="sd">                    input_text=&quot;What do I like?&quot;,</span>
<span class="sd">                    output_text=&quot;Ned likes watching movies.&quot;,</span>
<span class="sd">                ),</span>
<span class="sd">            ],</span>
<span class="sd">            temperature=0.3,</span>
<span class="sd">        )</span>

<span class="sd">        chat.send_message(&quot;Do you know any cool events this weekend?&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/chat_generation_1.0.0.yaml&quot;</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.ChatSession" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatSession</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatSessionBase">_ChatSessionBase</span></code></p>


      <p>ChatSession represents a chat session with a language model.</p>
<p>Within a chat session, the model keeps context and remembers the previous conversation.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ChatSession</span><span class="p">(</span><span class="n">_ChatSessionBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatSession represents a chat session with a language model.</span>

<span class="sd">    Within a chat session, the model keeps context and remembers the previous conversation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ChatModel</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.CodeChatModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">CodeChatModel</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatModelBase">_ChatModelBase</span></code>, <code><span title="vertexai.language_models._language_models._TunableChatModelMixin">_TunableChatModelMixin</span></code></p>


      <p>CodeChatModel represents a model that is capable of completing code.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <p>code_chat_model = CodeChatModel.from_pretrained("codechat-bison@001")</p>
<p>code_chat = code_chat_model.start_chat(
    context="I'm writing a large-scale enterprise application.",
    max_output_tokens=128,
    temperature=0.2,
)</p>
<p>code_chat.send_message("Please help write a function to calculate the min of two numbers")</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CodeChatModel</span><span class="p">(</span><span class="n">_ChatModelBase</span><span class="p">,</span> <span class="n">_TunableChatModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CodeChatModel represents a model that is capable of completing code.</span>

<span class="sd">    Examples:</span>
<span class="sd">        code_chat_model = CodeChatModel.from_pretrained(&quot;codechat-bison@001&quot;)</span>

<span class="sd">        code_chat = code_chat_model.start_chat(</span>
<span class="sd">            context=&quot;I&#39;m writing a large-scale enterprise application.&quot;,</span>
<span class="sd">            max_output_tokens=128,</span>
<span class="sd">            temperature=0.2,</span>
<span class="sd">        )</span>

<span class="sd">        code_chat.send_message(&quot;Please help write a function to calculate the min of two numbers&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/codechat_generation_1.0.0.yaml&quot;</span>

    <span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;CodeChatSession&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the code chat model.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">                For example, you can use context to specify words the model can or</span>
<span class="sd">                cannot use, topics to focus on or avoid, or the response format or style.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `ChatSession` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">CodeChatSession</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatModel.start_chat" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">start_chat</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">start_chat</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CodeChatSession</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Starts a chat session with the code chat model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Context shapes how the model responds throughout the conversation.
For example, you can use context to specify words the model can or
cannot use, topics to focus on or avoid, or the response format or style.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.CodeChatSession" href="../vertexai/language_models/#vertexai.language_models.CodeChatSession">CodeChatSession</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">ChatSession</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;CodeChatSession&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the code chat model.</span>

<span class="sd">    Args:</span>
<span class="sd">        context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">            For example, you can use context to specify words the model can or</span>
<span class="sd">            cannot use, topics to focus on or avoid, or the response format or style.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `ChatSession` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">CodeChatSession</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.CodeChatSession" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">CodeChatSession</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatSessionBase">_ChatSessionBase</span></code></p>


      <p>CodeChatSession represents a chat session with code chat language model.</p>
<p>Within a code chat session, the model keeps context and remembers the previous converstion.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CodeChatSession</span><span class="p">(</span><span class="n">_ChatSessionBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CodeChatSession represents a chat session with code chat language model.</span>

<span class="sd">    Within a code chat session, the model keeps context and remembers the previous converstion.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">CodeChatModel</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the code chat model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">                Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">                 Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the code chat model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">                Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">                 Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_async</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming_async</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiCandidateTextGenerationResponse</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the code chat model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].
Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].
 Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the code chat model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">             Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">        text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiCandidateTextGenerationResponse</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the code chat model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].
Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].
 Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the code chat model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">             Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">        text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_async</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message_streaming" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="../vertexai/language_models/#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="../vertexai/language_models/#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message_streaming_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming_async</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="../vertexai/language_models/#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="../vertexai/language_models/#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming_async</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.InputOutputTextPair" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">InputOutputTextPair</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>InputOutputTextPair represents a pair of input and output texts.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">InputOutputTextPair</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;InputOutputTextPair represents a pair of input and output texts.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">output_text</span><span class="p">:</span> <span class="nb">str</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.TextEmbedding" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextEmbedding</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>Text embedding vector and statistics.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextEmbedding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Text embedding vector and statistics.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">statistics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextEmbeddingStatistics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_parse_text_embedding_response</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">prediction_response</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">,</span> <span class="n">prediction_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TextEmbedding&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a `TextEmbedding` object from a prediction.</span>

<span class="sd">        Args:</span>
<span class="sd">            prediction_response: `aiplatform.models.Prediction` object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `TextEmbedding` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">[</span><span class="n">prediction_index</span><span class="p">]</span>
        <span class="n">is_prediction_from_pretrained_models</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">prediction</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Mapping</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_prediction_from_pretrained_models</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;embeddings&quot;</span><span class="p">]</span>
            <span class="n">embedding_stats</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;statistics&quot;</span><span class="p">]</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
                <span class="n">values</span><span class="o">=</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">],</span>
                <span class="n">statistics</span><span class="o">=</span><span class="n">TextEmbeddingStatistics</span><span class="p">(</span>
                    <span class="n">token_count</span><span class="o">=</span><span class="n">embedding_stats</span><span class="p">[</span><span class="s2">&quot;token_count&quot;</span><span class="p">],</span>
                    <span class="n">truncated</span><span class="o">=</span><span class="n">embedding_stats</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">],</span>
                <span class="p">),</span>
                <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.TextEmbeddingInput" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextEmbeddingInput</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>Structural text embedding input.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.TextEmbeddingInput.text">text</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The main text content to embed.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.TextEmbeddingInput.task_type">task_type</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The name of the downstream task the embeddings will be used for.
Valid values:
RETRIEVAL_QUERY
    Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT
    Specifies the given text is a document from the corpus being searched.
SEMANTIC_SIMILARITY
    Specifies the given text will be used for STS.
CLASSIFICATION
    Specifies that the given text will be classified.
CLUSTERING
    Specifies that the embeddings will be used for clustering.
QUESTION_ANSWERING
    Specifies that the embeddings will be used for question answering.
FACT_VERIFICATION
    Specifies that the embeddings will be used for fact verification.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.TextEmbeddingInput.title">title</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Optional identifier of the text content.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextEmbeddingInput</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Structural text embedding input.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        text: The main text content to embed.</span>
<span class="sd">        task_type: The name of the downstream task the embeddings will be used for.</span>
<span class="sd">            Valid values:</span>
<span class="sd">            RETRIEVAL_QUERY</span>
<span class="sd">                Specifies the given text is a query in a search/retrieval setting.</span>
<span class="sd">            RETRIEVAL_DOCUMENT</span>
<span class="sd">                Specifies the given text is a document from the corpus being searched.</span>
<span class="sd">            SEMANTIC_SIMILARITY</span>
<span class="sd">                Specifies the given text will be used for STS.</span>
<span class="sd">            CLASSIFICATION</span>
<span class="sd">                Specifies that the given text will be classified.</span>
<span class="sd">            CLUSTERING</span>
<span class="sd">                Specifies that the embeddings will be used for clustering.</span>
<span class="sd">            QUESTION_ANSWERING</span>
<span class="sd">                Specifies that the embeddings will be used for question answering.</span>
<span class="sd">            FACT_VERIFICATION</span>
<span class="sd">                Specifies that the embeddings will be used for fact verification.</span>
<span class="sd">        title: Optional identifier of the text content.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">task_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">title</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.TextGenerationResponse" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextGenerationResponse</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>TextGenerationResponse represents a response of a language model.
Attributes:
    text: The generated text
    is_blocked: Whether the the request was blocked.
    errors: The error codes indicate why the response was blocked.
        Learn more information about safety errors here:
        this documentation <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors">https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors</a>
    safety_attributes: Scores for safety attributes.
        Learn more about the safety attributes here:
        <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions">https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</a>
    grounding_metadata: Metadata for grounding.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextGenerationResponse</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TextGenerationResponse represents a response of a language model.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        text: The generated text</span>
<span class="sd">        is_blocked: Whether the the request was blocked.</span>
<span class="sd">        errors: The error codes indicate why the response was blocked.</span>
<span class="sd">            Learn more information about safety errors here:</span>
<span class="sd">            this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors</span>
<span class="sd">        safety_attributes: Scores for safety attributes.</span>
<span class="sd">            Learn more about the safety attributes here:</span>
<span class="sd">            https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</span>
<span class="sd">        grounding_metadata: Metadata for grounding.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">is_blocked</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">errors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>
    <span class="n">safety_attributes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
    <span class="n">grounding_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GroundingMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text</span>
        <span class="c1"># Falling back to the full representation</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">grounding_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="s2">&quot;TextGenerationResponse(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;text=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, is_blocked=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_blocked</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, errors=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, safety_attributes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">safety_attributes</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, grounding_metadata=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grounding_metadata</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;)&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="s2">&quot;TextGenerationResponse(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;text=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, is_blocked=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_blocked</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, errors=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, safety_attributes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">safety_attributes</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;)&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">raw_prediction_response</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Raw prediction response.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prediction_response</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">raw_prediction_response</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">raw_prediction_response</span><span class="p">:</span> <span class="n">Prediction</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Raw prediction response.</p>
    </div>

</div>





  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.expand", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.081f42fc.min.js"></script>
      
    
  </body>
</html>