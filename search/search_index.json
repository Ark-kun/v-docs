{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vertex Generative AI SDK for Python","text":"<p>The Vertex Generative AI SDK helps developers use Google's generative AI Gemini models and PaLM language models to build AI-powered features and applications. The SDKs support use cases like the following:</p> <ul> <li>Generate text from texts, images and videos (multimodal generation)</li> <li>Build stateful multi-turn conversations (chat)</li> <li>Function calling</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the google-cloud-aiplatform Python package, run the following command:</p> <pre><code>pip3 install --upgrade --user \"google-cloud-aiplatform&gt;=1.38\"\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>For detailed instructions, see quickstart and Introduction to multimodal classes in the Vertex AI SDK.</p>"},{"location":"#imports","title":"Imports:","text":"<pre><code>from vertexai.generative_models import GenerativeModel, Image, Content, Part, Tool, FunctionDeclaration, GenerationConfig\n</code></pre>"},{"location":"#basic-generation","title":"Basic generation:","text":"<pre><code>from vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\"Why is sky blue?\"))\n</code></pre>"},{"location":"#using-images-and-videos","title":"Using images and videos","text":"<pre><code>from vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-pro-vision\")\n\n# Local image\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_model.generate_content([\"What is shown in this image?\", image]))\n\n# Image from Cloud Storage\nimage_part = generative_models.Part.from_uri(\"gs://download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg\", mime_type=\"image/jpeg\")\nprint(vision_model.generate_content([image_part, \"Describe this image?\"]))\n\n# Text and video\nvideo_part = Part.from_uri(\"gs://cloud-samples-data/video/animals.mp4\", mime_type=\"video/mp4\")\nprint(vision_model.generate_content([\"What is in the video? \", video_part]))\n</code></pre>"},{"location":"#chat","title":"Chat","text":"<pre><code>from vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-ultra-vision\")\nvision_chat = vision_model.start_chat()\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_chat.send_message([\"I like this image.\", image]))\nprint(vision_chat.send_message(\"What things do I like?.\"))\n</code></pre>"},{"location":"#system-instructions","title":"System instructions","text":"<pre><code>from vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\n    \"gemini-1.0-pro\",\n    system_instruction=[\n        \"Talk like a pirate.\",\n        \"Don't use rude words.\",\n    ],\n)\nprint(model.generate_content(\"Why is sky blue?\"))\n</code></pre>"},{"location":"#function-calling","title":"Function calling","text":"<pre><code># First, create tools that the model is can use to answer your questions.\n# Describe a function by specifying it's schema (JsonSchema format)\nget_current_weather_func = generative_models.FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"celsius\",\n                    \"fahrenheit\",\n                ]\n            }\n        },\n        \"required\": [\n            \"location\"\n        ]\n    },\n)\n# Tool is a collection of related functions\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n\n# Use tools in chat:\nmodel = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\n# Send a message to the model. The model will respond with a function call.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\n# Then send a function response to the model. The model will use it to answer.\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather\": \"super nice\"},\n        }\n    ),\n))\n</code></pre>"},{"location":"#automatic-function-calling","title":"Automatic Function calling","text":"<pre><code>from vertexai.preview.generative_models import GenerativeModel, Tool, FunctionDeclaration, AutomaticFunctionCallingResponder\n\n# First, create functions that the model can use to answer your questions.\ndef get_current_weather(location: str, unit: str = \"centigrade\"):\n    \"\"\"Gets weather in the specified location.\n\n    Args:\n        location: The location for which to get the weather.\n        unit: Optional. Temperature unit. Can be Centigrade or Fahrenheit. Defaults to Centigrade.\n    \"\"\"\n    return dict(\n        location=location,\n        unit=unit,\n        weather=\"Super nice, but maybe a bit hot.\",\n    )\n\n# Infer function schema\nget_current_weather_func = FunctionDeclaration.from_func(get_current_weather)\n# Tool is a collection of related functions\nweather_tool = Tool(\n    function_declarations=[get_current_weather_func],\n)\n\n# Use tools in chat:\nmodel = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\n\n# Activate automatic function calling:\nafc_responder = AutomaticFunctionCallingResponder(\n    # Optional:\n    max_automatic_function_calls=5,\n)\nchat = model.start_chat(responder=afc_responder)\n# Send a message to the model. The model will respond with a function call.\n# The SDK will automatically call the requested function and respond to the model.\n# The model will use the function call response to answer the original question.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>You can find complete documentation for the Vertex AI SDKs and the Gemini model in the Google Cloud documentation</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See Contributing for more information on contributing to the Vertex AI Python SDK.</p>"},{"location":"#license","title":"License","text":"<p>The contents of this repository are licensed under the Apache License, version 2.0.</p>"},{"location":"reference/","title":"Reference","text":"<p>Classes for working with the Gemini models.</p> <p>Classes for working with language models.</p>"},{"location":"reference/#vertexai.generative_models.Candidate","title":"Candidate","text":"<p>A response candidate generated by the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Candidate:\n    \"\"\"A response candidate generated by the model.\"\"\"\n\n    def __init__(self):\n        raw_candidate = gapic_content_types.Candidate()\n        self._raw_candidate = raw_candidate\n\n    @classmethod\n    def _from_gapic(cls, raw_candidate: gapic_content_types.Candidate) -&gt; \"Candidate\":\n        candidate = cls()\n        candidate._raw_candidate = raw_candidate\n        return candidate\n\n    @classmethod\n    def from_dict(cls, candidate_dict: Dict[str, Any]) -&gt; \"Candidate\":\n        raw_candidate = gapic_content_types.Candidate()\n        json_format.ParseDict(candidate_dict, raw_candidate._pb)\n        return cls._from_gapic(raw_candidate=raw_candidate)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_candidate)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_candidate.__repr__()\n\n    @property\n    def content(self) -&gt; \"Content\":\n        return Content._from_gapic(\n            raw_content=self._raw_candidate.content,\n        )\n\n    @property\n    def finish_reason(self) -&gt; gapic_content_types.Candidate.FinishReason:\n        return self._raw_candidate.finish_reason\n\n    @property\n    def finish_message(self) -&gt; str:\n        return self._raw_candidate.finish_message\n\n    @property\n    def index(self) -&gt; int:\n        return self._raw_candidate.index\n\n    @property\n    def safety_ratings(self) -&gt; Sequence[gapic_content_types.SafetyRating]:\n        return self._raw_candidate.safety_ratings\n\n    @property\n    def citation_metadata(self) -&gt; gapic_content_types.CitationMetadata:\n        return self._raw_candidate.citation_metadata\n\n    @property\n    def grounding_metadata(self) -&gt; gapic_content_types.GroundingMetadata:\n        return self._raw_candidate.grounding_metadata\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        try:\n            return self.content.text\n        except (ValueError, AttributeError) as e:\n            # Enrich the error message with the whole Candidate.\n            # The Content object does not have full information.\n            raise ValueError(\n                \"Cannot get the Candidate text.\\n\"\n                f\"{e}\\n\"\n                \"Candidate:\\n\" + _dict_to_pretty_string(self.to_dict())\n            ) from e\n\n    @property\n    def function_calls(self) -&gt; Sequence[gapic_tool_types.FunctionCall]:\n        if not self.content or not self.content.parts:\n            return []\n        return [\n            part.function_call\n            for part in self.content.parts\n            if part and part.function_call\n        ]\n</code></pre>"},{"location":"reference/#vertexai.generative_models.ChatSession","title":"ChatSession","text":"<p>Chat session holds the chat history.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class ChatSession:\n    \"\"\"Chat session holds the chat history.\"\"\"\n\n    _USER_ROLE = \"user\"\n    _MODEL_ROLE = \"model\"\n\n    def __init__(\n        self,\n        model: _GenerativeModel,\n        *,\n        history: Optional[List[\"Content\"]] = None,\n        response_validation: bool = True,\n    ):\n        if history:\n            if not all(isinstance(item, Content) for item in history):\n                raise ValueError(\"history must be a list of Content objects.\")\n\n        self._model = model\n        self._history = history or []\n        self._response_validator = _validate_response if response_validation else None\n        # _responder is currently only set by PreviewChatSession\n        self._responder: Optional[\"AutomaticFunctionCallingResponder\"] = None\n\n    @property\n    def history(self) -&gt; List[\"Content\"]:\n        return self._history\n\n    def send_message(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n        stream: bool = False,\n    ) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"]]:\n        \"\"\"Generates content.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n            stream: Whether to stream the response.\n\n        Returns:\n            A single GenerationResponse object if stream == False\n            A stream of GenerationResponse objects if stream == True\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        if stream:\n            return self._send_message_streaming(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n        else:\n            return self._send_message(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n\n    def send_message_async(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n        stream: bool = False,\n    ) -&gt; Union[\n        Awaitable[\"GenerationResponse\"],\n        Awaitable[AsyncIterable[\"GenerationResponse\"]],\n    ]:\n        \"\"\"Generates content asynchronously.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n            stream: Whether to stream the response.\n\n        Returns:\n            An awaitable for a single GenerationResponse object if stream == False\n            An awaitable for a stream of GenerationResponse objects if stream == True\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        if stream:\n            return self._send_message_streaming_async(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n        else:\n            return self._send_message_async(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n\n    def _send_message(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; \"GenerationResponse\":\n        \"\"\"Generates content.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Returns:\n            A single GenerationResponse object\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        history_delta = [request_message]\n\n        message_responder = (\n            self._responder._create_responder_for_message(\n                tools=tools or self._model._tools\n            )\n            if self._responder\n            else None\n        )\n\n        while True:\n            request_history = self._history + history_delta\n            response = self._model._generate_content(\n                contents=request_history,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n            # By default we're not adding incomplete interactions to history.\n            if self._response_validator is not None:\n                self._response_validator(\n                    response=response,\n                    request_contents=request_history,\n                    response_chunks=[response],\n                )\n\n            # Adding the request and the first response candidate to history\n            response_message = response.candidates[0].content\n            # Response role is NOT set by the model.\n            response_message.role = self._MODEL_ROLE\n            history_delta.append(response_message)\n\n            auto_responder_content = (\n                message_responder.respond_to_model_response(response=response)\n                if message_responder\n                else None\n            )\n            if auto_responder_content:\n                auto_responder_content.role = self._USER_ROLE\n                history_delta.append(auto_responder_content)\n            else:\n                break\n\n        self._history.extend(history_delta)\n        return response\n\n    async def _send_message_async(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; \"GenerationResponse\":\n        \"\"\"Generates content asynchronously.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Returns:\n            An awaitable for a single GenerationResponse object\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        request_history = list(self._history)\n        request_history.append(request_message)\n\n        response = await self._model._generate_content_async(\n            contents=request_history,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n        # By default we're not adding incomplete interactions to history.\n        if self._response_validator is not None:\n            self._response_validator(\n                response=response,\n                request_contents=request_history,\n                response_chunks=[response],\n            )\n\n        # Adding the request and the first response candidate to history\n        response_message = response.candidates[0].content\n        # Response role is NOT set by the model.\n        response_message.role = self._MODEL_ROLE\n        self._history.append(request_message)\n        self._history.append(response_message)\n        return response\n\n    def _send_message_streaming(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; Iterable[\"GenerationResponse\"]:\n        \"\"\"Generates content.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Yields:\n            A stream of GenerationResponse objects\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        request_history = list(self._history)\n        request_history.append(request_message)\n\n        stream = self._model._generate_content_streaming(\n            contents=request_history,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n        chunks = []\n        full_response = None\n        for chunk in stream:\n            chunks.append(chunk)\n            # By default we're not adding incomplete interactions to history.\n            if self._response_validator is not None:\n                self._response_validator(\n                    response=chunk,\n                    request_contents=request_history,\n                    response_chunks=chunks,\n                )\n            if full_response:\n                _append_response(full_response, chunk)\n            else:\n                full_response = chunk\n            yield chunk\n        if not full_response:\n            return\n\n        # Adding the request and the first response candidate to history\n        response_message = full_response.candidates[0].content\n        # Response role is NOT set by the model.\n        response_message.role = self._MODEL_ROLE\n        self._history.append(request_message)\n        self._history.append(response_message)\n\n    async def _send_message_streaming_async(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; AsyncIterable[\"GenerationResponse\"]:\n        \"\"\"Generates content asynchronously.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Returns:\n            An awaitable for a stream of GenerationResponse objects\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        request_history = list(self._history)\n        request_history.append(request_message)\n\n        stream = await self._model._generate_content_streaming_async(\n            contents=request_history,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n\n        async def async_generator():\n            chunks = []\n            full_response = None\n            async for chunk in stream:\n                chunks.append(chunk)\n                # By default we're not adding incomplete interactions to history.\n                if self._response_validator is not None:\n                    self._response_validator(\n                        response=chunk,\n                        request_contents=request_history,\n                        response_chunks=chunks,\n                    )\n                if full_response:\n                    _append_response(full_response, chunk)\n                else:\n                    full_response = chunk\n                yield chunk\n            if not full_response:\n                return\n            # Adding the request and the first response candidate to history\n            response_message = full_response.candidates[0].content\n            # Response role is NOT set by the model.\n            response_message.role = self._MODEL_ROLE\n            self._history.append(request_message)\n            self._history.append(response_message)\n\n        return async_generator()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.ChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, Iterable[GenerationResponse]\n]\n</code></pre> <p>Generates content.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"]]:\n    \"\"\"Generates content.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        A single GenerationResponse object if stream == False\n        A stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"reference/#vertexai.generative_models.ChatSession.send_message_async","title":"send_message_async","text":"<pre><code>send_message_async(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    Awaitable[GenerationResponse],\n    Awaitable[AsyncIterable[GenerationResponse]],\n]\n</code></pre> <p>Generates content asynchronously.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a single GenerationResponse object if stream == False</p> <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message_async(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\n    Awaitable[\"GenerationResponse\"],\n    Awaitable[AsyncIterable[\"GenerationResponse\"]],\n]:\n    \"\"\"Generates content asynchronously.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        An awaitable for a single GenerationResponse object if stream == False\n        An awaitable for a stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"reference/#vertexai.generative_models.Content","title":"Content","text":"<p>The multi-part content of a message.</p> Usage <pre><code>response = model.generate_content(contents=[\n    Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n])\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Content:\n    r\"\"\"The multi-part content of a message.\n\n    Usage:\n        ```\n        response = model.generate_content(contents=[\n            Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n        ])\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        parts: List[\"Part\"] = None,\n        role: Optional[str] = None,\n    ):\n        raw_parts = [part._raw_part for part in parts or []]\n        self._raw_content = gapic_content_types.Content(parts=raw_parts, role=role)\n\n    @classmethod\n    def _from_gapic(cls, raw_content: gapic_content_types.Content) -&gt; \"Content\":\n        content = cls()\n        content._raw_content = raw_content\n        return content\n\n    @classmethod\n    def from_dict(cls, content_dict: Dict[str, Any]) -&gt; \"Content\":\n        raw_content = gapic_content_types.Content()\n        json_format.ParseDict(content_dict, raw_content._pb)\n        return cls._from_gapic(raw_content=raw_content)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_content)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_content.__repr__()\n\n    @property\n    def parts(self) -&gt; List[\"Part\"]:\n        return [\n            Part._from_gapic(raw_part=raw_part) for raw_part in self._raw_content.parts\n        ]\n\n    @property\n    def role(self) -&gt; str:\n        return self._raw_content.role\n\n    @role.setter\n    def role(self, role: str) -&gt; None:\n        self._raw_content.role = role\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        if len(self.parts) &gt; 1:\n            raise ValueError(\"Multiple content parts are not supported.\")\n        if not self.parts:\n            raise ValueError(\n                \"Response candidate content has no parts (and thus no text).\"\n                \" The candidate is likely blocked by the safety filters.\\n\"\n                \"Content:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        return self.parts[0].text\n</code></pre>"},{"location":"reference/#vertexai.generative_models.FunctionDeclaration","title":"FunctionDeclaration","text":"<p>A representation of a function declaration.</p> Usage <p>Create function declaration and tool: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"celsius\",\n                    \"fahrenheit\",\n                ]\n            }\n        },\n        \"required\": [\n            \"location\"\n        ]\n    },\n)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class FunctionDeclaration:\n    r\"\"\"A representation of a function declaration.\n\n    Usage:\n        Create function declaration and tool:\n        ```\n        get_current_weather_func = generative_models.FunctionDeclaration(\n            name=\"get_current_weather\",\n            description=\"Get the current weather in a given location\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\n                            \"celsius\",\n                            \"fahrenheit\",\n                        ]\n                    }\n                },\n                \"required\": [\n                    \"location\"\n                ]\n            },\n        )\n        weather_tool = generative_models.Tool(\n            function_declarations=[get_current_weather_func],\n        )\n        ```\n        Use tool in `GenerativeModel.generate_content`:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\n            \"What is the weather like in Boston?\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        ))\n        ```\n        Use tool in chat:\n        ```\n        model = GenerativeModel(\n            \"gemini-pro\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        )\n        chat = model.start_chat()\n        print(chat.send_message(\"What is the weather like in Boston?\"))\n        print(chat.send_message(\n            Part.from_function_response(\n                name=\"get_current_weather\",\n                response={\n                    \"content\": {\"weather_there\": \"super nice\"},\n                }\n            ),\n        ))\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        parameters: Dict[str, Any],\n        description: Optional[str] = None,\n    ):\n        \"\"\"Constructs a FunctionDeclaration.\n\n        Args:\n            name: The name of the function that the model can call.\n            parameters: Describes the parameters to this function in JSON Schema Object format.\n            description: Description and purpose of the function.\n                Model uses it to decide how and whether to call the function.\n        \"\"\"\n        gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n        self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n            name=name, description=description, parameters=raw_schema\n        )\n\n    @classmethod\n    def from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n        return CallableFunctionDeclaration.from_func(func)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_function_declaration)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_function_declaration.__repr__()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.FunctionDeclaration.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the function that the model can call.</p> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Describes the parameters to this function in JSON Schema Object format.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>description</code> <p>Description and purpose of the function. Model uses it to decide how and whether to call the function.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None,\n):\n    \"\"\"Constructs a FunctionDeclaration.\n\n    Args:\n        name: The name of the function that the model can call.\n        parameters: Describes the parameters to this function in JSON Schema Object format.\n        description: Description and purpose of the function.\n            Model uses it to decide how and whether to call the function.\n    \"\"\"\n    gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n    raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n        name=name, description=description, parameters=raw_schema\n    )\n</code></pre>"},{"location":"reference/#vertexai.generative_models.GenerationConfig","title":"GenerationConfig","text":"<p>Parameters for the generation.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GenerationConfig:\n    \"\"\"Parameters for the generation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        candidate_count: Optional[int] = None,\n        max_output_tokens: Optional[int] = None,\n        stop_sequences: Optional[List[str]] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        response_mime_type: Optional[str] = None,\n        response_schema: Optional[Dict[str, Any]] = None,\n    ):\n        r\"\"\"Constructs a GenerationConfig object.\n\n        Args:\n            temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n            top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n            top_k: If specified, top-k sampling will be used.\n            candidate_count: Number of candidates to generate.\n            max_output_tokens: The maximum number of output tokens to generate per message.\n            stop_sequences: A list of stop sequences.\n            presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n                thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n            frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n                text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n            response_mime_type: Output response mimetype of the generated\n                candidate text. Supported mimetypes:\n\n                -  ``text/plain``: (default) Text output.\n                -  ``application/json``: JSON response in the candidates.\n\n                The model needs to be prompted to output the appropriate\n                response type, otherwise the behavior is undefined.\n            response_schema: Output response schema of the genreated candidate text. Only valid when\n                response_mime_type is application/json.\n\n        Usage:\n            ```\n            response = model.generate_content(\n                \"Why is sky blue?\",\n                generation_config=GenerationConfig(\n                    temperature=0.1,\n                    top_p=0.95,\n                    top_k=20,\n                    candidate_count=1,\n                    max_output_tokens=100,\n                    stop_sequences=[\"\\n\\n\\n\"],\n                )\n            )\n            ```\n        \"\"\"\n        if response_schema is None:\n            raw_schema = None\n        else:\n            gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n            raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n        self._raw_generation_config = gapic_content_types.GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            candidate_count=candidate_count,\n            max_output_tokens=max_output_tokens,\n            stop_sequences=stop_sequences,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            response_mime_type=response_mime_type,\n            response_schema=raw_schema,\n        )\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_generation_config: gapic_content_types.GenerationConfig,\n    ) -&gt; \"GenerationConfig\":\n        response = cls()\n        response._raw_generation_config = raw_generation_config\n        return response\n\n    @classmethod\n    def from_dict(cls, generation_config_dict: Dict[str, Any]) -&gt; \"GenerationConfig\":\n        raw_generation_config = gapic_content_types.GenerationConfig(\n            generation_config_dict\n        )\n        return cls._from_gapic(raw_generation_config=raw_generation_config)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_generation_config)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_generation_config.__repr__()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.GenerationConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>temperature</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>If specified, top-k sampling will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to generate.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>The maximum number of output tokens to generate per message.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>A list of stop sequences.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>response_mime_type</code> <p>Output response mimetype of the generated candidate text. Supported mimetypes:</p> <ul> <li><code>text/plain</code>: (default) Text output.</li> <li><code>application/json</code>: JSON response in the candidates.</li> </ul> <p>The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>response_schema</code> <p>Output response schema of the genreated candidate text. Only valid when response_mime_type is application/json.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Usage <pre><code>response = model.generate_content(\n    \"Why is sky blue?\",\n    generation_config=GenerationConfig(\n        temperature=0.1,\n        top_p=0.95,\n        top_k=20,\n        candidate_count=1,\n        max_output_tokens=100,\n        stop_sequences=[\"\\n\\n\\n\"],\n    )\n)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None,\n):\n    r\"\"\"Constructs a GenerationConfig object.\n\n    Args:\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n        top_k: If specified, top-k sampling will be used.\n        candidate_count: Number of candidates to generate.\n        max_output_tokens: The maximum number of output tokens to generate per message.\n        stop_sequences: A list of stop sequences.\n        presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n        frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n        response_mime_type: Output response mimetype of the generated\n            candidate text. Supported mimetypes:\n\n            -  ``text/plain``: (default) Text output.\n            -  ``application/json``: JSON response in the candidates.\n\n            The model needs to be prompted to output the appropriate\n            response type, otherwise the behavior is undefined.\n        response_schema: Output response schema of the genreated candidate text. Only valid when\n            response_mime_type is application/json.\n\n    Usage:\n        ```\n        response = model.generate_content(\n            \"Why is sky blue?\",\n            generation_config=GenerationConfig(\n                temperature=0.1,\n                top_p=0.95,\n                top_k=20,\n                candidate_count=1,\n                max_output_tokens=100,\n                stop_sequences=[\"\\n\\n\\n\"],\n            )\n        )\n        ```\n    \"\"\"\n    if response_schema is None:\n        raw_schema = None\n    else:\n        gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_generation_config = gapic_content_types.GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        candidate_count=candidate_count,\n        max_output_tokens=max_output_tokens,\n        stop_sequences=stop_sequences,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        response_mime_type=response_mime_type,\n        response_schema=raw_schema,\n    )\n</code></pre>"},{"location":"reference/#vertexai.generative_models.GenerationResponse","title":"GenerationResponse","text":"<p>The response from the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GenerationResponse:\n    \"\"\"The response from the model.\"\"\"\n\n    def __init__(self):\n        raw_response = gapic_prediction_service_types.GenerateContentResponse()\n        self._raw_response = raw_response\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_response: gapic_prediction_service_types.GenerateContentResponse,\n    ) -&gt; \"GenerationResponse\":\n        response = cls()\n        response._raw_response = raw_response\n        return response\n\n    @classmethod\n    def from_dict(cls, response_dict: Dict[str, Any]) -&gt; \"GenerationResponse\":\n        raw_response = gapic_prediction_service_types.GenerateContentResponse()\n        json_format.ParseDict(response_dict, raw_response._pb)\n        return cls._from_gapic(raw_response=raw_response)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_response)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_response.__repr__()\n\n    @property\n    def candidates(self) -&gt; List[\"Candidate\"]:\n        return [\n            Candidate._from_gapic(raw_candidate=raw_candidate)\n            for raw_candidate in self._raw_response.candidates\n        ]\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        if len(self.candidates) &gt; 1:\n            raise ValueError(\n                \"The response has multiple candidates.\"\n                \" Use `response.candidate[i].text` to get text of a particular candidate.\"\n            )\n        if not self.candidates:\n            raise ValueError(\n                \"Response has no candidates (and thus no text).\"\n                \" The response is likely blocked by the safety filters.\\n\"\n                \"Response:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        try:\n            return self.candidates[0].text\n        except (ValueError, AttributeError) as e:\n            # Enrich the error message with the whole Response.\n            # The Candidate object does not have full information.\n            raise ValueError(\n                \"Cannot get the response text.\\n\"\n                f\"{e}\\n\"\n                \"Response:\\n\" + _dict_to_pretty_string(self.to_dict())\n            ) from e\n\n    @property\n    def prompt_feedback(\n        self,\n    ) -&gt; gapic_prediction_service_types.GenerateContentResponse.PromptFeedback:\n        return self._raw_response.prompt_feedback\n\n    @property\n    def usage_metadata(\n        self,\n    ) -&gt; gapic_prediction_service_types.GenerateContentResponse.UsageMetadata:\n        return self._raw_response.usage_metadata\n</code></pre>"},{"location":"reference/#vertexai.generative_models.Image","title":"Image","text":"<p>The image that can be sent to a generative model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Image:\n    \"\"\"The image that can be sent to a generative model.\"\"\"\n\n    _image_bytes: bytes\n    _loaded_image: Optional[\"PIL_Image.Image\"] = None\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"Image\":\n        \"\"\"Loads image from file.\n\n        Args:\n            location: Local path from where to load the image.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        image_bytes = pathlib.Path(location).read_bytes()\n        image = Image()\n        image._image_bytes = image_bytes\n        return image\n\n    @staticmethod\n    def from_bytes(data: bytes) -&gt; \"Image\":\n        \"\"\"Loads image from image bytes.\n\n        Args:\n            data: Image bytes.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        image = Image()\n        image._image_bytes = data\n        return image\n\n    @property\n    def _pil_image(self) -&gt; \"PIL_Image.Image\":\n        if self._loaded_image is None:\n            if not PIL_Image:\n                raise RuntimeError(\n                    \"The PIL module is not available. Please install the Pillow package.\"\n                )\n            self._loaded_image = PIL_Image.open(io.BytesIO(self._image_bytes))\n        return self._loaded_image\n\n    @property\n    def _mime_type(self) -&gt; str:\n        \"\"\"Returns the MIME type of the image.\"\"\"\n        if PIL_Image:\n            return _FORMAT_TO_MIME_TYPE[self._pil_image.format.lower()]\n        else:\n            # Fall back to jpeg\n            return \"image/jpeg\"\n\n    @property\n    def data(self) -&gt; bytes:\n        \"\"\"Returns the image data.\"\"\"\n        return self._image_bytes\n\n    def _repr_png_(self):\n        return self._pil_image._repr_png_()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.Image.data","title":"data  <code>property</code>","text":"<pre><code>data: bytes\n</code></pre> <p>Returns the image data.</p>"},{"location":"reference/#vertexai.generative_models.Image.from_bytes","title":"from_bytes  <code>staticmethod</code>","text":"<pre><code>from_bytes(data: bytes) -&gt; Image\n</code></pre> <p>Loads image from image bytes.</p> PARAMETER DESCRIPTION <code>data</code> <p>Image bytes.</p> <p> TYPE: <code>bytes</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_bytes(data: bytes) -&gt; \"Image\":\n    \"\"\"Loads image from image bytes.\n\n    Args:\n        data: Image bytes.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image = Image()\n    image._image_bytes = data\n    return image\n</code></pre>"},{"location":"reference/#vertexai.generative_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from file.\n\n    Args:\n        location: Local path from where to load the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image()\n    image._image_bytes = image_bytes\n    return image\n</code></pre>"},{"location":"reference/#vertexai.generative_models.Part","title":"Part","text":"<p>A part of a multi-part Content message.</p> Usage <pre><code>text_part = Part.from_text(\"Why is sky blue?\")\nimage_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\nvideo_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\nfunction_response_part = Part.from_function_response(\n    name=\"get_current_weather\",\n    response={\n        \"content\": {\"weather_there\": \"super nice\"},\n    }\n)\n\nresponse1 = model.generate_content([text_part, image_part])\nresponse2 = model.generate_content(video_part)\nresponse3 = chat.send_message(function_response_part)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Part:\n    r\"\"\"A part of a multi-part Content message.\n\n    Usage:\n        ```\n        text_part = Part.from_text(\"Why is sky blue?\")\n        image_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\n        video_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\n        function_response_part = Part.from_function_response(\n            name=\"get_current_weather\",\n            response={\n                \"content\": {\"weather_there\": \"super nice\"},\n            }\n        )\n\n        response1 = model.generate_content([text_part, image_part])\n        response2 = model.generate_content(video_part)\n        response3 = chat.send_message(function_response_part)\n        ```\n    \"\"\"\n\n    def __init__(self):\n        raw_part = gapic_content_types.Part()\n        self._raw_part = raw_part\n\n    @classmethod\n    def _from_gapic(cls, raw_part: gapic_content_types.Part) -&gt; \"Part\":\n        part = cls()\n        part._raw_part = raw_part\n        return part\n\n    @classmethod\n    def from_dict(cls, part_dict: Dict[str, Any]) -&gt; \"Part\":\n        raw_part = gapic_content_types.Part()\n        json_format.ParseDict(part_dict, raw_part._pb)\n        return cls._from_gapic(raw_part=raw_part)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_part.__repr__()\n\n    @staticmethod\n    def from_data(data: bytes, mime_type: str) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                inline_data=gapic_content_types.Blob(data=data, mime_type=mime_type)\n            )\n        )\n\n    @staticmethod\n    def from_uri(uri: str, mime_type: str) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                file_data=gapic_content_types.FileData(\n                    file_uri=uri, mime_type=mime_type\n                )\n            )\n        )\n\n    @staticmethod\n    def from_text(text: str) -&gt; \"Part\":\n        return Part._from_gapic(raw_part=gapic_content_types.Part(text=text))\n\n    @staticmethod\n    def from_image(image: \"Image\") -&gt; \"Part\":\n        return Part.from_data(data=image.data, mime_type=image._mime_type)\n\n    @staticmethod\n    def from_function_response(name: str, response: Dict[str, Any]) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                function_response=gapic_tool_types.FunctionResponse(\n                    name=name,\n                    response=response,\n                )\n            )\n        )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_part)\n\n    @property\n    def text(self) -&gt; str:\n        if \"text\" not in self._raw_part:\n            raise AttributeError(\n                \"Response candidate content part has no text.\\n\"\n                \"Part:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        return self._raw_part.text\n\n    @property\n    def mime_type(self) -&gt; Optional[str]:\n        part_type = self._raw_part._pb.WhichOneof(\"data\")\n        if part_type == \"inline_data\":\n            return self._raw_part.inline_data.mime_type\n        if part_type == \"file_data\":\n            return self._raw_part.file_data.mime_type\n        raise AttributeError(f\"Part has no mime_type.\\nPart:\\n{self.to_dict()}\")\n\n    @property\n    def inline_data(self) -&gt; gapic_content_types.Blob:\n        return self._raw_part.inline_data\n\n    @property\n    def file_data(self) -&gt; gapic_content_types.FileData:\n        return self._raw_part.file_data\n\n    @property\n    def function_call(self) -&gt; gapic_tool_types.FunctionCall:\n        return self._raw_part.function_call\n\n    @property\n    def function_response(self) -&gt; gapic_tool_types.FunctionResponse:\n        return self._raw_part.function_response\n\n    @property\n    def _image(self) -&gt; \"Image\":\n        return Image.from_bytes(data=self._raw_part.inline_data.data)\n</code></pre>"},{"location":"reference/#vertexai.generative_models.SafetySetting","title":"SafetySetting","text":"<p>Parameters for the generation.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class SafetySetting:\n    \"\"\"Parameters for the generation.\"\"\"\n\n    HarmCategory = gapic_content_types.HarmCategory\n    HarmBlockMethod = gapic_content_types.SafetySetting.HarmBlockMethod\n    HarmBlockThreshold = gapic_content_types.SafetySetting.HarmBlockThreshold\n\n    def __init__(\n        self,\n        *,\n        category: \"SafetySetting.HarmCategory\",\n        threshold: \"SafetySetting.HarmBlockThreshold\",\n        method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n    ):\n        r\"\"\"Safety settings.\n\n        Args:\n            category: Harm category.\n            threshold: The harm block threshold.\n            method: Specify if the threshold is used for probability or severity\n                score. If not specified, the threshold is used for probability\n                score.\n        \"\"\"\n        self._raw_safety_setting = gapic_content_types.SafetySetting(\n            category=category,\n            threshold=threshold,\n            method=method,\n        )\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_safety_setting: gapic_content_types.SafetySetting,\n    ) -&gt; \"SafetySetting\":\n        response = cls(\n            category=raw_safety_setting.category,\n            threshold=raw_safety_setting.threshold,\n        )\n        response._raw_safety_setting = raw_safety_setting\n        return response\n\n    @classmethod\n    def from_dict(cls, safety_setting_dict: Dict[str, Any]) -&gt; \"SafetySetting\":\n        raw_safety_setting = gapic_content_types.SafetySetting(safety_setting_dict)\n        return cls._from_gapic(raw_safety_setting=raw_safety_setting)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_safety_setting)\n\n    def __repr__(self):\n        return self._raw_safety_setting.__repr__()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.SafetySetting.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    category: SafetySetting.HarmCategory,\n    threshold: SafetySetting.HarmBlockThreshold,\n    method: Optional[SafetySetting.HarmBlockMethod] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>category</code> <p>Harm category.</p> <p> TYPE: <code>HarmCategory</code> </p> <code>threshold</code> <p>The harm block threshold.</p> <p> TYPE: <code>HarmBlockThreshold</code> </p> <code>method</code> <p>Specify if the threshold is used for probability or severity score. If not specified, the threshold is used for probability score.</p> <p> TYPE: <code>Optional[HarmBlockMethod]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    category: \"SafetySetting.HarmCategory\",\n    threshold: \"SafetySetting.HarmBlockThreshold\",\n    method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n):\n    r\"\"\"Safety settings.\n\n    Args:\n        category: Harm category.\n        threshold: The harm block threshold.\n        method: Specify if the threshold is used for probability or severity\n            score. If not specified, the threshold is used for probability\n            score.\n    \"\"\"\n    self._raw_safety_setting = gapic_content_types.SafetySetting(\n        category=category,\n        threshold=threshold,\n        method=method,\n    )\n</code></pre>"},{"location":"reference/#vertexai.generative_models.Tool","title":"Tool","text":"<p>A collection of functions that the model may use to generate response.</p> Usage <p>Create tool from function declarations: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(...)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Tool:\n    r\"\"\"A collection of functions that the model may use to generate response.\n\n    Usage:\n        Create tool from function declarations:\n        ```\n        get_current_weather_func = generative_models.FunctionDeclaration(...)\n        weather_tool = generative_models.Tool(\n            function_declarations=[get_current_weather_func],\n        )\n        ```\n        Use tool in `GenerativeModel.generate_content`:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\n            \"What is the weather like in Boston?\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        ))\n        ```\n        Use tool in chat:\n        ```\n        model = GenerativeModel(\n            \"gemini-pro\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        )\n        chat = model.start_chat()\n        print(chat.send_message(\"What is the weather like in Boston?\"))\n        print(chat.send_message(\n            Part.from_function_response(\n                name=\"get_current_weather\",\n                response={\n                    \"content\": {\"weather_there\": \"super nice\"},\n                }\n            ),\n        ))\n        ```\n    \"\"\"\n\n    _raw_tool: gapic_tool_types.Tool\n\n    def __init__(\n        self,\n        function_declarations: List[\"FunctionDeclaration\"],\n    ):\n        gapic_function_declarations = [\n            function_declaration._raw_function_declaration\n            for function_declaration in function_declarations\n        ]\n        self._raw_tool = gapic_tool_types.Tool(\n            function_declarations=gapic_function_declarations\n        )\n        callable_functions = {\n            function_declaration._raw_function_declaration.name: function_declaration\n            for function_declaration in function_declarations\n            if isinstance(function_declaration, CallableFunctionDeclaration)\n        }\n        self._callable_functions = callable_functions\n\n    @classmethod\n    def from_function_declarations(\n        cls,\n        function_declarations: List[\"FunctionDeclaration\"],\n    ) -&gt; \"Tool\":\n        return Tool(function_declarations=function_declarations)\n\n    @classmethod\n    def from_retrieval(\n        cls,\n        retrieval: Union[\"grounding.Retrieval\", \"rag.Retrieval\"],\n    ) -&gt; \"Tool\":\n        raw_tool = gapic_tool_types.Tool(retrieval=retrieval._raw_retrieval)\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    @classmethod\n    def from_google_search_retrieval(\n        cls,\n        google_search_retrieval: \"grounding.GoogleSearchRetrieval\",\n    ) -&gt; \"Tool\":\n        raw_tool = gapic_tool_types.Tool(\n            google_search_retrieval=google_search_retrieval._raw_google_search_retrieval\n        )\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_tool: gapic_tool_types.Tool,\n    ) -&gt; \"Tool\":\n        response = cls([])\n        response._raw_tool = raw_tool\n        return response\n\n    @classmethod\n    def from_dict(cls, tool_dict: Dict[str, Any]) -&gt; \"Tool\":\n        tool_dict = copy.deepcopy(tool_dict)\n        function_declarations = tool_dict[\"function_declarations\"]\n        for function_declaration in function_declarations:\n            function_declaration[\"parameters\"] = _convert_schema_dict_to_gapic(\n                function_declaration[\"parameters\"]\n            )\n        raw_tool = gapic_tool_types.Tool(tool_dict)\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_tool)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_tool.__repr__()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.grounding","title":"grounding","text":"<p>Grounding namespace.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class grounding:  # pylint: disable=invalid-name\n    \"\"\"Grounding namespace.\"\"\"\n\n    __module__ = \"vertexai.generative_models\"\n\n    def __init__(self):\n        raise RuntimeError(\"This class must not be instantiated.\")\n\n    class GoogleSearchRetrieval:\n        r\"\"\"Tool to retrieve public web data for grounding, powered by\n        Google Search.\n        \"\"\"\n\n        def __init__(self):\n            \"\"\"Initializes a Google Search Retrieval tool.\"\"\"\n            self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.grounding.GoogleSearchRetrieval","title":"GoogleSearchRetrieval","text":"<p>Tool to retrieve public web data for grounding, powered by Google Search.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GoogleSearchRetrieval:\n    r\"\"\"Tool to retrieve public web data for grounding, powered by\n    Google Search.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes a Google Search Retrieval tool.\"\"\"\n        self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval()\n</code></pre>"},{"location":"reference/#vertexai.generative_models.grounding.GoogleSearchRetrieval.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes a Google Search Retrieval tool.\"\"\"\n    self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval()\n</code></pre>"},{"location":"reference/#vertexai.language_models.ChatMessage","title":"ChatMessage  <code>dataclass</code>","text":"<p>A chat message.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>Content of the message.</p> <p> TYPE: <code>str</code> </p> <code>author</code> <p>Author of the message.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass ChatMessage:\n    \"\"\"A chat message.\n\n    Attributes:\n        content: Content of the message.\n        author: Author of the message.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    content: str\n    author: str\n</code></pre>"},{"location":"reference/#vertexai.language_models.ChatModel","title":"ChatModel","text":"<p>               Bases: <code>_ChatModelBase</code>, <code>_TunableChatModelMixin</code>, <code>_RlhfTunableModelMixin</code></p> <p>ChatModel represents a language model that is capable of chat.</p> <p>Examples::</p> <pre><code>chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n\nchat = chat_model.start_chat(\n    context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n    examples=[\n        InputOutputTextPair(\n            input_text=\"Who do you work for?\",\n            output_text=\"I work for Ned.\",\n        ),\n        InputOutputTextPair(\n            input_text=\"What do I like?\",\n            output_text=\"Ned likes watching movies.\",\n        ),\n    ],\n    temperature=0.3,\n)\n\nchat.send_message(\"Do you know any cool events this weekend?\")\n</code></pre> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class ChatModel(_ChatModelBase, _TunableChatModelMixin, _RlhfTunableModelMixin):\n    \"\"\"ChatModel represents a language model that is capable of chat.\n\n    Examples::\n\n        chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n\n        chat = chat_model.start_chat(\n            context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n            examples=[\n                InputOutputTextPair(\n                    input_text=\"Who do you work for?\",\n                    output_text=\"I work for Ned.\",\n                ),\n                InputOutputTextPair(\n                    input_text=\"What do I like?\",\n                    output_text=\"Ned likes watching movies.\",\n                ),\n            ],\n            temperature=0.3,\n        )\n\n        chat.send_message(\"Do you know any cool events this weekend?\")\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/chat_generation_1.0.0.yaml\"\n</code></pre>"},{"location":"reference/#vertexai.language_models.ChatSession","title":"ChatSession","text":"<p>               Bases: <code>_ChatSessionBase</code></p> <p>ChatSession represents a chat session with a language model.</p> <p>Within a chat session, the model keeps context and remembers the previous conversation.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class ChatSession(_ChatSessionBase):\n    \"\"\"ChatSession represents a chat session with a language model.\n\n    Within a chat session, the model keeps context and remembers the previous conversation.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    def __init__(\n        self,\n        model: ChatModel,\n        context: Optional[str] = None,\n        examples: Optional[List[InputOutputTextPair]] = None,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        message_history: Optional[List[ChatMessage]] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ):\n        super().__init__(\n            model=model,\n            context=context,\n            examples=examples,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            message_history=message_history,\n            stop_sequences=stop_sequences,\n        )\n</code></pre>"},{"location":"reference/#vertexai.language_models.CodeChatModel","title":"CodeChatModel","text":"<p>               Bases: <code>_ChatModelBase</code>, <code>_TunableChatModelMixin</code></p> <p>CodeChatModel represents a model that is capable of completing code.</p> <p>Examples:</p> <p>code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")</p> <p>code_chat = code_chat_model.start_chat(     context=\"I'm writing a large-scale enterprise application.\",     max_output_tokens=128,     temperature=0.2, )</p> <p>code_chat.send_message(\"Please help write a function to calculate the min of two numbers\")</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class CodeChatModel(_ChatModelBase, _TunableChatModelMixin):\n    \"\"\"CodeChatModel represents a model that is capable of completing code.\n\n    Examples:\n        code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n\n        code_chat = code_chat_model.start_chat(\n            context=\"I'm writing a large-scale enterprise application.\",\n            max_output_tokens=128,\n            temperature=0.2,\n        )\n\n        code_chat.send_message(\"Please help write a function to calculate the min of two numbers\")\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/codechat_generation_1.0.0.yaml\"\n\n    def start_chat(\n        self,\n        *,\n        context: Optional[str] = None,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        message_history: Optional[List[ChatMessage]] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ) -&gt; \"CodeChatSession\":\n        \"\"\"Starts a chat session with the code chat model.\n\n        Args:\n            context: Context shapes how the model responds throughout the conversation.\n                For example, you can use context to specify words the model can or\n                cannot use, topics to focus on or avoid, or the response format or style.\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            temperature: Controls the randomness of predictions. Range: [0, 1].\n            stop_sequences: Customized stop sequences to stop the decoding process.\n\n        Returns:\n            A `ChatSession` object.\n        \"\"\"\n        return CodeChatSession(\n            model=self,\n            context=context,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            message_history=message_history,\n            stop_sequences=stop_sequences,\n        )\n</code></pre>"},{"location":"reference/#vertexai.language_models.CodeChatModel.start_chat","title":"start_chat","text":"<pre><code>start_chat(\n    *,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; CodeChatSession\n</code></pre> <p>Starts a chat session with the code chat model.</p> PARAMETER DESCRIPTION <code>context</code> <p>Context shapes how the model responds throughout the conversation. For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000].</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CodeChatSession</code> <p>A <code>ChatSession</code> object.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def start_chat(\n    self,\n    *,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; \"CodeChatSession\":\n    \"\"\"Starts a chat session with the code chat model.\n\n    Args:\n        context: Context shapes how the model responds throughout the conversation.\n            For example, you can use context to specify words the model can or\n            cannot use, topics to focus on or avoid, or the response format or style.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n        stop_sequences: Customized stop sequences to stop the decoding process.\n\n    Returns:\n        A `ChatSession` object.\n    \"\"\"\n    return CodeChatSession(\n        model=self,\n        context=context,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        message_history=message_history,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"reference/#vertexai.language_models.CodeChatSession","title":"CodeChatSession","text":"<p>               Bases: <code>_ChatSessionBase</code></p> <p>CodeChatSession represents a chat session with code chat language model.</p> <p>Within a code chat session, the model keeps context and remembers the previous converstion.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class CodeChatSession(_ChatSessionBase):\n    \"\"\"CodeChatSession represents a chat session with code chat language model.\n\n    Within a code chat session, the model keeps context and remembers the previous converstion.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    def __init__(\n        self,\n        model: CodeChatModel,\n        context: Optional[str] = None,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        message_history: Optional[List[ChatMessage]] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ):\n        super().__init__(\n            model=model,\n            context=context,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            message_history=message_history,\n            stop_sequences=stop_sequences,\n        )\n\n    def send_message(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        stop_sequences: Optional[List[str]] = None,\n        candidate_count: Optional[int] = None,\n    ) -&gt; \"MultiCandidateTextGenerationResponse\":\n        \"\"\"Sends message to the code chat model and gets a response.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n                Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1].\n                 Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            stop_sequences: Customized stop sequences to stop the decoding process.\n            candidate_count: Number of candidates to return.\n\n        Returns:\n            A `MultiCandidateTextGenerationResponse` object that contains the\n            text produced by the model.\n        \"\"\"\n        return super().send_message(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            stop_sequences=stop_sequences,\n            candidate_count=candidate_count,\n        )\n\n    async def send_message_async(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        candidate_count: Optional[int] = None,\n    ) -&gt; \"MultiCandidateTextGenerationResponse\":\n        \"\"\"Asynchronously sends message to the code chat model and gets a response.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n                Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1].\n                 Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            candidate_count: Number of candidates to return.\n\n        Returns:\n            A `MultiCandidateTextGenerationResponse` object that contains the\n            text produced by the model.\n        \"\"\"\n        response = await super().send_message_async(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            candidate_count=candidate_count,\n        )\n        return response\n\n    def send_message_streaming(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ) -&gt; Iterator[TextGenerationResponse]:\n        \"\"\"Sends message to the language model and gets a streamed response.\n\n        The response is only added to the history once it's fully read.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            stop_sequences: Customized stop sequences to stop the decoding process.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n\n        Returns:\n            A stream of `TextGenerationResponse` objects that contain partial\n            responses produced by the model.\n        \"\"\"\n        return super().send_message_streaming(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            stop_sequences=stop_sequences,\n        )\n\n    def send_message_streaming_async(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ) -&gt; AsyncIterator[TextGenerationResponse]:\n        \"\"\"Asynchronously sends message to the language model and gets a streamed response.\n\n        The response is only added to the history once it's fully read.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            stop_sequences: Customized stop sequences to stop the decoding process.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n\n        Returns:\n            A stream of `TextGenerationResponse` objects that contain partial\n            responses produced by the model.\n        \"\"\"\n        return super().send_message_streaming_async(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            stop_sequences=stop_sequences,\n        )\n</code></pre>"},{"location":"reference/#vertexai.language_models.CodeChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Sends message to the code chat model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000]. Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].  Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the</p> <code>MultiCandidateTextGenerationResponse</code> <p>text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Sends message to the code chat model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n             Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        candidate_count: Number of candidates to return.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the\n        text produced by the model.\n    \"\"\"\n    return super().send_message(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n        candidate_count=candidate_count,\n    )\n</code></pre>"},{"location":"reference/#vertexai.language_models.CodeChatSession.send_message_async","title":"send_message_async  <code>async</code>","text":"<pre><code>send_message_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    candidate_count: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Asynchronously sends message to the code chat model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000]. Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].  Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the</p> <code>MultiCandidateTextGenerationResponse</code> <p>text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def send_message_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    candidate_count: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Asynchronously sends message to the code chat model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n             Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        candidate_count: Number of candidates to return.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the\n        text produced by the model.\n    \"\"\"\n    response = await super().send_message_async(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        candidate_count=candidate_count,\n    )\n    return response\n</code></pre>"},{"location":"reference/#vertexai.language_models.CodeChatSession.send_message_streaming","title":"send_message_streaming","text":"<pre><code>send_message_streaming(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; Iterator[TextGenerationResponse]\n</code></pre> <p>Sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TextGenerationResponse</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>TextGenerationResponse</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message_streaming(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; Iterator[TextGenerationResponse]:\n    \"\"\"Sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Returns:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    return super().send_message_streaming(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"reference/#vertexai.language_models.CodeChatSession.send_message_streaming_async","title":"send_message_streaming_async","text":"<pre><code>send_message_streaming_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; AsyncIterator[TextGenerationResponse]\n</code></pre> <p>Asynchronously sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AsyncIterator[TextGenerationResponse]</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>AsyncIterator[TextGenerationResponse]</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message_streaming_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; AsyncIterator[TextGenerationResponse]:\n    \"\"\"Asynchronously sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Returns:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    return super().send_message_streaming_async(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"reference/#vertexai.language_models.InputOutputTextPair","title":"InputOutputTextPair  <code>dataclass</code>","text":"<p>InputOutputTextPair represents a pair of input and output texts.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass InputOutputTextPair:\n    \"\"\"InputOutputTextPair represents a pair of input and output texts.\"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    input_text: str\n    output_text: str\n</code></pre>"},{"location":"reference/#vertexai.language_models.TextEmbedding","title":"TextEmbedding  <code>dataclass</code>","text":"<p>Text embedding vector and statistics.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextEmbedding:\n    \"\"\"Text embedding vector and statistics.\"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    values: List[float]\n    statistics: Optional[TextEmbeddingStatistics] = None\n    _prediction_response: Optional[aiplatform.models.Prediction] = None\n\n    @classmethod\n    def _parse_text_embedding_response(\n        cls, prediction_response: aiplatform.models.Prediction, prediction_index: int\n    ) -&gt; \"TextEmbedding\":\n        \"\"\"Creates a `TextEmbedding` object from a prediction.\n\n        Args:\n            prediction_response: `aiplatform.models.Prediction` object.\n\n        Returns:\n            `TextEmbedding` object.\n        \"\"\"\n        prediction = prediction_response.predictions[prediction_index]\n        is_prediction_from_pretrained_models = isinstance(\n            prediction, collections.abc.Mapping\n        )\n        if is_prediction_from_pretrained_models:\n            embeddings = prediction[\"embeddings\"]\n            embedding_stats = embeddings[\"statistics\"]\n            return cls(\n                values=embeddings[\"values\"],\n                statistics=TextEmbeddingStatistics(\n                    token_count=embedding_stats[\"token_count\"],\n                    truncated=embedding_stats[\"truncated\"],\n                ),\n                _prediction_response=prediction_response,\n            )\n        else:\n            return cls(values=prediction, _prediction_response=prediction_response)\n</code></pre>"},{"location":"reference/#vertexai.language_models.TextEmbeddingInput","title":"TextEmbeddingInput  <code>dataclass</code>","text":"<p>Structural text embedding input.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The main text content to embed.</p> <p> TYPE: <code>str</code> </p> <code>task_type</code> <p>The name of the downstream task the embeddings will be used for. Valid values: RETRIEVAL_QUERY     Specifies the given text is a query in a search/retrieval setting. RETRIEVAL_DOCUMENT     Specifies the given text is a document from the corpus being searched. SEMANTIC_SIMILARITY     Specifies the given text will be used for STS. CLASSIFICATION     Specifies that the given text will be classified. CLUSTERING     Specifies that the embeddings will be used for clustering. QUESTION_ANSWERING     Specifies that the embeddings will be used for question answering. FACT_VERIFICATION     Specifies that the embeddings will be used for fact verification.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>title</code> <p>Optional identifier of the text content.</p> <p> TYPE: <code>Optional[str]</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextEmbeddingInput:\n    \"\"\"Structural text embedding input.\n\n    Attributes:\n        text: The main text content to embed.\n        task_type: The name of the downstream task the embeddings will be used for.\n            Valid values:\n            RETRIEVAL_QUERY\n                Specifies the given text is a query in a search/retrieval setting.\n            RETRIEVAL_DOCUMENT\n                Specifies the given text is a document from the corpus being searched.\n            SEMANTIC_SIMILARITY\n                Specifies the given text will be used for STS.\n            CLASSIFICATION\n                Specifies that the given text will be classified.\n            CLUSTERING\n                Specifies that the embeddings will be used for clustering.\n            QUESTION_ANSWERING\n                Specifies that the embeddings will be used for question answering.\n            FACT_VERIFICATION\n                Specifies that the embeddings will be used for fact verification.\n        title: Optional identifier of the text content.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    text: str\n    task_type: Optional[str] = None\n    title: Optional[str] = None\n</code></pre>"},{"location":"reference/#vertexai.language_models.TextGenerationResponse","title":"TextGenerationResponse  <code>dataclass</code>","text":"<p>TextGenerationResponse represents a response of a language model. Attributes:     text: The generated text     is_blocked: Whether the the request was blocked.     errors: The error codes indicate why the response was blocked.         Learn more information about safety errors here:         this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors     safety_attributes: Scores for safety attributes.         Learn more about the safety attributes here:         https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions     grounding_metadata: Metadata for grounding.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextGenerationResponse:\n    \"\"\"TextGenerationResponse represents a response of a language model.\n    Attributes:\n        text: The generated text\n        is_blocked: Whether the the request was blocked.\n        errors: The error codes indicate why the response was blocked.\n            Learn more information about safety errors here:\n            this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors\n        safety_attributes: Scores for safety attributes.\n            Learn more about the safety attributes here:\n            https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions\n        grounding_metadata: Metadata for grounding.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    text: str\n    _prediction_response: Any\n    is_blocked: bool = False\n    errors: Tuple[int] = tuple()\n    safety_attributes: Dict[str, float] = dataclasses.field(default_factory=dict)\n    grounding_metadata: Optional[GroundingMetadata] = None\n\n    def __repr__(self):\n        if self.text:\n            return self.text\n        # Falling back to the full representation\n        elif self.grounding_metadata is not None:\n            return (\n                \"TextGenerationResponse(\"\n                f\"text={self.text!r}\"\n                f\", is_blocked={self.is_blocked!r}\"\n                f\", errors={self.errors!r}\"\n                f\", safety_attributes={self.safety_attributes!r}\"\n                f\", grounding_metadata={self.grounding_metadata!r}\"\n                \")\"\n            )\n        else:\n            return (\n                \"TextGenerationResponse(\"\n                f\"text={self.text!r}\"\n                f\", is_blocked={self.is_blocked!r}\"\n                f\", errors={self.errors!r}\"\n                f\", safety_attributes={self.safety_attributes!r}\"\n                \")\"\n            )\n\n    @property\n    def raw_prediction_response(self) -&gt; aiplatform.models.Prediction:\n        \"\"\"Raw prediction response.\"\"\"\n        return self._prediction_response\n</code></pre>"},{"location":"reference/#vertexai.language_models.TextGenerationResponse.raw_prediction_response","title":"raw_prediction_response  <code>property</code>","text":"<pre><code>raw_prediction_response: Prediction\n</code></pre> <p>Raw prediction response.</p>"},{"location":"vertexai/","title":"Index","text":"<p>The vertexai module.</p>"},{"location":"vertexai/generative_models/","title":"Generative models","text":"<p>Classes for working with the Gemini models.</p>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate","title":"Candidate","text":"<p>A response candidate generated by the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Candidate:\n    \"\"\"A response candidate generated by the model.\"\"\"\n\n    def __init__(self):\n        raw_candidate = gapic_content_types.Candidate()\n        self._raw_candidate = raw_candidate\n\n    @classmethod\n    def _from_gapic(cls, raw_candidate: gapic_content_types.Candidate) -&gt; \"Candidate\":\n        candidate = cls()\n        candidate._raw_candidate = raw_candidate\n        return candidate\n\n    @classmethod\n    def from_dict(cls, candidate_dict: Dict[str, Any]) -&gt; \"Candidate\":\n        raw_candidate = gapic_content_types.Candidate()\n        json_format.ParseDict(candidate_dict, raw_candidate._pb)\n        return cls._from_gapic(raw_candidate=raw_candidate)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_candidate)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_candidate.__repr__()\n\n    @property\n    def content(self) -&gt; \"Content\":\n        return Content._from_gapic(\n            raw_content=self._raw_candidate.content,\n        )\n\n    @property\n    def finish_reason(self) -&gt; gapic_content_types.Candidate.FinishReason:\n        return self._raw_candidate.finish_reason\n\n    @property\n    def finish_message(self) -&gt; str:\n        return self._raw_candidate.finish_message\n\n    @property\n    def index(self) -&gt; int:\n        return self._raw_candidate.index\n\n    @property\n    def safety_ratings(self) -&gt; Sequence[gapic_content_types.SafetyRating]:\n        return self._raw_candidate.safety_ratings\n\n    @property\n    def citation_metadata(self) -&gt; gapic_content_types.CitationMetadata:\n        return self._raw_candidate.citation_metadata\n\n    @property\n    def grounding_metadata(self) -&gt; gapic_content_types.GroundingMetadata:\n        return self._raw_candidate.grounding_metadata\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        try:\n            return self.content.text\n        except (ValueError, AttributeError) as e:\n            # Enrich the error message with the whole Candidate.\n            # The Content object does not have full information.\n            raise ValueError(\n                \"Cannot get the Candidate text.\\n\"\n                f\"{e}\\n\"\n                \"Candidate:\\n\" + _dict_to_pretty_string(self.to_dict())\n            ) from e\n\n    @property\n    def function_calls(self) -&gt; Sequence[gapic_tool_types.FunctionCall]:\n        if not self.content or not self.content.parts:\n            return []\n        return [\n            part.function_call\n            for part in self.content.parts\n            if part and part.function_call\n        ]\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ChatSession","title":"ChatSession","text":"<p>Chat session holds the chat history.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class ChatSession:\n    \"\"\"Chat session holds the chat history.\"\"\"\n\n    _USER_ROLE = \"user\"\n    _MODEL_ROLE = \"model\"\n\n    def __init__(\n        self,\n        model: _GenerativeModel,\n        *,\n        history: Optional[List[\"Content\"]] = None,\n        response_validation: bool = True,\n    ):\n        if history:\n            if not all(isinstance(item, Content) for item in history):\n                raise ValueError(\"history must be a list of Content objects.\")\n\n        self._model = model\n        self._history = history or []\n        self._response_validator = _validate_response if response_validation else None\n        # _responder is currently only set by PreviewChatSession\n        self._responder: Optional[\"AutomaticFunctionCallingResponder\"] = None\n\n    @property\n    def history(self) -&gt; List[\"Content\"]:\n        return self._history\n\n    def send_message(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n        stream: bool = False,\n    ) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"]]:\n        \"\"\"Generates content.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n            stream: Whether to stream the response.\n\n        Returns:\n            A single GenerationResponse object if stream == False\n            A stream of GenerationResponse objects if stream == True\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        if stream:\n            return self._send_message_streaming(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n        else:\n            return self._send_message(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n\n    def send_message_async(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n        stream: bool = False,\n    ) -&gt; Union[\n        Awaitable[\"GenerationResponse\"],\n        Awaitable[AsyncIterable[\"GenerationResponse\"]],\n    ]:\n        \"\"\"Generates content asynchronously.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n            stream: Whether to stream the response.\n\n        Returns:\n            An awaitable for a single GenerationResponse object if stream == False\n            An awaitable for a stream of GenerationResponse objects if stream == True\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        if stream:\n            return self._send_message_streaming_async(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n        else:\n            return self._send_message_async(\n                content=content,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n\n    def _send_message(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; \"GenerationResponse\":\n        \"\"\"Generates content.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Returns:\n            A single GenerationResponse object\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        history_delta = [request_message]\n\n        message_responder = (\n            self._responder._create_responder_for_message(\n                tools=tools or self._model._tools\n            )\n            if self._responder\n            else None\n        )\n\n        while True:\n            request_history = self._history + history_delta\n            response = self._model._generate_content(\n                contents=request_history,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools,\n            )\n            # By default we're not adding incomplete interactions to history.\n            if self._response_validator is not None:\n                self._response_validator(\n                    response=response,\n                    request_contents=request_history,\n                    response_chunks=[response],\n                )\n\n            # Adding the request and the first response candidate to history\n            response_message = response.candidates[0].content\n            # Response role is NOT set by the model.\n            response_message.role = self._MODEL_ROLE\n            history_delta.append(response_message)\n\n            auto_responder_content = (\n                message_responder.respond_to_model_response(response=response)\n                if message_responder\n                else None\n            )\n            if auto_responder_content:\n                auto_responder_content.role = self._USER_ROLE\n                history_delta.append(auto_responder_content)\n            else:\n                break\n\n        self._history.extend(history_delta)\n        return response\n\n    async def _send_message_async(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; \"GenerationResponse\":\n        \"\"\"Generates content asynchronously.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Returns:\n            An awaitable for a single GenerationResponse object\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        request_history = list(self._history)\n        request_history.append(request_message)\n\n        response = await self._model._generate_content_async(\n            contents=request_history,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n        # By default we're not adding incomplete interactions to history.\n        if self._response_validator is not None:\n            self._response_validator(\n                response=response,\n                request_contents=request_history,\n                response_chunks=[response],\n            )\n\n        # Adding the request and the first response candidate to history\n        response_message = response.candidates[0].content\n        # Response role is NOT set by the model.\n        response_message.role = self._MODEL_ROLE\n        self._history.append(request_message)\n        self._history.append(response_message)\n        return response\n\n    def _send_message_streaming(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; Iterable[\"GenerationResponse\"]:\n        \"\"\"Generates content.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Yields:\n            A stream of GenerationResponse objects\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        request_history = list(self._history)\n        request_history.append(request_message)\n\n        stream = self._model._generate_content_streaming(\n            contents=request_history,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n        chunks = []\n        full_response = None\n        for chunk in stream:\n            chunks.append(chunk)\n            # By default we're not adding incomplete interactions to history.\n            if self._response_validator is not None:\n                self._response_validator(\n                    response=chunk,\n                    request_contents=request_history,\n                    response_chunks=chunks,\n                )\n            if full_response:\n                _append_response(full_response, chunk)\n            else:\n                full_response = chunk\n            yield chunk\n        if not full_response:\n            return\n\n        # Adding the request and the first response candidate to history\n        response_message = full_response.candidates[0].content\n        # Response role is NOT set by the model.\n        response_message.role = self._MODEL_ROLE\n        self._history.append(request_message)\n        self._history.append(response_message)\n\n    async def _send_message_streaming_async(\n        self,\n        content: PartsType,\n        *,\n        generation_config: Optional[GenerationConfigType] = None,\n        safety_settings: Optional[SafetySettingsType] = None,\n        tools: Optional[List[\"Tool\"]] = None,\n    ) -&gt; AsyncIterable[\"GenerationResponse\"]:\n        \"\"\"Generates content asynchronously.\n\n        Args:\n            content: Content to send to the model.\n                Supports a value that can be converted to a Part or a list of such values.\n                Supports\n                * str, Image, Part,\n                * List[Union[str, Image, Part]],\n            generation_config: Parameters for the generation.\n            safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n            tools: A list of tools (functions) that the model can try calling.\n\n        Returns:\n            An awaitable for a stream of GenerationResponse objects\n\n        Raises:\n            ResponseValidationError: If the response was blocked or is incomplete.\n        \"\"\"\n        # Preparing the message history to send\n        request_message = Content._from_gapic(\n            _to_content(value=content, role=self._USER_ROLE)\n        )\n        request_history = list(self._history)\n        request_history.append(request_message)\n\n        stream = await self._model._generate_content_streaming_async(\n            contents=request_history,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n\n        async def async_generator():\n            chunks = []\n            full_response = None\n            async for chunk in stream:\n                chunks.append(chunk)\n                # By default we're not adding incomplete interactions to history.\n                if self._response_validator is not None:\n                    self._response_validator(\n                        response=chunk,\n                        request_contents=request_history,\n                        response_chunks=chunks,\n                    )\n                if full_response:\n                    _append_response(full_response, chunk)\n                else:\n                    full_response = chunk\n                yield chunk\n            if not full_response:\n                return\n            # Adding the request and the first response candidate to history\n            response_message = full_response.candidates[0].content\n            # Response role is NOT set by the model.\n            response_message.role = self._MODEL_ROLE\n            self._history.append(request_message)\n            self._history.append(response_message)\n\n        return async_generator()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, Iterable[GenerationResponse]\n]\n</code></pre> <p>Generates content.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"]]:\n    \"\"\"Generates content.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        A single GenerationResponse object if stream == False\n        A stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ChatSession.send_message_async","title":"send_message_async","text":"<pre><code>send_message_async(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    Awaitable[GenerationResponse],\n    Awaitable[AsyncIterable[GenerationResponse]],\n]\n</code></pre> <p>Generates content asynchronously.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a single GenerationResponse object if stream == False</p> <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message_async(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\n    Awaitable[\"GenerationResponse\"],\n    Awaitable[AsyncIterable[\"GenerationResponse\"]],\n]:\n    \"\"\"Generates content asynchronously.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        An awaitable for a single GenerationResponse object if stream == False\n        An awaitable for a stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Content","title":"Content","text":"<p>The multi-part content of a message.</p> Usage <pre><code>response = model.generate_content(contents=[\n    Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n])\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Content:\n    r\"\"\"The multi-part content of a message.\n\n    Usage:\n        ```\n        response = model.generate_content(contents=[\n            Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n        ])\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        parts: List[\"Part\"] = None,\n        role: Optional[str] = None,\n    ):\n        raw_parts = [part._raw_part for part in parts or []]\n        self._raw_content = gapic_content_types.Content(parts=raw_parts, role=role)\n\n    @classmethod\n    def _from_gapic(cls, raw_content: gapic_content_types.Content) -&gt; \"Content\":\n        content = cls()\n        content._raw_content = raw_content\n        return content\n\n    @classmethod\n    def from_dict(cls, content_dict: Dict[str, Any]) -&gt; \"Content\":\n        raw_content = gapic_content_types.Content()\n        json_format.ParseDict(content_dict, raw_content._pb)\n        return cls._from_gapic(raw_content=raw_content)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_content)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_content.__repr__()\n\n    @property\n    def parts(self) -&gt; List[\"Part\"]:\n        return [\n            Part._from_gapic(raw_part=raw_part) for raw_part in self._raw_content.parts\n        ]\n\n    @property\n    def role(self) -&gt; str:\n        return self._raw_content.role\n\n    @role.setter\n    def role(self, role: str) -&gt; None:\n        self._raw_content.role = role\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        if len(self.parts) &gt; 1:\n            raise ValueError(\"Multiple content parts are not supported.\")\n        if not self.parts:\n            raise ValueError(\n                \"Response candidate content has no parts (and thus no text).\"\n                \" The candidate is likely blocked by the safety filters.\\n\"\n                \"Content:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        return self.parts[0].text\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.FunctionDeclaration","title":"FunctionDeclaration","text":"<p>A representation of a function declaration.</p> Usage <p>Create function declaration and tool: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"celsius\",\n                    \"fahrenheit\",\n                ]\n            }\n        },\n        \"required\": [\n            \"location\"\n        ]\n    },\n)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class FunctionDeclaration:\n    r\"\"\"A representation of a function declaration.\n\n    Usage:\n        Create function declaration and tool:\n        ```\n        get_current_weather_func = generative_models.FunctionDeclaration(\n            name=\"get_current_weather\",\n            description=\"Get the current weather in a given location\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\n                            \"celsius\",\n                            \"fahrenheit\",\n                        ]\n                    }\n                },\n                \"required\": [\n                    \"location\"\n                ]\n            },\n        )\n        weather_tool = generative_models.Tool(\n            function_declarations=[get_current_weather_func],\n        )\n        ```\n        Use tool in `GenerativeModel.generate_content`:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\n            \"What is the weather like in Boston?\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        ))\n        ```\n        Use tool in chat:\n        ```\n        model = GenerativeModel(\n            \"gemini-pro\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        )\n        chat = model.start_chat()\n        print(chat.send_message(\"What is the weather like in Boston?\"))\n        print(chat.send_message(\n            Part.from_function_response(\n                name=\"get_current_weather\",\n                response={\n                    \"content\": {\"weather_there\": \"super nice\"},\n                }\n            ),\n        ))\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        parameters: Dict[str, Any],\n        description: Optional[str] = None,\n    ):\n        \"\"\"Constructs a FunctionDeclaration.\n\n        Args:\n            name: The name of the function that the model can call.\n            parameters: Describes the parameters to this function in JSON Schema Object format.\n            description: Description and purpose of the function.\n                Model uses it to decide how and whether to call the function.\n        \"\"\"\n        gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n        self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n            name=name, description=description, parameters=raw_schema\n        )\n\n    @classmethod\n    def from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n        return CallableFunctionDeclaration.from_func(func)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_function_declaration)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_function_declaration.__repr__()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.FunctionDeclaration.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the function that the model can call.</p> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Describes the parameters to this function in JSON Schema Object format.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>description</code> <p>Description and purpose of the function. Model uses it to decide how and whether to call the function.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None,\n):\n    \"\"\"Constructs a FunctionDeclaration.\n\n    Args:\n        name: The name of the function that the model can call.\n        parameters: Describes the parameters to this function in JSON Schema Object format.\n        description: Description and purpose of the function.\n            Model uses it to decide how and whether to call the function.\n    \"\"\"\n    gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n    raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n        name=name, description=description, parameters=raw_schema\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationConfig","title":"GenerationConfig","text":"<p>Parameters for the generation.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GenerationConfig:\n    \"\"\"Parameters for the generation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        candidate_count: Optional[int] = None,\n        max_output_tokens: Optional[int] = None,\n        stop_sequences: Optional[List[str]] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        response_mime_type: Optional[str] = None,\n        response_schema: Optional[Dict[str, Any]] = None,\n    ):\n        r\"\"\"Constructs a GenerationConfig object.\n\n        Args:\n            temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n            top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n            top_k: If specified, top-k sampling will be used.\n            candidate_count: Number of candidates to generate.\n            max_output_tokens: The maximum number of output tokens to generate per message.\n            stop_sequences: A list of stop sequences.\n            presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n                thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n            frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n                text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n            response_mime_type: Output response mimetype of the generated\n                candidate text. Supported mimetypes:\n\n                -  ``text/plain``: (default) Text output.\n                -  ``application/json``: JSON response in the candidates.\n\n                The model needs to be prompted to output the appropriate\n                response type, otherwise the behavior is undefined.\n            response_schema: Output response schema of the genreated candidate text. Only valid when\n                response_mime_type is application/json.\n\n        Usage:\n            ```\n            response = model.generate_content(\n                \"Why is sky blue?\",\n                generation_config=GenerationConfig(\n                    temperature=0.1,\n                    top_p=0.95,\n                    top_k=20,\n                    candidate_count=1,\n                    max_output_tokens=100,\n                    stop_sequences=[\"\\n\\n\\n\"],\n                )\n            )\n            ```\n        \"\"\"\n        if response_schema is None:\n            raw_schema = None\n        else:\n            gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n            raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n        self._raw_generation_config = gapic_content_types.GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            candidate_count=candidate_count,\n            max_output_tokens=max_output_tokens,\n            stop_sequences=stop_sequences,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            response_mime_type=response_mime_type,\n            response_schema=raw_schema,\n        )\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_generation_config: gapic_content_types.GenerationConfig,\n    ) -&gt; \"GenerationConfig\":\n        response = cls()\n        response._raw_generation_config = raw_generation_config\n        return response\n\n    @classmethod\n    def from_dict(cls, generation_config_dict: Dict[str, Any]) -&gt; \"GenerationConfig\":\n        raw_generation_config = gapic_content_types.GenerationConfig(\n            generation_config_dict\n        )\n        return cls._from_gapic(raw_generation_config=raw_generation_config)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_generation_config)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_generation_config.__repr__()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>temperature</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>If specified, top-k sampling will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to generate.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>The maximum number of output tokens to generate per message.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>A list of stop sequences.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>response_mime_type</code> <p>Output response mimetype of the generated candidate text. Supported mimetypes:</p> <ul> <li><code>text/plain</code>: (default) Text output.</li> <li><code>application/json</code>: JSON response in the candidates.</li> </ul> <p>The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>response_schema</code> <p>Output response schema of the genreated candidate text. Only valid when response_mime_type is application/json.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Usage <pre><code>response = model.generate_content(\n    \"Why is sky blue?\",\n    generation_config=GenerationConfig(\n        temperature=0.1,\n        top_p=0.95,\n        top_k=20,\n        candidate_count=1,\n        max_output_tokens=100,\n        stop_sequences=[\"\\n\\n\\n\"],\n    )\n)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None,\n):\n    r\"\"\"Constructs a GenerationConfig object.\n\n    Args:\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n        top_k: If specified, top-k sampling will be used.\n        candidate_count: Number of candidates to generate.\n        max_output_tokens: The maximum number of output tokens to generate per message.\n        stop_sequences: A list of stop sequences.\n        presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n        frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n        response_mime_type: Output response mimetype of the generated\n            candidate text. Supported mimetypes:\n\n            -  ``text/plain``: (default) Text output.\n            -  ``application/json``: JSON response in the candidates.\n\n            The model needs to be prompted to output the appropriate\n            response type, otherwise the behavior is undefined.\n        response_schema: Output response schema of the genreated candidate text. Only valid when\n            response_mime_type is application/json.\n\n    Usage:\n        ```\n        response = model.generate_content(\n            \"Why is sky blue?\",\n            generation_config=GenerationConfig(\n                temperature=0.1,\n                top_p=0.95,\n                top_k=20,\n                candidate_count=1,\n                max_output_tokens=100,\n                stop_sequences=[\"\\n\\n\\n\"],\n            )\n        )\n        ```\n    \"\"\"\n    if response_schema is None:\n        raw_schema = None\n    else:\n        gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_generation_config = gapic_content_types.GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        candidate_count=candidate_count,\n        max_output_tokens=max_output_tokens,\n        stop_sequences=stop_sequences,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        response_mime_type=response_mime_type,\n        response_schema=raw_schema,\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse","title":"GenerationResponse","text":"<p>The response from the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GenerationResponse:\n    \"\"\"The response from the model.\"\"\"\n\n    def __init__(self):\n        raw_response = gapic_prediction_service_types.GenerateContentResponse()\n        self._raw_response = raw_response\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_response: gapic_prediction_service_types.GenerateContentResponse,\n    ) -&gt; \"GenerationResponse\":\n        response = cls()\n        response._raw_response = raw_response\n        return response\n\n    @classmethod\n    def from_dict(cls, response_dict: Dict[str, Any]) -&gt; \"GenerationResponse\":\n        raw_response = gapic_prediction_service_types.GenerateContentResponse()\n        json_format.ParseDict(response_dict, raw_response._pb)\n        return cls._from_gapic(raw_response=raw_response)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_response)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_response.__repr__()\n\n    @property\n    def candidates(self) -&gt; List[\"Candidate\"]:\n        return [\n            Candidate._from_gapic(raw_candidate=raw_candidate)\n            for raw_candidate in self._raw_response.candidates\n        ]\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        if len(self.candidates) &gt; 1:\n            raise ValueError(\n                \"The response has multiple candidates.\"\n                \" Use `response.candidate[i].text` to get text of a particular candidate.\"\n            )\n        if not self.candidates:\n            raise ValueError(\n                \"Response has no candidates (and thus no text).\"\n                \" The response is likely blocked by the safety filters.\\n\"\n                \"Response:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        try:\n            return self.candidates[0].text\n        except (ValueError, AttributeError) as e:\n            # Enrich the error message with the whole Response.\n            # The Candidate object does not have full information.\n            raise ValueError(\n                \"Cannot get the response text.\\n\"\n                f\"{e}\\n\"\n                \"Response:\\n\" + _dict_to_pretty_string(self.to_dict())\n            ) from e\n\n    @property\n    def prompt_feedback(\n        self,\n    ) -&gt; gapic_prediction_service_types.GenerateContentResponse.PromptFeedback:\n        return self._raw_response.prompt_feedback\n\n    @property\n    def usage_metadata(\n        self,\n    ) -&gt; gapic_prediction_service_types.GenerateContentResponse.UsageMetadata:\n        return self._raw_response.usage_metadata\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image","title":"Image","text":"<p>The image that can be sent to a generative model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Image:\n    \"\"\"The image that can be sent to a generative model.\"\"\"\n\n    _image_bytes: bytes\n    _loaded_image: Optional[\"PIL_Image.Image\"] = None\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"Image\":\n        \"\"\"Loads image from file.\n\n        Args:\n            location: Local path from where to load the image.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        image_bytes = pathlib.Path(location).read_bytes()\n        image = Image()\n        image._image_bytes = image_bytes\n        return image\n\n    @staticmethod\n    def from_bytes(data: bytes) -&gt; \"Image\":\n        \"\"\"Loads image from image bytes.\n\n        Args:\n            data: Image bytes.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        image = Image()\n        image._image_bytes = data\n        return image\n\n    @property\n    def _pil_image(self) -&gt; \"PIL_Image.Image\":\n        if self._loaded_image is None:\n            if not PIL_Image:\n                raise RuntimeError(\n                    \"The PIL module is not available. Please install the Pillow package.\"\n                )\n            self._loaded_image = PIL_Image.open(io.BytesIO(self._image_bytes))\n        return self._loaded_image\n\n    @property\n    def _mime_type(self) -&gt; str:\n        \"\"\"Returns the MIME type of the image.\"\"\"\n        if PIL_Image:\n            return _FORMAT_TO_MIME_TYPE[self._pil_image.format.lower()]\n        else:\n            # Fall back to jpeg\n            return \"image/jpeg\"\n\n    @property\n    def data(self) -&gt; bytes:\n        \"\"\"Returns the image data.\"\"\"\n        return self._image_bytes\n\n    def _repr_png_(self):\n        return self._pil_image._repr_png_()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image.data","title":"data  <code>property</code>","text":"<pre><code>data: bytes\n</code></pre> <p>Returns the image data.</p>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image.from_bytes","title":"from_bytes  <code>staticmethod</code>","text":"<pre><code>from_bytes(data: bytes) -&gt; Image\n</code></pre> <p>Loads image from image bytes.</p> PARAMETER DESCRIPTION <code>data</code> <p>Image bytes.</p> <p> TYPE: <code>bytes</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_bytes(data: bytes) -&gt; \"Image\":\n    \"\"\"Loads image from image bytes.\n\n    Args:\n        data: Image bytes.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image = Image()\n    image._image_bytes = data\n    return image\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from file.\n\n    Args:\n        location: Local path from where to load the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image()\n    image._image_bytes = image_bytes\n    return image\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part","title":"Part","text":"<p>A part of a multi-part Content message.</p> Usage <pre><code>text_part = Part.from_text(\"Why is sky blue?\")\nimage_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\nvideo_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\nfunction_response_part = Part.from_function_response(\n    name=\"get_current_weather\",\n    response={\n        \"content\": {\"weather_there\": \"super nice\"},\n    }\n)\n\nresponse1 = model.generate_content([text_part, image_part])\nresponse2 = model.generate_content(video_part)\nresponse3 = chat.send_message(function_response_part)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Part:\n    r\"\"\"A part of a multi-part Content message.\n\n    Usage:\n        ```\n        text_part = Part.from_text(\"Why is sky blue?\")\n        image_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\n        video_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\n        function_response_part = Part.from_function_response(\n            name=\"get_current_weather\",\n            response={\n                \"content\": {\"weather_there\": \"super nice\"},\n            }\n        )\n\n        response1 = model.generate_content([text_part, image_part])\n        response2 = model.generate_content(video_part)\n        response3 = chat.send_message(function_response_part)\n        ```\n    \"\"\"\n\n    def __init__(self):\n        raw_part = gapic_content_types.Part()\n        self._raw_part = raw_part\n\n    @classmethod\n    def _from_gapic(cls, raw_part: gapic_content_types.Part) -&gt; \"Part\":\n        part = cls()\n        part._raw_part = raw_part\n        return part\n\n    @classmethod\n    def from_dict(cls, part_dict: Dict[str, Any]) -&gt; \"Part\":\n        raw_part = gapic_content_types.Part()\n        json_format.ParseDict(part_dict, raw_part._pb)\n        return cls._from_gapic(raw_part=raw_part)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_part.__repr__()\n\n    @staticmethod\n    def from_data(data: bytes, mime_type: str) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                inline_data=gapic_content_types.Blob(data=data, mime_type=mime_type)\n            )\n        )\n\n    @staticmethod\n    def from_uri(uri: str, mime_type: str) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                file_data=gapic_content_types.FileData(\n                    file_uri=uri, mime_type=mime_type\n                )\n            )\n        )\n\n    @staticmethod\n    def from_text(text: str) -&gt; \"Part\":\n        return Part._from_gapic(raw_part=gapic_content_types.Part(text=text))\n\n    @staticmethod\n    def from_image(image: \"Image\") -&gt; \"Part\":\n        return Part.from_data(data=image.data, mime_type=image._mime_type)\n\n    @staticmethod\n    def from_function_response(name: str, response: Dict[str, Any]) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                function_response=gapic_tool_types.FunctionResponse(\n                    name=name,\n                    response=response,\n                )\n            )\n        )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_part)\n\n    @property\n    def text(self) -&gt; str:\n        if \"text\" not in self._raw_part:\n            raise AttributeError(\n                \"Response candidate content part has no text.\\n\"\n                \"Part:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        return self._raw_part.text\n\n    @property\n    def mime_type(self) -&gt; Optional[str]:\n        part_type = self._raw_part._pb.WhichOneof(\"data\")\n        if part_type == \"inline_data\":\n            return self._raw_part.inline_data.mime_type\n        if part_type == \"file_data\":\n            return self._raw_part.file_data.mime_type\n        raise AttributeError(f\"Part has no mime_type.\\nPart:\\n{self.to_dict()}\")\n\n    @property\n    def inline_data(self) -&gt; gapic_content_types.Blob:\n        return self._raw_part.inline_data\n\n    @property\n    def file_data(self) -&gt; gapic_content_types.FileData:\n        return self._raw_part.file_data\n\n    @property\n    def function_call(self) -&gt; gapic_tool_types.FunctionCall:\n        return self._raw_part.function_call\n\n    @property\n    def function_response(self) -&gt; gapic_tool_types.FunctionResponse:\n        return self._raw_part.function_response\n\n    @property\n    def _image(self) -&gt; \"Image\":\n        return Image.from_bytes(data=self._raw_part.inline_data.data)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting","title":"SafetySetting","text":"<p>Parameters for the generation.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class SafetySetting:\n    \"\"\"Parameters for the generation.\"\"\"\n\n    HarmCategory = gapic_content_types.HarmCategory\n    HarmBlockMethod = gapic_content_types.SafetySetting.HarmBlockMethod\n    HarmBlockThreshold = gapic_content_types.SafetySetting.HarmBlockThreshold\n\n    def __init__(\n        self,\n        *,\n        category: \"SafetySetting.HarmCategory\",\n        threshold: \"SafetySetting.HarmBlockThreshold\",\n        method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n    ):\n        r\"\"\"Safety settings.\n\n        Args:\n            category: Harm category.\n            threshold: The harm block threshold.\n            method: Specify if the threshold is used for probability or severity\n                score. If not specified, the threshold is used for probability\n                score.\n        \"\"\"\n        self._raw_safety_setting = gapic_content_types.SafetySetting(\n            category=category,\n            threshold=threshold,\n            method=method,\n        )\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_safety_setting: gapic_content_types.SafetySetting,\n    ) -&gt; \"SafetySetting\":\n        response = cls(\n            category=raw_safety_setting.category,\n            threshold=raw_safety_setting.threshold,\n        )\n        response._raw_safety_setting = raw_safety_setting\n        return response\n\n    @classmethod\n    def from_dict(cls, safety_setting_dict: Dict[str, Any]) -&gt; \"SafetySetting\":\n        raw_safety_setting = gapic_content_types.SafetySetting(safety_setting_dict)\n        return cls._from_gapic(raw_safety_setting=raw_safety_setting)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_safety_setting)\n\n    def __repr__(self):\n        return self._raw_safety_setting.__repr__()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    category: SafetySetting.HarmCategory,\n    threshold: SafetySetting.HarmBlockThreshold,\n    method: Optional[SafetySetting.HarmBlockMethod] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>category</code> <p>Harm category.</p> <p> TYPE: <code>HarmCategory</code> </p> <code>threshold</code> <p>The harm block threshold.</p> <p> TYPE: <code>HarmBlockThreshold</code> </p> <code>method</code> <p>Specify if the threshold is used for probability or severity score. If not specified, the threshold is used for probability score.</p> <p> TYPE: <code>Optional[HarmBlockMethod]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    category: \"SafetySetting.HarmCategory\",\n    threshold: \"SafetySetting.HarmBlockThreshold\",\n    method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n):\n    r\"\"\"Safety settings.\n\n    Args:\n        category: Harm category.\n        threshold: The harm block threshold.\n        method: Specify if the threshold is used for probability or severity\n            score. If not specified, the threshold is used for probability\n            score.\n    \"\"\"\n    self._raw_safety_setting = gapic_content_types.SafetySetting(\n        category=category,\n        threshold=threshold,\n        method=method,\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Tool","title":"Tool","text":"<p>A collection of functions that the model may use to generate response.</p> Usage <p>Create tool from function declarations: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(...)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Tool:\n    r\"\"\"A collection of functions that the model may use to generate response.\n\n    Usage:\n        Create tool from function declarations:\n        ```\n        get_current_weather_func = generative_models.FunctionDeclaration(...)\n        weather_tool = generative_models.Tool(\n            function_declarations=[get_current_weather_func],\n        )\n        ```\n        Use tool in `GenerativeModel.generate_content`:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\n            \"What is the weather like in Boston?\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        ))\n        ```\n        Use tool in chat:\n        ```\n        model = GenerativeModel(\n            \"gemini-pro\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        )\n        chat = model.start_chat()\n        print(chat.send_message(\"What is the weather like in Boston?\"))\n        print(chat.send_message(\n            Part.from_function_response(\n                name=\"get_current_weather\",\n                response={\n                    \"content\": {\"weather_there\": \"super nice\"},\n                }\n            ),\n        ))\n        ```\n    \"\"\"\n\n    _raw_tool: gapic_tool_types.Tool\n\n    def __init__(\n        self,\n        function_declarations: List[\"FunctionDeclaration\"],\n    ):\n        gapic_function_declarations = [\n            function_declaration._raw_function_declaration\n            for function_declaration in function_declarations\n        ]\n        self._raw_tool = gapic_tool_types.Tool(\n            function_declarations=gapic_function_declarations\n        )\n        callable_functions = {\n            function_declaration._raw_function_declaration.name: function_declaration\n            for function_declaration in function_declarations\n            if isinstance(function_declaration, CallableFunctionDeclaration)\n        }\n        self._callable_functions = callable_functions\n\n    @classmethod\n    def from_function_declarations(\n        cls,\n        function_declarations: List[\"FunctionDeclaration\"],\n    ) -&gt; \"Tool\":\n        return Tool(function_declarations=function_declarations)\n\n    @classmethod\n    def from_retrieval(\n        cls,\n        retrieval: Union[\"grounding.Retrieval\", \"rag.Retrieval\"],\n    ) -&gt; \"Tool\":\n        raw_tool = gapic_tool_types.Tool(retrieval=retrieval._raw_retrieval)\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    @classmethod\n    def from_google_search_retrieval(\n        cls,\n        google_search_retrieval: \"grounding.GoogleSearchRetrieval\",\n    ) -&gt; \"Tool\":\n        raw_tool = gapic_tool_types.Tool(\n            google_search_retrieval=google_search_retrieval._raw_google_search_retrieval\n        )\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_tool: gapic_tool_types.Tool,\n    ) -&gt; \"Tool\":\n        response = cls([])\n        response._raw_tool = raw_tool\n        return response\n\n    @classmethod\n    def from_dict(cls, tool_dict: Dict[str, Any]) -&gt; \"Tool\":\n        tool_dict = copy.deepcopy(tool_dict)\n        function_declarations = tool_dict[\"function_declarations\"]\n        for function_declaration in function_declarations:\n            function_declaration[\"parameters\"] = _convert_schema_dict_to_gapic(\n                function_declaration[\"parameters\"]\n            )\n        raw_tool = gapic_tool_types.Tool(tool_dict)\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_tool)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_tool.__repr__()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.grounding","title":"grounding","text":"<p>Grounding namespace.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class grounding:  # pylint: disable=invalid-name\n    \"\"\"Grounding namespace.\"\"\"\n\n    __module__ = \"vertexai.generative_models\"\n\n    def __init__(self):\n        raise RuntimeError(\"This class must not be instantiated.\")\n\n    class GoogleSearchRetrieval:\n        r\"\"\"Tool to retrieve public web data for grounding, powered by\n        Google Search.\n        \"\"\"\n\n        def __init__(self):\n            \"\"\"Initializes a Google Search Retrieval tool.\"\"\"\n            self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.grounding.GoogleSearchRetrieval","title":"GoogleSearchRetrieval","text":"<p>Tool to retrieve public web data for grounding, powered by Google Search.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GoogleSearchRetrieval:\n    r\"\"\"Tool to retrieve public web data for grounding, powered by\n    Google Search.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes a Google Search Retrieval tool.\"\"\"\n        self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval()\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.grounding.GoogleSearchRetrieval.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes a Google Search Retrieval tool.\"\"\"\n    self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval()\n</code></pre>"},{"location":"vertexai/language_models/","title":"Language models","text":"<p>Classes for working with language models.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatMessage","title":"ChatMessage  <code>dataclass</code>","text":"<p>A chat message.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>Content of the message.</p> <p> TYPE: <code>str</code> </p> <code>author</code> <p>Author of the message.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass ChatMessage:\n    \"\"\"A chat message.\n\n    Attributes:\n        content: Content of the message.\n        author: Author of the message.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    content: str\n    author: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatModel","title":"ChatModel","text":"<p>               Bases: <code>_ChatModelBase</code>, <code>_TunableChatModelMixin</code>, <code>_RlhfTunableModelMixin</code></p> <p>ChatModel represents a language model that is capable of chat.</p> <p>Examples::</p> <pre><code>chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n\nchat = chat_model.start_chat(\n    context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n    examples=[\n        InputOutputTextPair(\n            input_text=\"Who do you work for?\",\n            output_text=\"I work for Ned.\",\n        ),\n        InputOutputTextPair(\n            input_text=\"What do I like?\",\n            output_text=\"Ned likes watching movies.\",\n        ),\n    ],\n    temperature=0.3,\n)\n\nchat.send_message(\"Do you know any cool events this weekend?\")\n</code></pre> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class ChatModel(_ChatModelBase, _TunableChatModelMixin, _RlhfTunableModelMixin):\n    \"\"\"ChatModel represents a language model that is capable of chat.\n\n    Examples::\n\n        chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n\n        chat = chat_model.start_chat(\n            context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n            examples=[\n                InputOutputTextPair(\n                    input_text=\"Who do you work for?\",\n                    output_text=\"I work for Ned.\",\n                ),\n                InputOutputTextPair(\n                    input_text=\"What do I like?\",\n                    output_text=\"Ned likes watching movies.\",\n                ),\n            ],\n            temperature=0.3,\n        )\n\n        chat.send_message(\"Do you know any cool events this weekend?\")\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/chat_generation_1.0.0.yaml\"\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession","title":"ChatSession","text":"<p>               Bases: <code>_ChatSessionBase</code></p> <p>ChatSession represents a chat session with a language model.</p> <p>Within a chat session, the model keeps context and remembers the previous conversation.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class ChatSession(_ChatSessionBase):\n    \"\"\"ChatSession represents a chat session with a language model.\n\n    Within a chat session, the model keeps context and remembers the previous conversation.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    def __init__(\n        self,\n        model: ChatModel,\n        context: Optional[str] = None,\n        examples: Optional[List[InputOutputTextPair]] = None,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        message_history: Optional[List[ChatMessage]] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ):\n        super().__init__(\n            model=model,\n            context=context,\n            examples=examples,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            message_history=message_history,\n            stop_sequences=stop_sequences,\n        )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatModel","title":"CodeChatModel","text":"<p>               Bases: <code>_ChatModelBase</code>, <code>_TunableChatModelMixin</code></p> <p>CodeChatModel represents a model that is capable of completing code.</p> <p>Examples:</p> <p>code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")</p> <p>code_chat = code_chat_model.start_chat(     context=\"I'm writing a large-scale enterprise application.\",     max_output_tokens=128,     temperature=0.2, )</p> <p>code_chat.send_message(\"Please help write a function to calculate the min of two numbers\")</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class CodeChatModel(_ChatModelBase, _TunableChatModelMixin):\n    \"\"\"CodeChatModel represents a model that is capable of completing code.\n\n    Examples:\n        code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n\n        code_chat = code_chat_model.start_chat(\n            context=\"I'm writing a large-scale enterprise application.\",\n            max_output_tokens=128,\n            temperature=0.2,\n        )\n\n        code_chat.send_message(\"Please help write a function to calculate the min of two numbers\")\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/codechat_generation_1.0.0.yaml\"\n\n    def start_chat(\n        self,\n        *,\n        context: Optional[str] = None,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        message_history: Optional[List[ChatMessage]] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ) -&gt; \"CodeChatSession\":\n        \"\"\"Starts a chat session with the code chat model.\n\n        Args:\n            context: Context shapes how the model responds throughout the conversation.\n                For example, you can use context to specify words the model can or\n                cannot use, topics to focus on or avoid, or the response format or style.\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            temperature: Controls the randomness of predictions. Range: [0, 1].\n            stop_sequences: Customized stop sequences to stop the decoding process.\n\n        Returns:\n            A `ChatSession` object.\n        \"\"\"\n        return CodeChatSession(\n            model=self,\n            context=context,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            message_history=message_history,\n            stop_sequences=stop_sequences,\n        )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatModel.start_chat","title":"start_chat","text":"<pre><code>start_chat(\n    *,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; CodeChatSession\n</code></pre> <p>Starts a chat session with the code chat model.</p> PARAMETER DESCRIPTION <code>context</code> <p>Context shapes how the model responds throughout the conversation. For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000].</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CodeChatSession</code> <p>A <code>ChatSession</code> object.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def start_chat(\n    self,\n    *,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; \"CodeChatSession\":\n    \"\"\"Starts a chat session with the code chat model.\n\n    Args:\n        context: Context shapes how the model responds throughout the conversation.\n            For example, you can use context to specify words the model can or\n            cannot use, topics to focus on or avoid, or the response format or style.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n        stop_sequences: Customized stop sequences to stop the decoding process.\n\n    Returns:\n        A `ChatSession` object.\n    \"\"\"\n    return CodeChatSession(\n        model=self,\n        context=context,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        message_history=message_history,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession","title":"CodeChatSession","text":"<p>               Bases: <code>_ChatSessionBase</code></p> <p>CodeChatSession represents a chat session with code chat language model.</p> <p>Within a code chat session, the model keeps context and remembers the previous converstion.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>class CodeChatSession(_ChatSessionBase):\n    \"\"\"CodeChatSession represents a chat session with code chat language model.\n\n    Within a code chat session, the model keeps context and remembers the previous converstion.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    def __init__(\n        self,\n        model: CodeChatModel,\n        context: Optional[str] = None,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        message_history: Optional[List[ChatMessage]] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ):\n        super().__init__(\n            model=model,\n            context=context,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            message_history=message_history,\n            stop_sequences=stop_sequences,\n        )\n\n    def send_message(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        stop_sequences: Optional[List[str]] = None,\n        candidate_count: Optional[int] = None,\n    ) -&gt; \"MultiCandidateTextGenerationResponse\":\n        \"\"\"Sends message to the code chat model and gets a response.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n                Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1].\n                 Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            stop_sequences: Customized stop sequences to stop the decoding process.\n            candidate_count: Number of candidates to return.\n\n        Returns:\n            A `MultiCandidateTextGenerationResponse` object that contains the\n            text produced by the model.\n        \"\"\"\n        return super().send_message(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            stop_sequences=stop_sequences,\n            candidate_count=candidate_count,\n        )\n\n    async def send_message_async(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        candidate_count: Optional[int] = None,\n    ) -&gt; \"MultiCandidateTextGenerationResponse\":\n        \"\"\"Asynchronously sends message to the code chat model and gets a response.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n                Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1].\n                 Uses the value specified when calling `CodeChatModel.start_chat` by default.\n            candidate_count: Number of candidates to return.\n\n        Returns:\n            A `MultiCandidateTextGenerationResponse` object that contains the\n            text produced by the model.\n        \"\"\"\n        response = await super().send_message_async(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            candidate_count=candidate_count,\n        )\n        return response\n\n    def send_message_streaming(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ) -&gt; Iterator[TextGenerationResponse]:\n        \"\"\"Sends message to the language model and gets a streamed response.\n\n        The response is only added to the history once it's fully read.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            stop_sequences: Customized stop sequences to stop the decoding process.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n\n        Returns:\n            A stream of `TextGenerationResponse` objects that contain partial\n            responses produced by the model.\n        \"\"\"\n        return super().send_message_streaming(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            stop_sequences=stop_sequences,\n        )\n\n    def send_message_streaming_async(\n        self,\n        message: str,\n        *,\n        max_output_tokens: Optional[int] = None,\n        temperature: Optional[float] = None,\n        stop_sequences: Optional[List[str]] = None,\n    ) -&gt; AsyncIterator[TextGenerationResponse]:\n        \"\"\"Asynchronously sends message to the language model and gets a streamed response.\n\n        The response is only added to the history once it's fully read.\n\n        Args:\n            message: Message to send to the model\n            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n            stop_sequences: Customized stop sequences to stop the decoding process.\n                Uses the value specified when calling `ChatModel.start_chat` by default.\n\n        Returns:\n            A stream of `TextGenerationResponse` objects that contain partial\n            responses produced by the model.\n        \"\"\"\n        return super().send_message_streaming_async(\n            message=message,\n            max_output_tokens=max_output_tokens,\n            temperature=temperature,\n            stop_sequences=stop_sequences,\n        )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Sends message to the code chat model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000]. Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].  Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the</p> <code>MultiCandidateTextGenerationResponse</code> <p>text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Sends message to the code chat model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n             Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        candidate_count: Number of candidates to return.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the\n        text produced by the model.\n    \"\"\"\n    return super().send_message(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n        candidate_count=candidate_count,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message_async","title":"send_message_async  <code>async</code>","text":"<pre><code>send_message_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    candidate_count: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Asynchronously sends message to the code chat model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000]. Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].  Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the</p> <code>MultiCandidateTextGenerationResponse</code> <p>text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def send_message_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    candidate_count: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Asynchronously sends message to the code chat model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n             Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        candidate_count: Number of candidates to return.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the\n        text produced by the model.\n    \"\"\"\n    response = await super().send_message_async(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        candidate_count=candidate_count,\n    )\n    return response\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message_streaming","title":"send_message_streaming","text":"<pre><code>send_message_streaming(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; Iterator[TextGenerationResponse]\n</code></pre> <p>Sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TextGenerationResponse</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>TextGenerationResponse</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message_streaming(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; Iterator[TextGenerationResponse]:\n    \"\"\"Sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Returns:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    return super().send_message_streaming(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message_streaming_async","title":"send_message_streaming_async","text":"<pre><code>send_message_streaming_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; AsyncIterator[TextGenerationResponse]\n</code></pre> <p>Asynchronously sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AsyncIterator[TextGenerationResponse]</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>AsyncIterator[TextGenerationResponse]</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message_streaming_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; AsyncIterator[TextGenerationResponse]:\n    \"\"\"Asynchronously sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Returns:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    return super().send_message_streaming_async(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.InputOutputTextPair","title":"InputOutputTextPair  <code>dataclass</code>","text":"<p>InputOutputTextPair represents a pair of input and output texts.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass InputOutputTextPair:\n    \"\"\"InputOutputTextPair represents a pair of input and output texts.\"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    input_text: str\n    output_text: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbedding","title":"TextEmbedding  <code>dataclass</code>","text":"<p>Text embedding vector and statistics.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextEmbedding:\n    \"\"\"Text embedding vector and statistics.\"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    values: List[float]\n    statistics: Optional[TextEmbeddingStatistics] = None\n    _prediction_response: Optional[aiplatform.models.Prediction] = None\n\n    @classmethod\n    def _parse_text_embedding_response(\n        cls, prediction_response: aiplatform.models.Prediction, prediction_index: int\n    ) -&gt; \"TextEmbedding\":\n        \"\"\"Creates a `TextEmbedding` object from a prediction.\n\n        Args:\n            prediction_response: `aiplatform.models.Prediction` object.\n\n        Returns:\n            `TextEmbedding` object.\n        \"\"\"\n        prediction = prediction_response.predictions[prediction_index]\n        is_prediction_from_pretrained_models = isinstance(\n            prediction, collections.abc.Mapping\n        )\n        if is_prediction_from_pretrained_models:\n            embeddings = prediction[\"embeddings\"]\n            embedding_stats = embeddings[\"statistics\"]\n            return cls(\n                values=embeddings[\"values\"],\n                statistics=TextEmbeddingStatistics(\n                    token_count=embedding_stats[\"token_count\"],\n                    truncated=embedding_stats[\"truncated\"],\n                ),\n                _prediction_response=prediction_response,\n            )\n        else:\n            return cls(values=prediction, _prediction_response=prediction_response)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingInput","title":"TextEmbeddingInput  <code>dataclass</code>","text":"<p>Structural text embedding input.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The main text content to embed.</p> <p> TYPE: <code>str</code> </p> <code>task_type</code> <p>The name of the downstream task the embeddings will be used for. Valid values: RETRIEVAL_QUERY     Specifies the given text is a query in a search/retrieval setting. RETRIEVAL_DOCUMENT     Specifies the given text is a document from the corpus being searched. SEMANTIC_SIMILARITY     Specifies the given text will be used for STS. CLASSIFICATION     Specifies that the given text will be classified. CLUSTERING     Specifies that the embeddings will be used for clustering. QUESTION_ANSWERING     Specifies that the embeddings will be used for question answering. FACT_VERIFICATION     Specifies that the embeddings will be used for fact verification.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>title</code> <p>Optional identifier of the text content.</p> <p> TYPE: <code>Optional[str]</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextEmbeddingInput:\n    \"\"\"Structural text embedding input.\n\n    Attributes:\n        text: The main text content to embed.\n        task_type: The name of the downstream task the embeddings will be used for.\n            Valid values:\n            RETRIEVAL_QUERY\n                Specifies the given text is a query in a search/retrieval setting.\n            RETRIEVAL_DOCUMENT\n                Specifies the given text is a document from the corpus being searched.\n            SEMANTIC_SIMILARITY\n                Specifies the given text will be used for STS.\n            CLASSIFICATION\n                Specifies that the given text will be classified.\n            CLUSTERING\n                Specifies that the embeddings will be used for clustering.\n            QUESTION_ANSWERING\n                Specifies that the embeddings will be used for question answering.\n            FACT_VERIFICATION\n                Specifies that the embeddings will be used for fact verification.\n        title: Optional identifier of the text content.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    text: str\n    task_type: Optional[str] = None\n    title: Optional[str] = None\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse","title":"TextGenerationResponse  <code>dataclass</code>","text":"<p>TextGenerationResponse represents a response of a language model. Attributes:     text: The generated text     is_blocked: Whether the the request was blocked.     errors: The error codes indicate why the response was blocked.         Learn more information about safety errors here:         this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors     safety_attributes: Scores for safety attributes.         Learn more about the safety attributes here:         https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions     grounding_metadata: Metadata for grounding.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextGenerationResponse:\n    \"\"\"TextGenerationResponse represents a response of a language model.\n    Attributes:\n        text: The generated text\n        is_blocked: Whether the the request was blocked.\n        errors: The error codes indicate why the response was blocked.\n            Learn more information about safety errors here:\n            this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors\n        safety_attributes: Scores for safety attributes.\n            Learn more about the safety attributes here:\n            https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions\n        grounding_metadata: Metadata for grounding.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    text: str\n    _prediction_response: Any\n    is_blocked: bool = False\n    errors: Tuple[int] = tuple()\n    safety_attributes: Dict[str, float] = dataclasses.field(default_factory=dict)\n    grounding_metadata: Optional[GroundingMetadata] = None\n\n    def __repr__(self):\n        if self.text:\n            return self.text\n        # Falling back to the full representation\n        elif self.grounding_metadata is not None:\n            return (\n                \"TextGenerationResponse(\"\n                f\"text={self.text!r}\"\n                f\", is_blocked={self.is_blocked!r}\"\n                f\", errors={self.errors!r}\"\n                f\", safety_attributes={self.safety_attributes!r}\"\n                f\", grounding_metadata={self.grounding_metadata!r}\"\n                \")\"\n            )\n        else:\n            return (\n                \"TextGenerationResponse(\"\n                f\"text={self.text!r}\"\n                f\", is_blocked={self.is_blocked!r}\"\n                f\", errors={self.errors!r}\"\n                f\", safety_attributes={self.safety_attributes!r}\"\n                \")\"\n            )\n\n    @property\n    def raw_prediction_response(self) -&gt; aiplatform.models.Prediction:\n        \"\"\"Raw prediction response.\"\"\"\n        return self._prediction_response\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse.raw_prediction_response","title":"raw_prediction_response  <code>property</code>","text":"<pre><code>raw_prediction_response: Prediction\n</code></pre> <p>Raw prediction response.</p>"},{"location":"vertexai/vision_models/","title":"Vision models","text":"<p>Classes for working with vision models.</p>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image","title":"Image","text":"<p>Image.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class Image:\n    \"\"\"Image.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _loaded_bytes: Optional[bytes] = None\n    _loaded_image: Optional[\"PIL_Image.Image\"] = None\n    _gcs_uri: Optional[str] = None\n\n    def __init__(\n        self,\n        image_bytes: Optional[bytes] = None,\n        gcs_uri: Optional[str] = None,\n    ):\n        \"\"\"Creates an `Image` object.\n\n        Args:\n            image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n            gcs_uri: Image URI in Google Cloud Storage.\n        \"\"\"\n        if bool(image_bytes) == bool(gcs_uri):\n            raise ValueError(\"Either image_bytes or gcs_uri must be provided.\")\n\n        self._image_bytes = image_bytes\n        self._gcs_uri = gcs_uri\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"Image\":\n        \"\"\"Loads image from local file or Google Cloud Storage.\n\n        Args:\n            location: Local path or Google Cloud Storage uri from where to load\n                the image.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        parsed_url = urllib.parse.urlparse(location)\n        if (\n            parsed_url.scheme == \"https\"\n            and parsed_url.netloc == \"storage.googleapis.com\"\n        ):\n            parsed_url = parsed_url._replace(\n                scheme=\"gs\", netloc=\"\", path=f\"/{urllib.parse.unquote(parsed_url.path)}\"\n            )\n            location = urllib.parse.urlunparse(parsed_url)\n\n        if parsed_url.scheme == \"gs\":\n            return Image(gcs_uri=location)\n\n        # Load image from local path\n        image_bytes = pathlib.Path(location).read_bytes()\n        image = Image(image_bytes=image_bytes)\n        return image\n\n    @property\n    def _blob(self) -&gt; storage.Blob:\n        if self._gcs_uri is None:\n            raise AttributeError(\"_blob is only supported when gcs_uri is set.\")\n        storage_client = storage.Client(\n            credentials=aiplatform_initializer.global_config.credentials\n        )\n        blob = storage.Blob.from_string(uri=self._gcs_uri, client=storage_client)\n        # Needed to populate `blob.content_type`\n        blob.reload()\n        return blob\n\n    @property\n    def _image_bytes(self) -&gt; bytes:\n        if self._loaded_bytes is None:\n            self._loaded_bytes = self._blob.download_as_bytes()\n        return self._loaded_bytes\n\n    @_image_bytes.setter\n    def _image_bytes(self, value: bytes):\n        self._loaded_bytes = value\n\n    @property\n    def _pil_image(self) -&gt; \"PIL_Image.Image\":\n        if self._loaded_image is None:\n            if not PIL_Image:\n                raise RuntimeError(\n                    \"The PIL module is not available. Please install the Pillow package.\"\n                )\n            self._loaded_image = PIL_Image.open(io.BytesIO(self._image_bytes))\n        return self._loaded_image\n\n    @property\n    def _size(self):\n        return self._pil_image.size\n\n    @property\n    def _mime_type(self) -&gt; str:\n        \"\"\"Returns the MIME type of the image.\"\"\"\n        if self._gcs_uri:\n            return self._blob.content_type\n        if PIL_Image:\n            return PIL_Image.MIME.get(self._pil_image.format, \"image/jpeg\")\n        # Fall back to jpeg\n        return \"image/jpeg\"\n\n    def show(self):\n        \"\"\"Shows the image.\n\n        This method only works when in a notebook environment.\n        \"\"\"\n        if PIL_Image and IPython_display:\n            IPython_display.display(self._pil_image)\n\n    def save(self, location: str):\n        \"\"\"Saves image to a file.\n\n        Args:\n            location: Local path where to save the image.\n        \"\"\"\n        pathlib.Path(location).write_bytes(self._image_bytes)\n\n    def _as_base64_string(self) -&gt; str:\n        \"\"\"Encodes image using the base64 encoding.\n\n        Returns:\n            Base64 encoding of the image as a string.\n        \"\"\"\n        # ! b64encode returns `bytes` object, not `str`.\n        # We need to convert `bytes` to `str`, otherwise we get service error:\n        # \"received initial metadata size exceeds limit\"\n        return base64.b64encode(self._image_bytes).decode(\"ascii\")\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image.__init__","title":"__init__","text":"<pre><code>__init__(\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>image_bytes</code> <p>Image file bytes. Image can be in PNG or JPEG format.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(image_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either image_bytes or gcs_uri must be provided.\")\n\n    self._image_bytes = image_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(location)\n    if (\n        parsed_url.scheme == \"https\"\n        and parsed_url.netloc == \"storage.googleapis.com\"\n    ):\n        parsed_url = parsed_url._replace(\n            scheme=\"gs\", netloc=\"\", path=f\"/{urllib.parse.unquote(parsed_url.path)}\"\n        )\n        location = urllib.parse.urlunparse(parsed_url)\n\n    if parsed_url.scheme == \"gs\":\n        return Image(gcs_uri=location)\n\n    # Load image from local path\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image(image_bytes=image_bytes)\n    return image\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves image to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the image.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves image to a file.\n\n    Args:\n        location: Local path where to save the image.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._image_bytes)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image.show","title":"show","text":"<pre><code>show()\n</code></pre> <p>Shows the image.</p> <p>This method only works when in a notebook environment.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def show(self):\n    \"\"\"Shows the image.\n\n    This method only works when in a notebook environment.\n    \"\"\"\n    if PIL_Image and IPython_display:\n        IPython_display.display(self._pil_image)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageCaptioningModel","title":"ImageCaptioningModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates captions from image.</p> <p>Examples::</p> <pre><code>model = ImageCaptioningModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class ImageCaptioningModel(\n    _model_garden_models._ModelGardenModel  # pylint: disable=protected-access\n):\n    \"\"\"Generates captions from image.\n\n    Examples::\n\n        model = ImageCaptioningModel.from_pretrained(\"imagetext@001\")\n        image = Image.load_from_file(\"image.png\")\n        captions = model.get_captions(\n            image=image,\n            # Optional:\n            number_of_results=1,\n            language=\"en\",\n        )\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml\"\n\n    def get_captions(\n        self,\n        image: Image,\n        *,\n        number_of_results: int = 1,\n        language: str = \"en\",\n        output_gcs_uri: Optional[str] = None,\n    ) -&gt; List[str]:\n        \"\"\"Generates captions for a given image.\n\n        Args:\n            image: The image to get captions for. Size limit: 10 MB.\n            number_of_results: Number of captions to produce. Range: 1-3.\n            language: Language to use for captions.\n                Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n            output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n        Returns:\n            A list of image caption strings.\n        \"\"\"\n        instance = {}\n\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n        parameters = {\n            \"sampleCount\": number_of_results,\n            \"language\": language,\n        }\n        if output_gcs_uri is not None:\n            parameters[\"storageUri\"] = output_gcs_uri\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n        return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageCaptioningModel.get_captions","title":"get_captions","text":"<pre><code>get_captions(\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None\n) -&gt; List[str]\n</code></pre> <p>Generates captions for a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>language</code> <p>Language to use for captions. Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the captioned images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of image caption strings.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_captions(\n    self,\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None,\n) -&gt; List[str]:\n    \"\"\"Generates captions for a given image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        number_of_results: Number of captions to produce. Range: 1-3.\n        language: Language to use for captions.\n            Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n        output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n    Returns:\n        A list of image caption strings.\n    \"\"\"\n    instance = {}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n        \"language\": language,\n    }\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageQnAModel","title":"ImageQnAModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Answers questions about an image.</p> <p>Examples::</p> <pre><code>model = ImageQnAModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class ImageQnAModel(\n    _model_garden_models._ModelGardenModel  # pylint: disable=protected-access\n):\n    \"\"\"Answers questions about an image.\n\n    Examples::\n\n        model = ImageQnAModel.from_pretrained(\"imagetext@001\")\n        image = Image.load_from_file(\"image.png\")\n        answers = model.ask_question(\n            image=image,\n            question=\"What color is the car in this image?\",\n            # Optional:\n            number_of_results=1,\n        )\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml\"\n\n    def ask_question(\n        self,\n        image: Image,\n        question: str,\n        *,\n        number_of_results: int = 1,\n    ) -&gt; List[str]:\n        \"\"\"Answers questions about an image.\n\n        Args:\n            image: The image to get captions for. Size limit: 10 MB.\n            question: Question to ask about the image.\n            number_of_results: Number of captions to produce. Range: 1-3.\n\n        Returns:\n            A list of answers.\n        \"\"\"\n        instance = {\"prompt\": question}\n\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n        parameters = {\n            \"sampleCount\": number_of_results,\n        }\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n        return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageQnAModel.ask_question","title":"ask_question","text":"<pre><code>ask_question(\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1\n) -&gt; List[str]\n</code></pre> <p>Answers questions about an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>question</code> <p>Question to ask about the image.</p> <p> TYPE: <code>str</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of answers.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def ask_question(\n    self,\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1,\n) -&gt; List[str]:\n    \"\"\"Answers questions about an image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        question: Question to ask about the image.\n        number_of_results: Number of captions to produce. Range: 1-3.\n\n    Returns:\n        A list of answers.\n    \"\"\"\n    instance = {\"prompt\": question}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n    }\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageTextModel","title":"ImageTextModel","text":"<p>               Bases: <code>ImageCaptioningModel</code>, <code>ImageQnAModel</code></p> <p>Generates text from images.</p> <p>Examples::</p> <pre><code>model = ImageTextModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\n\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class ImageTextModel(ImageCaptioningModel, ImageQnAModel):\n    \"\"\"Generates text from images.\n\n    Examples::\n\n        model = ImageTextModel.from_pretrained(\"imagetext@001\")\n        image = Image.load_from_file(\"image.png\")\n\n        captions = model.get_captions(\n            image=image,\n            # Optional:\n            number_of_results=1,\n            language=\"en\",\n        )\n\n        answers = model.ask_question(\n            image=image,\n            question=\"What color is the car in this image?\",\n            # Optional:\n            number_of_results=1,\n        )\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    # NOTE: Using this ImageTextModel class is recommended over using ImageQnAModel or ImageCaptioningModel,\n    # since SDK Model Garden classes should follow the design pattern of exactly 1 SDK class to 1 Model Garden schema URI\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml\"\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingModel","title":"MultiModalEmbeddingModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates embedding vectors from images and videos.</p> <p>Examples::</p> <pre><code>model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\nimage = Image.load_from_file(\"image.png\")\nvideo = Video.load_from_file(\"video.mp4\")\n\nembeddings = model.get_embeddings(\n    image=image,\n    video=video,\n    contextual_text=\"Hello world\",\n)\nimage_embedding = embeddings.image_embedding\nvideo_embeddings = embeddings.video_embeddings\ntext_embedding = embeddings.text_embedding\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class MultiModalEmbeddingModel(_model_garden_models._ModelGardenModel):\n    \"\"\"Generates embedding vectors from images and videos.\n\n    Examples::\n\n        model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\n        image = Image.load_from_file(\"image.png\")\n        video = Video.load_from_file(\"video.mp4\")\n\n        embeddings = model.get_embeddings(\n            image=image,\n            video=video,\n            contextual_text=\"Hello world\",\n        )\n        image_embedding = embeddings.image_embedding\n        video_embeddings = embeddings.video_embeddings\n        text_embedding = embeddings.text_embedding\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_embedding_model_1.0.0.yaml\"\n\n    def get_embeddings(\n        self,\n        image: Optional[Image] = None,\n        video: Optional[Video] = None,\n        contextual_text: Optional[str] = None,\n        dimension: Optional[int] = None,\n        video_segment_config: Optional[VideoSegmentConfig] = None,\n    ) -&gt; \"MultiModalEmbeddingResponse\":\n        \"\"\"Gets embedding vectors from the provided image.\n\n        Args:\n            image (Image): Optional. The image to generate embeddings for. One of\n              `image`, `video`, or `contextual_text` is required.\n            video (Video): Optional. The video to generate embeddings for. One of\n              `image`, `video` or `contextual_text` is required.\n            contextual_text (str): Optional. Contextual text for your input image or video.\n              If provided, the model will also generate an embedding vector for the\n              provided contextual text. The returned image and text embedding\n              vectors are in the same semantic space with the same dimensionality,\n              and the vectors can be used interchangeably for use cases like\n              searching image by text or searching text by image. One of `image`, `video` or\n              `contextual_text` is required.\n            dimension (int): Optional. The number of embedding dimensions. Lower\n              values offer decreased latency when using these embeddings for\n              subsequent tasks, while higher values offer better accuracy.\n              Available values: `128`, `256`, `512`, and `1408` (default).\n            video_segment_config (VideoSegmentConfig): Optional. The specific\n              video segments (in seconds) the embeddings are generated for.\n\n        Returns:\n            MultiModalEmbeddingResponse:\n                The image and text embedding vectors.\n        \"\"\"\n\n        if not image and not video and not contextual_text:\n            raise ValueError(\n                \"One of `image`, `video`, or `contextual_text` is required.\"\n            )\n\n        instance = {}\n\n        if image:\n            if image._gcs_uri:  # pylint: disable=protected-access\n                instance[\"image\"] = {\n                    \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n                }\n            else:\n                instance[\"image\"] = {\n                    \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n                }\n\n        if video:\n            if video._gcs_uri:  # pylint: disable=protected-access\n                instance[\"video\"] = {\n                    \"gcsUri\": video._gcs_uri  # pylint: disable=protected-access\n                }\n            else:\n                instance[\"video\"] = {\n                    \"bytesBase64Encoded\": video._as_base64_string()  # pylint: disable=protected-access\n                }  # pylint: disable=protected-access\n\n            if video_segment_config:\n                instance[\"video\"][\"videoSegmentConfig\"] = {\n                    \"startOffsetSec\": video_segment_config.start_offset_sec,\n                    \"endOffsetSec\": video_segment_config.end_offset_sec,\n                    \"intervalSec\": video_segment_config.interval_sec,\n                }\n\n        if contextual_text:\n            instance[\"text\"] = contextual_text\n\n        parameters = {}\n        if dimension:\n            parameters[\"dimension\"] = dimension\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n        image_embedding = response.predictions[0].get(\"imageEmbedding\")\n        video_embeddings = []\n        for video_embedding in response.predictions[0].get(\"videoEmbeddings\", []):\n            video_embeddings.append(\n                VideoEmbedding(\n                    embedding=video_embedding[\"embedding\"],\n                    start_offset_sec=video_embedding[\"startOffsetSec\"],\n                    end_offset_sec=video_embedding[\"endOffsetSec\"],\n                )\n            )\n        text_embedding = (\n            response.predictions[0].get(\"textEmbedding\")\n            if \"textEmbedding\" in response.predictions[0]\n            else None\n        )\n        return MultiModalEmbeddingResponse(\n            image_embedding=image_embedding,\n            video_embeddings=video_embeddings,\n            _prediction_response=response,\n            text_embedding=text_embedding,\n        )\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[\n        VideoSegmentConfig\n    ] = None,\n) -&gt; MultiModalEmbeddingResponse\n</code></pre> <p>Gets embedding vectors from the provided image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Optional. The image to generate embeddings for. One of <code>image</code>, <code>video</code>, or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Image</code> DEFAULT: <code>None</code> </p> <code>video</code> <p>Optional. The video to generate embeddings for. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Video</code> DEFAULT: <code>None</code> </p> <code>contextual_text</code> <p>Optional. Contextual text for your input image or video. If provided, the model will also generate an embedding vector for the provided contextual text. The returned image and text embedding vectors are in the same semantic space with the same dimensionality, and the vectors can be used interchangeably for use cases like searching image by text or searching text by image. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>dimension</code> <p>Optional. The number of embedding dimensions. Lower values offer decreased latency when using these embeddings for subsequent tasks, while higher values offer better accuracy. Available values: <code>128</code>, <code>256</code>, <code>512</code>, and <code>1408</code> (default).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>video_segment_config</code> <p>Optional. The specific video segments (in seconds) the embeddings are generated for.</p> <p> TYPE: <code>VideoSegmentConfig</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiModalEmbeddingResponse</code> <p>The image and text embedding vectors.</p> <p> TYPE: <code>MultiModalEmbeddingResponse</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_embeddings(\n    self,\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[VideoSegmentConfig] = None,\n) -&gt; \"MultiModalEmbeddingResponse\":\n    \"\"\"Gets embedding vectors from the provided image.\n\n    Args:\n        image (Image): Optional. The image to generate embeddings for. One of\n          `image`, `video`, or `contextual_text` is required.\n        video (Video): Optional. The video to generate embeddings for. One of\n          `image`, `video` or `contextual_text` is required.\n        contextual_text (str): Optional. Contextual text for your input image or video.\n          If provided, the model will also generate an embedding vector for the\n          provided contextual text. The returned image and text embedding\n          vectors are in the same semantic space with the same dimensionality,\n          and the vectors can be used interchangeably for use cases like\n          searching image by text or searching text by image. One of `image`, `video` or\n          `contextual_text` is required.\n        dimension (int): Optional. The number of embedding dimensions. Lower\n          values offer decreased latency when using these embeddings for\n          subsequent tasks, while higher values offer better accuracy.\n          Available values: `128`, `256`, `512`, and `1408` (default).\n        video_segment_config (VideoSegmentConfig): Optional. The specific\n          video segments (in seconds) the embeddings are generated for.\n\n    Returns:\n        MultiModalEmbeddingResponse:\n            The image and text embedding vectors.\n    \"\"\"\n\n    if not image and not video and not contextual_text:\n        raise ValueError(\n            \"One of `image`, `video`, or `contextual_text` is required.\"\n        )\n\n    instance = {}\n\n    if image:\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n\n    if video:\n        if video._gcs_uri:  # pylint: disable=protected-access\n            instance[\"video\"] = {\n                \"gcsUri\": video._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"video\"] = {\n                \"bytesBase64Encoded\": video._as_base64_string()  # pylint: disable=protected-access\n            }  # pylint: disable=protected-access\n\n        if video_segment_config:\n            instance[\"video\"][\"videoSegmentConfig\"] = {\n                \"startOffsetSec\": video_segment_config.start_offset_sec,\n                \"endOffsetSec\": video_segment_config.end_offset_sec,\n                \"intervalSec\": video_segment_config.interval_sec,\n            }\n\n    if contextual_text:\n        instance[\"text\"] = contextual_text\n\n    parameters = {}\n    if dimension:\n        parameters[\"dimension\"] = dimension\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    image_embedding = response.predictions[0].get(\"imageEmbedding\")\n    video_embeddings = []\n    for video_embedding in response.predictions[0].get(\"videoEmbeddings\", []):\n        video_embeddings.append(\n            VideoEmbedding(\n                embedding=video_embedding[\"embedding\"],\n                start_offset_sec=video_embedding[\"startOffsetSec\"],\n                end_offset_sec=video_embedding[\"endOffsetSec\"],\n            )\n        )\n    text_embedding = (\n        response.predictions[0].get(\"textEmbedding\")\n        if \"textEmbedding\" in response.predictions[0]\n        else None\n    )\n    return MultiModalEmbeddingResponse(\n        image_embedding=image_embedding,\n        video_embeddings=video_embeddings,\n        _prediction_response=response,\n        text_embedding=text_embedding,\n    )\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingResponse","title":"MultiModalEmbeddingResponse  <code>dataclass</code>","text":"<p>The multimodal embedding response.</p> ATTRIBUTE DESCRIPTION <code>image_embedding</code> <p>Optional. The embedding vector generated from your image.</p> <p> TYPE: <code>List[float]</code> </p> <code>video_embeddings</code> <p>Optional. The embedding vectors generated from your video.</p> <p> TYPE: <code>List[VideoEmbedding]</code> </p> <code>text_embedding</code> <p>Optional. The embedding vector generated from the contextual text provided for your image or video.</p> <p> TYPE: <code>List[float]</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@dataclasses.dataclass\nclass MultiModalEmbeddingResponse:\n    \"\"\"The multimodal embedding response.\n\n    Attributes:\n        image_embedding (List[float]):\n            Optional. The embedding vector generated from your image.\n        video_embeddings (List[VideoEmbedding]):\n            Optional. The embedding vectors generated from your video.\n        text_embedding (List[float]):\n            Optional. The embedding vector generated from the contextual text provided for your image or video.\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _prediction_response: Any\n    image_embedding: Optional[List[float]] = None\n    video_embeddings: Optional[List[VideoEmbedding]] = None\n    text_embedding: Optional[List[float]] = None\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Video","title":"Video","text":"<p>Video.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class Video:\n    \"\"\"Video.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _loaded_bytes: Optional[bytes] = None\n    _gcs_uri: Optional[str] = None\n\n    def __init__(\n        self,\n        video_bytes: Optional[bytes] = None,\n        gcs_uri: Optional[str] = None,\n    ):\n        \"\"\"Creates an `Image` object.\n\n        Args:\n            video_bytes: Video file bytes. Video can be in AVI, FLV, MKV, MOV,\n                MP4, MPEG, MPG, WEBM, and WMV formats.\n            gcs_uri: Image URI in Google Cloud Storage.\n        \"\"\"\n        if bool(video_bytes) == bool(gcs_uri):\n            raise ValueError(\"Either video_bytes or gcs_uri must be provided.\")\n\n        self._video_bytes = video_bytes\n        self._gcs_uri = gcs_uri\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"Video\":\n        \"\"\"Loads video from local file or Google Cloud Storage.\n\n        Args:\n            location: Local path or Google Cloud Storage uri from where to load\n                the video.\n\n        Returns:\n            Loaded video as an `Video` object.\n        \"\"\"\n        if location.startswith(\"gs://\"):\n            return Video(gcs_uri=location)\n\n        video_bytes = pathlib.Path(location).read_bytes()\n        video = Video(video_bytes=video_bytes)\n        return video\n\n    @property\n    def _blob(self) -&gt; storage.Blob:\n        if self._gcs_uri is None:\n            raise AttributeError(\"_blob is only supported when gcs_uri is set.\")\n        storage_client = storage.Client(\n            credentials=aiplatform_initializer.global_config.credentials\n        )\n        blob = storage.Blob.from_string(uri=self._gcs_uri, client=storage_client)\n        # Needed to populate `blob.content_type`\n        blob.reload()\n        return blob\n\n    @property\n    def _video_bytes(self) -&gt; bytes:\n        if self._loaded_bytes is None:\n            self._loaded_bytes = self._blob.download_as_bytes()\n        return self._loaded_bytes\n\n    @_video_bytes.setter\n    def _video_bytes(self, value: bytes):\n        self._loaded_bytes = value\n\n    @property\n    def _mime_type(self) -&gt; str:\n        \"\"\"Returns the MIME type of the video.\"\"\"\n        if self._gcs_uri:\n            return self._blob.content_type\n        # Fall back to mp4\n        return \"video/mp4\"\n\n    def save(self, location: str):\n        \"\"\"Saves video to a file.\n\n        Args:\n            location: Local path where to save the video.\n        \"\"\"\n        pathlib.Path(location).write_bytes(self._video_bytes)\n\n    def _as_base64_string(self) -&gt; str:\n        \"\"\"Encodes video using the base64 encoding.\n\n        Returns:\n            Base64 encoding of the video as a string.\n        \"\"\"\n        # ! b64encode returns `bytes` object, not `str`.\n        # We need to convert `bytes` to `str`, otherwise we get service error:\n        # \"received initial metadata size exceeds limit\"\n        return base64.b64encode(self._video_bytes).decode(\"ascii\")\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Video.__init__","title":"__init__","text":"<pre><code>__init__(\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>video_bytes</code> <p>Video file bytes. Video can be in AVI, FLV, MKV, MOV, MP4, MPEG, MPG, WEBM, and WMV formats.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        video_bytes: Video file bytes. Video can be in AVI, FLV, MKV, MOV,\n            MP4, MPEG, MPG, WEBM, and WMV formats.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(video_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either video_bytes or gcs_uri must be provided.\")\n\n    self._video_bytes = video_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Video.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Video\n</code></pre> <p>Loads video from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the video.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Video</code> <p>Loaded video as an <code>Video</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Video\":\n    \"\"\"Loads video from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the video.\n\n    Returns:\n        Loaded video as an `Video` object.\n    \"\"\"\n    if location.startswith(\"gs://\"):\n        return Video(gcs_uri=location)\n\n    video_bytes = pathlib.Path(location).read_bytes()\n    video = Video(video_bytes=video_bytes)\n    return video\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Video.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves video to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the video.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves video to a file.\n\n    Args:\n        location: Local path where to save the video.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._video_bytes)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoEmbedding","title":"VideoEmbedding","text":"<p>Embeddings generated from video with offset times.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class VideoEmbedding:\n    \"\"\"Embeddings generated from video with offset times.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    start_offset_sec: int\n    end_offset_sec: int\n    embedding: List[float]\n\n    def __init__(\n        self, start_offset_sec: int, end_offset_sec: int, embedding: List[float]\n    ):\n        \"\"\"Creates a `VideoEmbedding` object.\n\n        Args:\n            start_offset_sec: Start time offset (in seconds) of generated embeddings.\n            end_offset_sec: End time offset (in seconds) of generated embeddings.\n            embedding: Generated embedding for interval.\n        \"\"\"\n        self.start_offset_sec = start_offset_sec\n        self.end_offset_sec = end_offset_sec\n        self.embedding = embedding\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(\n    start_offset_sec: int,\n    end_offset_sec: int,\n    embedding: List[float],\n)\n</code></pre> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>embedding</code> <p>Generated embedding for interval.</p> <p> TYPE: <code>List[float]</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self, start_offset_sec: int, end_offset_sec: int, embedding: List[float]\n):\n    \"\"\"Creates a `VideoEmbedding` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) of generated embeddings.\n        end_offset_sec: End time offset (in seconds) of generated embeddings.\n        embedding: Generated embedding for interval.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.embedding = embedding\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoSegmentConfig","title":"VideoSegmentConfig","text":"<p>The specific video segments (in seconds) the embeddings are generated for.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class VideoSegmentConfig:\n    \"\"\"The specific video segments (in seconds) the embeddings are generated for.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    start_offset_sec: int\n    end_offset_sec: int\n    interval_sec: int\n\n    def __init__(\n        self,\n        start_offset_sec: int = 0,\n        end_offset_sec: int = 120,\n        interval_sec: int = 16,\n    ):\n        \"\"\"Creates a `VideoSegmentConfig` object.\n\n        Args:\n            start_offset_sec: Start time offset (in seconds) to generate embeddings for.\n            end_offset_sec: End time offset (in seconds) to generate embeddings for.\n            interval_sec: Interval to divide video for generated embeddings.\n        \"\"\"\n        self.start_offset_sec = start_offset_sec\n        self.end_offset_sec = end_offset_sec\n        self.interval_sec = interval_sec\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoSegmentConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n)\n</code></pre> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>120</code> </p> <code>interval_sec</code> <p>Interval to divide video for generated embeddings.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n):\n    \"\"\"Creates a `VideoSegmentConfig` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) to generate embeddings for.\n        end_offset_sec: End time offset (in seconds) to generate embeddings for.\n        interval_sec: Interval to divide video for generated embeddings.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.interval_sec = interval_sec\n</code></pre>"},{"location":"vertexai/preview/","title":"Index","text":""},{"location":"vertexai/preview/generative_models/","title":"Generative models","text":"<p>Classes for working with the Gemini models.</p>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.AutomaticFunctionCallingResponder","title":"AutomaticFunctionCallingResponder","text":"<p>Responder that automatically responds to model's function calls.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class AutomaticFunctionCallingResponder:\n    \"\"\"Responder that automatically responds to model's function calls.\"\"\"\n\n    def __init__(self, max_automatic_function_calls: int = 1):\n        \"\"\"Initializes the responder.\n\n        Args:\n            max_automatic_function_calls: Maximum number of automatic function calls.\n        \"\"\"\n        if not (max_automatic_function_calls &gt; 0):\n            raise ValueError(\"max_automatic_function_calls must be positive.\")\n        self._max_automatic_function_calls = max_automatic_function_calls\n\n    def _create_responder_for_message(self, tools: List[Tool]) -&gt; \"_MessageResponder\":\n        return AutomaticFunctionCallingResponder._MessageResponder(\n            tools=tools,\n            max_automatic_function_calls=self._max_automatic_function_calls,\n        )\n\n    class _MessageResponder:\n        \"\"\"Automatic Function Calling responder for a single user message.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            tools: List[Tool],\n            max_automatic_function_calls: int = 1,\n            **_,\n        ):\n            self._tools = tools\n            self._max_automatic_function_calls = max_automatic_function_calls\n            self._remaining_function_calls = max_automatic_function_calls\n\n        def respond_to_model_response(\n            self,\n            *,\n            response: \"GenerationResponse\",\n            **_,\n        ) -&gt; Optional[\"Content\"]:\n            \"\"\"Responds to model's response.\n\n            Args:\n                response: Model's response that can be auto-responded.\n\n            Returns:\n                Optional response to model's response.\n            \"\"\"\n            chosen_candidate = response.candidates[0]\n            function_calls = chosen_candidate.function_calls\n            if not function_calls:\n                return None\n            tools = self._tools or self._model._tools\n            function_response_parts = []\n            for function_call in function_calls:\n                if self._remaining_function_calls &gt; 0:\n                    self._remaining_function_calls -= 1\n                else:\n                    raise RuntimeError(\n                        f\"Exceeded the maximum number of automatic function calls ({self._max_automatic_function_calls}).\"\n                        \" If more automatic function calls are needed, set `max_automatic_function_calls` to a higher number.\"\n                        f\" The last function calls: {function_calls}\"\n                    )\n                callable_function = None\n                for tool in tools:\n                    new_callable_function = tool._callable_functions.get(\n                        function_call.name\n                    )\n                    if new_callable_function and callable_function:\n                        raise ValueError(\n                            \"Multiple functions with the same name are not supported.\"\n                            f\" Found {callable_function} and {new_callable_function}.\"\n                        )\n                    callable_function = new_callable_function\n                if not callable_function:\n                    raise RuntimeError(\n                        f\"\"\"Model has asked to call function \"{function_call.name}\" which was not found.\"\"\"\n                    )\n\n                try:\n                    # We cannot use `function_args = type(function_call.args).to_dict(function_call.args)`\n                    # due to: AttributeError: type object 'MapComposite' has no attribute 'to_dict'\n                    function_args = type(function_call).to_dict(function_call)[\"args\"]\n                    function_call_result = callable_function._function(**function_args)\n                except Exception as ex:\n                    raise RuntimeError(\n                        f\"\"\"Error raised when calling function \"{function_call.name}\" as requested by the model.\"\"\"\n                    ) from ex\n                function_response_part = Part.from_function_response(\n                    name=function_call.name,\n                    response=function_call_result,\n                )\n                function_response_parts.append(function_response_part)\n            function_response_content = Content(\n                parts=function_response_parts,\n            )\n            return function_response_content\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.AutomaticFunctionCallingResponder.__init__","title":"__init__","text":"<pre><code>__init__(max_automatic_function_calls: int = 1)\n</code></pre> PARAMETER DESCRIPTION <code>max_automatic_function_calls</code> <p>Maximum number of automatic function calls.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self, max_automatic_function_calls: int = 1):\n    \"\"\"Initializes the responder.\n\n    Args:\n        max_automatic_function_calls: Maximum number of automatic function calls.\n    \"\"\"\n    if not (max_automatic_function_calls &gt; 0):\n        raise ValueError(\"max_automatic_function_calls must be positive.\")\n    self._max_automatic_function_calls = max_automatic_function_calls\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.CallableFunctionDeclaration","title":"CallableFunctionDeclaration","text":"<p>               Bases: <code>FunctionDeclaration</code></p> <p>A function declaration plus a function.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class CallableFunctionDeclaration(FunctionDeclaration):\n    \"\"\"A function declaration plus a function.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        function: Callable[..., Any],\n        parameters: Dict[str, Any],\n        description: Optional[str] = None,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            parameters=parameters,\n        )\n        self._function = function\n\n    @classmethod\n    def from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n        \"\"\"Automatically creates a CallableFunctionDeclaration from a Python function.\n\n        The function parameter schema is automatically extracted.\n        Args:\n          func: The function from which to extract schema.\n\n        Returns:\n            CallableFunctionDeclaration.\n        \"\"\"\n        from vertexai.generative_models import (\n            _function_calling_utils,\n        )\n\n        function_schema = _function_calling_utils.generate_json_schema_from_function(\n            func\n        )\n        # Getting out the description first since it will be removed from the schema.\n        function_description = function_schema[\"description\"]\n        function_schema = (\n            _function_calling_utils.adapt_json_schema_to_google_tool_schema(\n                function_schema\n            )\n        )\n\n        return CallableFunctionDeclaration(\n            name=func.__name__,\n            function=func,\n            description=function_description,\n            parameters=function_schema,\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.CallableFunctionDeclaration.from_func","title":"from_func  <code>classmethod</code>","text":"<pre><code>from_func(\n    func: Callable[..., Any]\n) -&gt; CallableFunctionDeclaration\n</code></pre> <p>Automatically creates a CallableFunctionDeclaration from a Python function.</p> <p>The function parameter schema is automatically extracted. Args:   func: The function from which to extract schema.</p> RETURNS DESCRIPTION <code>CallableFunctionDeclaration</code> <p>CallableFunctionDeclaration.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n    \"\"\"Automatically creates a CallableFunctionDeclaration from a Python function.\n\n    The function parameter schema is automatically extracted.\n    Args:\n      func: The function from which to extract schema.\n\n    Returns:\n        CallableFunctionDeclaration.\n    \"\"\"\n    from vertexai.generative_models import (\n        _function_calling_utils,\n    )\n\n    function_schema = _function_calling_utils.generate_json_schema_from_function(\n        func\n    )\n    # Getting out the description first since it will be removed from the schema.\n    function_description = function_schema[\"description\"]\n    function_schema = (\n        _function_calling_utils.adapt_json_schema_to_google_tool_schema(\n            function_schema\n        )\n    )\n\n    return CallableFunctionDeclaration(\n        name=func.__name__,\n        function=func,\n        description=function_description,\n        parameters=function_schema,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate","title":"Candidate","text":"<p>A response candidate generated by the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Candidate:\n    \"\"\"A response candidate generated by the model.\"\"\"\n\n    def __init__(self):\n        raw_candidate = gapic_content_types.Candidate()\n        self._raw_candidate = raw_candidate\n\n    @classmethod\n    def _from_gapic(cls, raw_candidate: gapic_content_types.Candidate) -&gt; \"Candidate\":\n        candidate = cls()\n        candidate._raw_candidate = raw_candidate\n        return candidate\n\n    @classmethod\n    def from_dict(cls, candidate_dict: Dict[str, Any]) -&gt; \"Candidate\":\n        raw_candidate = gapic_content_types.Candidate()\n        json_format.ParseDict(candidate_dict, raw_candidate._pb)\n        return cls._from_gapic(raw_candidate=raw_candidate)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_candidate)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_candidate.__repr__()\n\n    @property\n    def content(self) -&gt; \"Content\":\n        return Content._from_gapic(\n            raw_content=self._raw_candidate.content,\n        )\n\n    @property\n    def finish_reason(self) -&gt; gapic_content_types.Candidate.FinishReason:\n        return self._raw_candidate.finish_reason\n\n    @property\n    def finish_message(self) -&gt; str:\n        return self._raw_candidate.finish_message\n\n    @property\n    def index(self) -&gt; int:\n        return self._raw_candidate.index\n\n    @property\n    def safety_ratings(self) -&gt; Sequence[gapic_content_types.SafetyRating]:\n        return self._raw_candidate.safety_ratings\n\n    @property\n    def citation_metadata(self) -&gt; gapic_content_types.CitationMetadata:\n        return self._raw_candidate.citation_metadata\n\n    @property\n    def grounding_metadata(self) -&gt; gapic_content_types.GroundingMetadata:\n        return self._raw_candidate.grounding_metadata\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        try:\n            return self.content.text\n        except (ValueError, AttributeError) as e:\n            # Enrich the error message with the whole Candidate.\n            # The Content object does not have full information.\n            raise ValueError(\n                \"Cannot get the Candidate text.\\n\"\n                f\"{e}\\n\"\n                \"Candidate:\\n\" + _dict_to_pretty_string(self.to_dict())\n            ) from e\n\n    @property\n    def function_calls(self) -&gt; Sequence[gapic_tool_types.FunctionCall]:\n        if not self.content or not self.content.parts:\n            return []\n        return [\n            part.function_call\n            for part in self.content.parts\n            if part and part.function_call\n        ]\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Content","title":"Content","text":"<p>The multi-part content of a message.</p> Usage <pre><code>response = model.generate_content(contents=[\n    Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n])\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Content:\n    r\"\"\"The multi-part content of a message.\n\n    Usage:\n        ```\n        response = model.generate_content(contents=[\n            Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n        ])\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        parts: List[\"Part\"] = None,\n        role: Optional[str] = None,\n    ):\n        raw_parts = [part._raw_part for part in parts or []]\n        self._raw_content = gapic_content_types.Content(parts=raw_parts, role=role)\n\n    @classmethod\n    def _from_gapic(cls, raw_content: gapic_content_types.Content) -&gt; \"Content\":\n        content = cls()\n        content._raw_content = raw_content\n        return content\n\n    @classmethod\n    def from_dict(cls, content_dict: Dict[str, Any]) -&gt; \"Content\":\n        raw_content = gapic_content_types.Content()\n        json_format.ParseDict(content_dict, raw_content._pb)\n        return cls._from_gapic(raw_content=raw_content)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_content)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_content.__repr__()\n\n    @property\n    def parts(self) -&gt; List[\"Part\"]:\n        return [\n            Part._from_gapic(raw_part=raw_part) for raw_part in self._raw_content.parts\n        ]\n\n    @property\n    def role(self) -&gt; str:\n        return self._raw_content.role\n\n    @role.setter\n    def role(self, role: str) -&gt; None:\n        self._raw_content.role = role\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        if len(self.parts) &gt; 1:\n            raise ValueError(\"Multiple content parts are not supported.\")\n        if not self.parts:\n            raise ValueError(\n                \"Response candidate content has no parts (and thus no text).\"\n                \" The candidate is likely blocked by the safety filters.\\n\"\n                \"Content:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        return self.parts[0].text\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.FunctionDeclaration","title":"FunctionDeclaration","text":"<p>A representation of a function declaration.</p> Usage <p>Create function declaration and tool: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"celsius\",\n                    \"fahrenheit\",\n                ]\n            }\n        },\n        \"required\": [\n            \"location\"\n        ]\n    },\n)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class FunctionDeclaration:\n    r\"\"\"A representation of a function declaration.\n\n    Usage:\n        Create function declaration and tool:\n        ```\n        get_current_weather_func = generative_models.FunctionDeclaration(\n            name=\"get_current_weather\",\n            description=\"Get the current weather in a given location\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\n                            \"celsius\",\n                            \"fahrenheit\",\n                        ]\n                    }\n                },\n                \"required\": [\n                    \"location\"\n                ]\n            },\n        )\n        weather_tool = generative_models.Tool(\n            function_declarations=[get_current_weather_func],\n        )\n        ```\n        Use tool in `GenerativeModel.generate_content`:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\n            \"What is the weather like in Boston?\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        ))\n        ```\n        Use tool in chat:\n        ```\n        model = GenerativeModel(\n            \"gemini-pro\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        )\n        chat = model.start_chat()\n        print(chat.send_message(\"What is the weather like in Boston?\"))\n        print(chat.send_message(\n            Part.from_function_response(\n                name=\"get_current_weather\",\n                response={\n                    \"content\": {\"weather_there\": \"super nice\"},\n                }\n            ),\n        ))\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        parameters: Dict[str, Any],\n        description: Optional[str] = None,\n    ):\n        \"\"\"Constructs a FunctionDeclaration.\n\n        Args:\n            name: The name of the function that the model can call.\n            parameters: Describes the parameters to this function in JSON Schema Object format.\n            description: Description and purpose of the function.\n                Model uses it to decide how and whether to call the function.\n        \"\"\"\n        gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n        self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n            name=name, description=description, parameters=raw_schema\n        )\n\n    @classmethod\n    def from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n        return CallableFunctionDeclaration.from_func(func)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_function_declaration)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_function_declaration.__repr__()\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.FunctionDeclaration.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the function that the model can call.</p> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Describes the parameters to this function in JSON Schema Object format.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>description</code> <p>Description and purpose of the function. Model uses it to decide how and whether to call the function.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None,\n):\n    \"\"\"Constructs a FunctionDeclaration.\n\n    Args:\n        name: The name of the function that the model can call.\n        parameters: Describes the parameters to this function in JSON Schema Object format.\n        description: Description and purpose of the function.\n            Model uses it to decide how and whether to call the function.\n    \"\"\"\n    gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n    raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n        name=name, description=description, parameters=raw_schema\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationConfig","title":"GenerationConfig","text":"<p>Parameters for the generation.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GenerationConfig:\n    \"\"\"Parameters for the generation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        top_k: Optional[int] = None,\n        candidate_count: Optional[int] = None,\n        max_output_tokens: Optional[int] = None,\n        stop_sequences: Optional[List[str]] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        response_mime_type: Optional[str] = None,\n        response_schema: Optional[Dict[str, Any]] = None,\n    ):\n        r\"\"\"Constructs a GenerationConfig object.\n\n        Args:\n            temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n            top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n            top_k: If specified, top-k sampling will be used.\n            candidate_count: Number of candidates to generate.\n            max_output_tokens: The maximum number of output tokens to generate per message.\n            stop_sequences: A list of stop sequences.\n            presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n                thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n            frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n                text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n            response_mime_type: Output response mimetype of the generated\n                candidate text. Supported mimetypes:\n\n                -  ``text/plain``: (default) Text output.\n                -  ``application/json``: JSON response in the candidates.\n\n                The model needs to be prompted to output the appropriate\n                response type, otherwise the behavior is undefined.\n            response_schema: Output response schema of the genreated candidate text. Only valid when\n                response_mime_type is application/json.\n\n        Usage:\n            ```\n            response = model.generate_content(\n                \"Why is sky blue?\",\n                generation_config=GenerationConfig(\n                    temperature=0.1,\n                    top_p=0.95,\n                    top_k=20,\n                    candidate_count=1,\n                    max_output_tokens=100,\n                    stop_sequences=[\"\\n\\n\\n\"],\n                )\n            )\n            ```\n        \"\"\"\n        if response_schema is None:\n            raw_schema = None\n        else:\n            gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n            raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n        self._raw_generation_config = gapic_content_types.GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            candidate_count=candidate_count,\n            max_output_tokens=max_output_tokens,\n            stop_sequences=stop_sequences,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            response_mime_type=response_mime_type,\n            response_schema=raw_schema,\n        )\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_generation_config: gapic_content_types.GenerationConfig,\n    ) -&gt; \"GenerationConfig\":\n        response = cls()\n        response._raw_generation_config = raw_generation_config\n        return response\n\n    @classmethod\n    def from_dict(cls, generation_config_dict: Dict[str, Any]) -&gt; \"GenerationConfig\":\n        raw_generation_config = gapic_content_types.GenerationConfig(\n            generation_config_dict\n        )\n        return cls._from_gapic(raw_generation_config=raw_generation_config)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_generation_config)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_generation_config.__repr__()\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>temperature</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>If specified, top-k sampling will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to generate.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>The maximum number of output tokens to generate per message.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>A list of stop sequences.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>response_mime_type</code> <p>Output response mimetype of the generated candidate text. Supported mimetypes:</p> <ul> <li><code>text/plain</code>: (default) Text output.</li> <li><code>application/json</code>: JSON response in the candidates.</li> </ul> <p>The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>response_schema</code> <p>Output response schema of the genreated candidate text. Only valid when response_mime_type is application/json.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Usage <pre><code>response = model.generate_content(\n    \"Why is sky blue?\",\n    generation_config=GenerationConfig(\n        temperature=0.1,\n        top_p=0.95,\n        top_k=20,\n        candidate_count=1,\n        max_output_tokens=100,\n        stop_sequences=[\"\\n\\n\\n\"],\n    )\n)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None,\n):\n    r\"\"\"Constructs a GenerationConfig object.\n\n    Args:\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n        top_k: If specified, top-k sampling will be used.\n        candidate_count: Number of candidates to generate.\n        max_output_tokens: The maximum number of output tokens to generate per message.\n        stop_sequences: A list of stop sequences.\n        presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n        frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n        response_mime_type: Output response mimetype of the generated\n            candidate text. Supported mimetypes:\n\n            -  ``text/plain``: (default) Text output.\n            -  ``application/json``: JSON response in the candidates.\n\n            The model needs to be prompted to output the appropriate\n            response type, otherwise the behavior is undefined.\n        response_schema: Output response schema of the genreated candidate text. Only valid when\n            response_mime_type is application/json.\n\n    Usage:\n        ```\n        response = model.generate_content(\n            \"Why is sky blue?\",\n            generation_config=GenerationConfig(\n                temperature=0.1,\n                top_p=0.95,\n                top_k=20,\n                candidate_count=1,\n                max_output_tokens=100,\n                stop_sequences=[\"\\n\\n\\n\"],\n            )\n        )\n        ```\n    \"\"\"\n    if response_schema is None:\n        raw_schema = None\n    else:\n        gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_generation_config = gapic_content_types.GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        candidate_count=candidate_count,\n        max_output_tokens=max_output_tokens,\n        stop_sequences=stop_sequences,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        response_mime_type=response_mime_type,\n        response_schema=raw_schema,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse","title":"GenerationResponse","text":"<p>The response from the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GenerationResponse:\n    \"\"\"The response from the model.\"\"\"\n\n    def __init__(self):\n        raw_response = gapic_prediction_service_types.GenerateContentResponse()\n        self._raw_response = raw_response\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_response: gapic_prediction_service_types.GenerateContentResponse,\n    ) -&gt; \"GenerationResponse\":\n        response = cls()\n        response._raw_response = raw_response\n        return response\n\n    @classmethod\n    def from_dict(cls, response_dict: Dict[str, Any]) -&gt; \"GenerationResponse\":\n        raw_response = gapic_prediction_service_types.GenerateContentResponse()\n        json_format.ParseDict(response_dict, raw_response._pb)\n        return cls._from_gapic(raw_response=raw_response)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_response)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_response.__repr__()\n\n    @property\n    def candidates(self) -&gt; List[\"Candidate\"]:\n        return [\n            Candidate._from_gapic(raw_candidate=raw_candidate)\n            for raw_candidate in self._raw_response.candidates\n        ]\n\n    # GenerationPart properties\n    @property\n    def text(self) -&gt; str:\n        if len(self.candidates) &gt; 1:\n            raise ValueError(\n                \"The response has multiple candidates.\"\n                \" Use `response.candidate[i].text` to get text of a particular candidate.\"\n            )\n        if not self.candidates:\n            raise ValueError(\n                \"Response has no candidates (and thus no text).\"\n                \" The response is likely blocked by the safety filters.\\n\"\n                \"Response:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        try:\n            return self.candidates[0].text\n        except (ValueError, AttributeError) as e:\n            # Enrich the error message with the whole Response.\n            # The Candidate object does not have full information.\n            raise ValueError(\n                \"Cannot get the response text.\\n\"\n                f\"{e}\\n\"\n                \"Response:\\n\" + _dict_to_pretty_string(self.to_dict())\n            ) from e\n\n    @property\n    def prompt_feedback(\n        self,\n    ) -&gt; gapic_prediction_service_types.GenerateContentResponse.PromptFeedback:\n        return self._raw_response.prompt_feedback\n\n    @property\n    def usage_metadata(\n        self,\n    ) -&gt; gapic_prediction_service_types.GenerateContentResponse.UsageMetadata:\n        return self._raw_response.usage_metadata\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image","title":"Image","text":"<p>The image that can be sent to a generative model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Image:\n    \"\"\"The image that can be sent to a generative model.\"\"\"\n\n    _image_bytes: bytes\n    _loaded_image: Optional[\"PIL_Image.Image\"] = None\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"Image\":\n        \"\"\"Loads image from file.\n\n        Args:\n            location: Local path from where to load the image.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        image_bytes = pathlib.Path(location).read_bytes()\n        image = Image()\n        image._image_bytes = image_bytes\n        return image\n\n    @staticmethod\n    def from_bytes(data: bytes) -&gt; \"Image\":\n        \"\"\"Loads image from image bytes.\n\n        Args:\n            data: Image bytes.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        image = Image()\n        image._image_bytes = data\n        return image\n\n    @property\n    def _pil_image(self) -&gt; \"PIL_Image.Image\":\n        if self._loaded_image is None:\n            if not PIL_Image:\n                raise RuntimeError(\n                    \"The PIL module is not available. Please install the Pillow package.\"\n                )\n            self._loaded_image = PIL_Image.open(io.BytesIO(self._image_bytes))\n        return self._loaded_image\n\n    @property\n    def _mime_type(self) -&gt; str:\n        \"\"\"Returns the MIME type of the image.\"\"\"\n        if PIL_Image:\n            return _FORMAT_TO_MIME_TYPE[self._pil_image.format.lower()]\n        else:\n            # Fall back to jpeg\n            return \"image/jpeg\"\n\n    @property\n    def data(self) -&gt; bytes:\n        \"\"\"Returns the image data.\"\"\"\n        return self._image_bytes\n\n    def _repr_png_(self):\n        return self._pil_image._repr_png_()\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image.data","title":"data  <code>property</code>","text":"<pre><code>data: bytes\n</code></pre> <p>Returns the image data.</p>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image.from_bytes","title":"from_bytes  <code>staticmethod</code>","text":"<pre><code>from_bytes(data: bytes) -&gt; Image\n</code></pre> <p>Loads image from image bytes.</p> PARAMETER DESCRIPTION <code>data</code> <p>Image bytes.</p> <p> TYPE: <code>bytes</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_bytes(data: bytes) -&gt; \"Image\":\n    \"\"\"Loads image from image bytes.\n\n    Args:\n        data: Image bytes.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image = Image()\n    image._image_bytes = data\n    return image\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from file.\n\n    Args:\n        location: Local path from where to load the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image()\n    image._image_bytes = image_bytes\n    return image\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part","title":"Part","text":"<p>A part of a multi-part Content message.</p> Usage <pre><code>text_part = Part.from_text(\"Why is sky blue?\")\nimage_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\nvideo_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\nfunction_response_part = Part.from_function_response(\n    name=\"get_current_weather\",\n    response={\n        \"content\": {\"weather_there\": \"super nice\"},\n    }\n)\n\nresponse1 = model.generate_content([text_part, image_part])\nresponse2 = model.generate_content(video_part)\nresponse3 = chat.send_message(function_response_part)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Part:\n    r\"\"\"A part of a multi-part Content message.\n\n    Usage:\n        ```\n        text_part = Part.from_text(\"Why is sky blue?\")\n        image_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\n        video_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\n        function_response_part = Part.from_function_response(\n            name=\"get_current_weather\",\n            response={\n                \"content\": {\"weather_there\": \"super nice\"},\n            }\n        )\n\n        response1 = model.generate_content([text_part, image_part])\n        response2 = model.generate_content(video_part)\n        response3 = chat.send_message(function_response_part)\n        ```\n    \"\"\"\n\n    def __init__(self):\n        raw_part = gapic_content_types.Part()\n        self._raw_part = raw_part\n\n    @classmethod\n    def _from_gapic(cls, raw_part: gapic_content_types.Part) -&gt; \"Part\":\n        part = cls()\n        part._raw_part = raw_part\n        return part\n\n    @classmethod\n    def from_dict(cls, part_dict: Dict[str, Any]) -&gt; \"Part\":\n        raw_part = gapic_content_types.Part()\n        json_format.ParseDict(part_dict, raw_part._pb)\n        return cls._from_gapic(raw_part=raw_part)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_part.__repr__()\n\n    @staticmethod\n    def from_data(data: bytes, mime_type: str) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                inline_data=gapic_content_types.Blob(data=data, mime_type=mime_type)\n            )\n        )\n\n    @staticmethod\n    def from_uri(uri: str, mime_type: str) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                file_data=gapic_content_types.FileData(\n                    file_uri=uri, mime_type=mime_type\n                )\n            )\n        )\n\n    @staticmethod\n    def from_text(text: str) -&gt; \"Part\":\n        return Part._from_gapic(raw_part=gapic_content_types.Part(text=text))\n\n    @staticmethod\n    def from_image(image: \"Image\") -&gt; \"Part\":\n        return Part.from_data(data=image.data, mime_type=image._mime_type)\n\n    @staticmethod\n    def from_function_response(name: str, response: Dict[str, Any]) -&gt; \"Part\":\n        return Part._from_gapic(\n            raw_part=gapic_content_types.Part(\n                function_response=gapic_tool_types.FunctionResponse(\n                    name=name,\n                    response=response,\n                )\n            )\n        )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_part)\n\n    @property\n    def text(self) -&gt; str:\n        if \"text\" not in self._raw_part:\n            raise AttributeError(\n                \"Response candidate content part has no text.\\n\"\n                \"Part:\\n\" + _dict_to_pretty_string(self.to_dict())\n            )\n        return self._raw_part.text\n\n    @property\n    def mime_type(self) -&gt; Optional[str]:\n        part_type = self._raw_part._pb.WhichOneof(\"data\")\n        if part_type == \"inline_data\":\n            return self._raw_part.inline_data.mime_type\n        if part_type == \"file_data\":\n            return self._raw_part.file_data.mime_type\n        raise AttributeError(f\"Part has no mime_type.\\nPart:\\n{self.to_dict()}\")\n\n    @property\n    def inline_data(self) -&gt; gapic_content_types.Blob:\n        return self._raw_part.inline_data\n\n    @property\n    def file_data(self) -&gt; gapic_content_types.FileData:\n        return self._raw_part.file_data\n\n    @property\n    def function_call(self) -&gt; gapic_tool_types.FunctionCall:\n        return self._raw_part.function_call\n\n    @property\n    def function_response(self) -&gt; gapic_tool_types.FunctionResponse:\n        return self._raw_part.function_response\n\n    @property\n    def _image(self) -&gt; \"Image\":\n        return Image.from_bytes(data=self._raw_part.inline_data.data)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting","title":"SafetySetting","text":"<p>Parameters for the generation.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class SafetySetting:\n    \"\"\"Parameters for the generation.\"\"\"\n\n    HarmCategory = gapic_content_types.HarmCategory\n    HarmBlockMethod = gapic_content_types.SafetySetting.HarmBlockMethod\n    HarmBlockThreshold = gapic_content_types.SafetySetting.HarmBlockThreshold\n\n    def __init__(\n        self,\n        *,\n        category: \"SafetySetting.HarmCategory\",\n        threshold: \"SafetySetting.HarmBlockThreshold\",\n        method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n    ):\n        r\"\"\"Safety settings.\n\n        Args:\n            category: Harm category.\n            threshold: The harm block threshold.\n            method: Specify if the threshold is used for probability or severity\n                score. If not specified, the threshold is used for probability\n                score.\n        \"\"\"\n        self._raw_safety_setting = gapic_content_types.SafetySetting(\n            category=category,\n            threshold=threshold,\n            method=method,\n        )\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_safety_setting: gapic_content_types.SafetySetting,\n    ) -&gt; \"SafetySetting\":\n        response = cls(\n            category=raw_safety_setting.category,\n            threshold=raw_safety_setting.threshold,\n        )\n        response._raw_safety_setting = raw_safety_setting\n        return response\n\n    @classmethod\n    def from_dict(cls, safety_setting_dict: Dict[str, Any]) -&gt; \"SafetySetting\":\n        raw_safety_setting = gapic_content_types.SafetySetting(safety_setting_dict)\n        return cls._from_gapic(raw_safety_setting=raw_safety_setting)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_safety_setting)\n\n    def __repr__(self):\n        return self._raw_safety_setting.__repr__()\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    category: SafetySetting.HarmCategory,\n    threshold: SafetySetting.HarmBlockThreshold,\n    method: Optional[SafetySetting.HarmBlockMethod] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>category</code> <p>Harm category.</p> <p> TYPE: <code>HarmCategory</code> </p> <code>threshold</code> <p>The harm block threshold.</p> <p> TYPE: <code>HarmBlockThreshold</code> </p> <code>method</code> <p>Specify if the threshold is used for probability or severity score. If not specified, the threshold is used for probability score.</p> <p> TYPE: <code>Optional[HarmBlockMethod]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    category: \"SafetySetting.HarmCategory\",\n    threshold: \"SafetySetting.HarmBlockThreshold\",\n    method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n):\n    r\"\"\"Safety settings.\n\n    Args:\n        category: Harm category.\n        threshold: The harm block threshold.\n        method: Specify if the threshold is used for probability or severity\n            score. If not specified, the threshold is used for probability\n            score.\n    \"\"\"\n    self._raw_safety_setting = gapic_content_types.SafetySetting(\n        category=category,\n        threshold=threshold,\n        method=method,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Tool","title":"Tool","text":"<p>A collection of functions that the model may use to generate response.</p> Usage <p>Create tool from function declarations: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(...)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Tool:\n    r\"\"\"A collection of functions that the model may use to generate response.\n\n    Usage:\n        Create tool from function declarations:\n        ```\n        get_current_weather_func = generative_models.FunctionDeclaration(...)\n        weather_tool = generative_models.Tool(\n            function_declarations=[get_current_weather_func],\n        )\n        ```\n        Use tool in `GenerativeModel.generate_content`:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\n            \"What is the weather like in Boston?\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        ))\n        ```\n        Use tool in chat:\n        ```\n        model = GenerativeModel(\n            \"gemini-pro\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n        )\n        chat = model.start_chat()\n        print(chat.send_message(\"What is the weather like in Boston?\"))\n        print(chat.send_message(\n            Part.from_function_response(\n                name=\"get_current_weather\",\n                response={\n                    \"content\": {\"weather_there\": \"super nice\"},\n                }\n            ),\n        ))\n        ```\n    \"\"\"\n\n    _raw_tool: gapic_tool_types.Tool\n\n    def __init__(\n        self,\n        function_declarations: List[\"FunctionDeclaration\"],\n    ):\n        gapic_function_declarations = [\n            function_declaration._raw_function_declaration\n            for function_declaration in function_declarations\n        ]\n        self._raw_tool = gapic_tool_types.Tool(\n            function_declarations=gapic_function_declarations\n        )\n        callable_functions = {\n            function_declaration._raw_function_declaration.name: function_declaration\n            for function_declaration in function_declarations\n            if isinstance(function_declaration, CallableFunctionDeclaration)\n        }\n        self._callable_functions = callable_functions\n\n    @classmethod\n    def from_function_declarations(\n        cls,\n        function_declarations: List[\"FunctionDeclaration\"],\n    ) -&gt; \"Tool\":\n        return Tool(function_declarations=function_declarations)\n\n    @classmethod\n    def from_retrieval(\n        cls,\n        retrieval: Union[\"grounding.Retrieval\", \"rag.Retrieval\"],\n    ) -&gt; \"Tool\":\n        raw_tool = gapic_tool_types.Tool(retrieval=retrieval._raw_retrieval)\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    @classmethod\n    def from_google_search_retrieval(\n        cls,\n        google_search_retrieval: \"grounding.GoogleSearchRetrieval\",\n    ) -&gt; \"Tool\":\n        raw_tool = gapic_tool_types.Tool(\n            google_search_retrieval=google_search_retrieval._raw_google_search_retrieval\n        )\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    @classmethod\n    def _from_gapic(\n        cls,\n        raw_tool: gapic_tool_types.Tool,\n    ) -&gt; \"Tool\":\n        response = cls([])\n        response._raw_tool = raw_tool\n        return response\n\n    @classmethod\n    def from_dict(cls, tool_dict: Dict[str, Any]) -&gt; \"Tool\":\n        tool_dict = copy.deepcopy(tool_dict)\n        function_declarations = tool_dict[\"function_declarations\"]\n        for function_declaration in function_declarations:\n            function_declaration[\"parameters\"] = _convert_schema_dict_to_gapic(\n                function_declaration[\"parameters\"]\n            )\n        raw_tool = gapic_tool_types.Tool(tool_dict)\n        return cls._from_gapic(raw_tool=raw_tool)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return _proto_to_dict(self._raw_tool)\n\n    def __repr__(self) -&gt; str:\n        return self._raw_tool.__repr__()\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ToolConfig","title":"ToolConfig","text":"<p>Config shared for all tools provided in the request.</p> Usage <p>Create ToolConfig <pre><code>tool_config = ToolConfig(\n    function_calling_config=ToolConfig.FunctionCallingConfig(\n        mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n        allowed_function_names=[\"get_current_weather_func\"],\n))\n</code></pre> Use ToolConfig in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n    tool_config=tool_config,\n))\n</code></pre> Use ToolConfig in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n    tool_config=tool_config,\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class ToolConfig:\n    r\"\"\"Config shared for all tools provided in the request.\n\n    Usage:\n        Create ToolConfig\n        ```\n        tool_config = ToolConfig(\n            function_calling_config=ToolConfig.FunctionCallingConfig(\n                mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n                allowed_function_names=[\"get_current_weather_func\"],\n        ))\n        ```\n        Use ToolConfig in `GenerativeModel.generate_content`:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\n            \"What is the weather like in Boston?\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n            tool_config=tool_config,\n        ))\n        ```\n        Use ToolConfig in chat:\n        ```\n        model = GenerativeModel(\n            \"gemini-pro\",\n            # You can specify tools when creating a model to avoid having to send them with every request.\n            tools=[weather_tool],\n            tool_config=tool_config,\n        )\n        chat = model.start_chat()\n        print(chat.send_message(\"What is the weather like in Boston?\"))\n        print(chat.send_message(\n            Part.from_function_response(\n                name=\"get_current_weather\",\n                response={\n                    \"content\": {\"weather_there\": \"super nice\"},\n                }\n            ),\n        ))\n        ```\n    \"\"\"\n\n    class FunctionCallingConfig:\n        Mode = gapic_tool_types.FunctionCallingConfig.Mode\n\n        def __init__(\n            self,\n            mode: \"ToolConfig.FunctionCallingConfig.Mode\",\n            allowed_function_names: Optional[List[str]] = None,\n        ):\n            \"\"\"Constructs FunctionCallingConfig.\n\n            Args:\n                mode: Enum describing the function calling mode\n                allowed_function_names: A list of allowed function names\n                    (must match from Tool). Only set when the Mode is ANY.\n            \"\"\"\n            self._gapic_function_calling_config = (\n                gapic_tool_types.FunctionCallingConfig(\n                    mode=mode,\n                    allowed_function_names=allowed_function_names,\n                )\n            )\n\n    def __init__(self, function_calling_config: \"ToolConfig.FunctionCallingConfig\"):\n        self._gapic_tool_config = gapic_tool_types.ToolConfig(\n            function_calling_config=function_calling_config._gapic_function_calling_config\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ToolConfig.FunctionCallingConfig","title":"FunctionCallingConfig","text":"Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class FunctionCallingConfig:\n    Mode = gapic_tool_types.FunctionCallingConfig.Mode\n\n    def __init__(\n        self,\n        mode: \"ToolConfig.FunctionCallingConfig.Mode\",\n        allowed_function_names: Optional[List[str]] = None,\n    ):\n        \"\"\"Constructs FunctionCallingConfig.\n\n        Args:\n            mode: Enum describing the function calling mode\n            allowed_function_names: A list of allowed function names\n                (must match from Tool). Only set when the Mode is ANY.\n        \"\"\"\n        self._gapic_function_calling_config = (\n            gapic_tool_types.FunctionCallingConfig(\n                mode=mode,\n                allowed_function_names=allowed_function_names,\n            )\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ToolConfig.FunctionCallingConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    mode: ToolConfig.FunctionCallingConfig.Mode,\n    allowed_function_names: Optional[List[str]] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>mode</code> <p>Enum describing the function calling mode</p> <p> TYPE: <code>Mode</code> </p> <code>allowed_function_names</code> <p>A list of allowed function names (must match from Tool). Only set when the Mode is ANY.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    mode: \"ToolConfig.FunctionCallingConfig.Mode\",\n    allowed_function_names: Optional[List[str]] = None,\n):\n    \"\"\"Constructs FunctionCallingConfig.\n\n    Args:\n        mode: Enum describing the function calling mode\n        allowed_function_names: A list of allowed function names\n            (must match from Tool). Only set when the Mode is ANY.\n    \"\"\"\n    self._gapic_function_calling_config = (\n        gapic_tool_types.FunctionCallingConfig(\n            mode=mode,\n            allowed_function_names=allowed_function_names,\n        )\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding","title":"grounding","text":"<p>Grounding namespace (preview).</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class preview_grounding:  # pylint: disable=invalid-name\n    \"\"\"Grounding namespace (preview).\"\"\"\n\n    __name__ = \"grounding\"\n    __module__ = \"vertexai.preview.generative_models\"\n\n    def __init__(self):\n        raise RuntimeError(\"This class must not be instantiated.\")\n\n    class Retrieval:\n        \"\"\"Defines a retrieval tool that model can call to access external knowledge.\"\"\"\n\n        def __init__(\n            self,\n            source: Union[\"grounding.VertexAISearch\"],\n            disable_attribution: Optional[bool] = None,\n        ):\n            \"\"\"Initializes a Retrieval tool.\n\n            Args:\n                source (VertexAISearch):\n                    Set to use data source powered by Vertex AI Search.\n                disable_attribution (bool):\n                    Optional. Disable using the result from this\n                    tool in detecting grounding attribution. This\n                    does not affect how the result is given to the\n                    model for generation.\n            \"\"\"\n            self._raw_retrieval = gapic_tool_types.Retrieval(\n                vertex_ai_search=source._raw_vertex_ai_search,\n                disable_attribution=disable_attribution,\n            )\n\n    class VertexAISearch:\n        r\"\"\"Retrieve from Vertex AI Search datastore for grounding.\n        See https://cloud.google.com/vertex-ai-search-and-conversation\n        \"\"\"\n\n        def __init__(\n            self,\n            datastore: str,\n        ):\n            \"\"\"Initializes a Vertex AI Search tool.\n\n            Args:\n                datastore (str):\n                    Required. Fully-qualified Vertex AI Search's\n                    datastore resource ID.\n                    projects/&lt;&gt;/locations/&lt;&gt;/collections/&lt;&gt;/dataStores/&lt;&gt;\n            \"\"\"\n            self._raw_vertex_ai_search = gapic_tool_types.VertexAISearch(\n                datastore=datastore,\n            )\n\n    class GoogleSearchRetrieval:\n        r\"\"\"Tool to retrieve public web data for grounding, powered by\n        Google Search.\n\n        Attributes:\n            disable_attribution (bool):\n                Optional. Disable using the result from this\n                tool in detecting grounding attribution. This\n                does not affect how the result is given to the\n                model for generation.\n        \"\"\"\n\n        def __init__(\n            self,\n            disable_attribution: Optional[bool] = None,\n        ):\n            \"\"\"Initializes a Google Search Retrieval tool.\n\n            Args:\n                disable_attribution (bool):\n                    Optional. Disable using the result from this\n                    tool in detecting grounding attribution. This\n                    does not affect how the result is given to the\n                    model for generation.\n            \"\"\"\n            self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval(\n                disable_attribution=disable_attribution,\n            )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.GoogleSearchRetrieval","title":"GoogleSearchRetrieval","text":"<p>Tool to retrieve public web data for grounding, powered by Google Search.</p> ATTRIBUTE DESCRIPTION <code>disable_attribution</code> <p>Optional. Disable using the result from this tool in detecting grounding attribution. This does not affect how the result is given to the model for generation.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class GoogleSearchRetrieval:\n    r\"\"\"Tool to retrieve public web data for grounding, powered by\n    Google Search.\n\n    Attributes:\n        disable_attribution (bool):\n            Optional. Disable using the result from this\n            tool in detecting grounding attribution. This\n            does not affect how the result is given to the\n            model for generation.\n    \"\"\"\n\n    def __init__(\n        self,\n        disable_attribution: Optional[bool] = None,\n    ):\n        \"\"\"Initializes a Google Search Retrieval tool.\n\n        Args:\n            disable_attribution (bool):\n                Optional. Disable using the result from this\n                tool in detecting grounding attribution. This\n                does not affect how the result is given to the\n                model for generation.\n        \"\"\"\n        self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval(\n            disable_attribution=disable_attribution,\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.GoogleSearchRetrieval.__init__","title":"__init__","text":"<pre><code>__init__(disable_attribution: Optional[bool] = None)\n</code></pre> PARAMETER DESCRIPTION <code>disable_attribution</code> <p>Optional. Disable using the result from this tool in detecting grounding attribution. This does not affect how the result is given to the model for generation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    disable_attribution: Optional[bool] = None,\n):\n    \"\"\"Initializes a Google Search Retrieval tool.\n\n    Args:\n        disable_attribution (bool):\n            Optional. Disable using the result from this\n            tool in detecting grounding attribution. This\n            does not affect how the result is given to the\n            model for generation.\n    \"\"\"\n    self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval(\n        disable_attribution=disable_attribution,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.Retrieval","title":"Retrieval","text":"<p>Defines a retrieval tool that model can call to access external knowledge.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class Retrieval:\n    \"\"\"Defines a retrieval tool that model can call to access external knowledge.\"\"\"\n\n    def __init__(\n        self,\n        source: Union[\"grounding.VertexAISearch\"],\n        disable_attribution: Optional[bool] = None,\n    ):\n        \"\"\"Initializes a Retrieval tool.\n\n        Args:\n            source (VertexAISearch):\n                Set to use data source powered by Vertex AI Search.\n            disable_attribution (bool):\n                Optional. Disable using the result from this\n                tool in detecting grounding attribution. This\n                does not affect how the result is given to the\n                model for generation.\n        \"\"\"\n        self._raw_retrieval = gapic_tool_types.Retrieval(\n            vertex_ai_search=source._raw_vertex_ai_search,\n            disable_attribution=disable_attribution,\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.Retrieval.__init__","title":"__init__","text":"<pre><code>__init__(\n    source: Union[grounding.VertexAISearch],\n    disable_attribution: Optional[bool] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>source</code> <p>Set to use data source powered by Vertex AI Search.</p> <p> TYPE: <code>VertexAISearch</code> </p> <code>disable_attribution</code> <p>Optional. Disable using the result from this tool in detecting grounding attribution. This does not affect how the result is given to the model for generation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    source: Union[\"grounding.VertexAISearch\"],\n    disable_attribution: Optional[bool] = None,\n):\n    \"\"\"Initializes a Retrieval tool.\n\n    Args:\n        source (VertexAISearch):\n            Set to use data source powered by Vertex AI Search.\n        disable_attribution (bool):\n            Optional. Disable using the result from this\n            tool in detecting grounding attribution. This\n            does not affect how the result is given to the\n            model for generation.\n    \"\"\"\n    self._raw_retrieval = gapic_tool_types.Retrieval(\n        vertex_ai_search=source._raw_vertex_ai_search,\n        disable_attribution=disable_attribution,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.VertexAISearch","title":"VertexAISearch","text":"<p>Retrieve from Vertex AI Search datastore for grounding. See https://cloud.google.com/vertex-ai-search-and-conversation</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>class VertexAISearch:\n    r\"\"\"Retrieve from Vertex AI Search datastore for grounding.\n    See https://cloud.google.com/vertex-ai-search-and-conversation\n    \"\"\"\n\n    def __init__(\n        self,\n        datastore: str,\n    ):\n        \"\"\"Initializes a Vertex AI Search tool.\n\n        Args:\n            datastore (str):\n                Required. Fully-qualified Vertex AI Search's\n                datastore resource ID.\n                projects/&lt;&gt;/locations/&lt;&gt;/collections/&lt;&gt;/dataStores/&lt;&gt;\n        \"\"\"\n        self._raw_vertex_ai_search = gapic_tool_types.VertexAISearch(\n            datastore=datastore,\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.VertexAISearch.__init__","title":"__init__","text":"<pre><code>__init__(datastore: str)\n</code></pre> PARAMETER DESCRIPTION <code>datastore</code> <p>Required. Fully-qualified Vertex AI Search's datastore resource ID. projects/&lt;&gt;/locations/&lt;&gt;/collections/&lt;&gt;/dataStores/&lt;&gt;</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    datastore: str,\n):\n    \"\"\"Initializes a Vertex AI Search tool.\n\n    Args:\n        datastore (str):\n            Required. Fully-qualified Vertex AI Search's\n            datastore resource ID.\n            projects/&lt;&gt;/locations/&lt;&gt;/collections/&lt;&gt;/dataStores/&lt;&gt;\n    \"\"\"\n    self._raw_vertex_ai_search = gapic_tool_types.VertexAISearch(\n        datastore=datastore,\n    )\n</code></pre>"},{"location":"vertexai/preview/language_models/","title":"Language models","text":"<p>Classes for working with language models.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.ChatMessage","title":"ChatMessage  <code>dataclass</code>","text":"<p>A chat message.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>Content of the message.</p> <p> TYPE: <code>str</code> </p> <code>author</code> <p>Author of the message.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass ChatMessage:\n    \"\"\"A chat message.\n\n    Attributes:\n        content: Content of the message.\n        author: Author of the message.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    content: str\n    author: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.CountTokensResponse","title":"CountTokensResponse  <code>dataclass</code>","text":"<p>The response from a count_tokens request. Attributes:     total_tokens (int):         The total number of tokens counted across all         instances passed to the request.     total_billable_characters (int):         The total number of billable characters         counted across all instances from the request.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass CountTokensResponse:\n    \"\"\"The response from a count_tokens request.\n    Attributes:\n        total_tokens (int):\n            The total number of tokens counted across all\n            instances passed to the request.\n        total_billable_characters (int):\n            The total number of billable characters\n            counted across all instances from the request.\n    \"\"\"\n\n    total_tokens: int\n    total_billable_characters: int\n    _count_tokens_response: Any\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric","title":"EvaluationClassificationMetric  <code>dataclass</code>","text":"<p>               Bases: <code>_EvaluationMetricBase</code></p> <p>The evaluation metric response for classification metrics.</p> PARAMETER DESCRIPTION <code>label_name</code> <p>Optional. The name of the label associated with the metrics. This is only returned when <code>only_summary_metrics=False</code> is passed to evaluate().</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auPrc</code> <p>Optional. The area under the precision recall curve.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>auRoc</code> <p>Optional. The area under the receiver operating characteristic curve.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>logLoss</code> <p>Optional. Logarithmic loss.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>confidenceMetrics</code> <p>Optional. This is only returned when <code>only_summary_metrics=False</code> is passed to evaluate().</p> <p> TYPE: <code>List[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>confusionMatrix</code> <p>Optional. This is only returned when <code>only_summary_metrics=False</code> is passed to evaluate().</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\language_models\\_evaluatable_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationClassificationMetric(_EvaluationMetricBase):\n    \"\"\"The evaluation metric response for classification metrics.\n\n    Args:\n        label_name (str):\n            Optional. The name of the label associated with the metrics. This is only\n            returned when `only_summary_metrics=False` is passed to evaluate().\n        auPrc (float):\n            Optional. The area under the precision recall curve.\n        auRoc (float):\n            Optional. The area under the receiver operating characteristic curve.\n        logLoss (float):\n            Optional. Logarithmic loss.\n        confidenceMetrics (List[Dict[str, Any]]):\n            Optional. This is only returned when `only_summary_metrics=False` is\n            passed to evaluate().\n        confusionMatrix (Dict[str, Any]):\n          Optional. This is only returned when `only_summary_metrics=False` is\n          passed to evaluate().\n    \"\"\"\n\n    label_name: Optional[str] = None\n    auPrc: Optional[float] = None\n    auRoc: Optional[float] = None\n    logLoss: Optional[float] = None\n    confidenceMetrics: Optional[List[Dict[str, Any]]] = None\n    confusionMatrix: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationMetric","title":"EvaluationMetric  <code>dataclass</code>","text":"<p>               Bases: <code>_EvaluationMetricBase</code></p> <p>The evaluation metric response.</p> PARAMETER DESCRIPTION <code>bleu</code> <p>Optional. BLEU (Bilingual evauation understudy). Scores based on sacrebleu implementation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>rougeLSum</code> <p>Optional. ROUGE-L (Longest Common Subsequence) scoring at summary level.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\language_models\\_evaluatable_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationMetric(_EvaluationMetricBase):\n    \"\"\"The evaluation metric response.\n\n    Args:\n        bleu (float):\n            Optional. BLEU (Bilingual evauation understudy). Scores based on sacrebleu implementation.\n        rougeLSum (float):\n            Optional. ROUGE-L (Longest Common Subsequence) scoring at summary level.\n    \"\"\"\n\n    bleu: Optional[float] = None\n    rougeLSum: Optional[float] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationQuestionAnsweringSpec","title":"EvaluationQuestionAnsweringSpec  <code>dataclass</code>","text":"<p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for question answering model evaluation tasks.</p> Source code in <code>vertexai\\language_models\\_evaluatable_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationQuestionAnsweringSpec(_EvaluationTaskSpec):\n    \"\"\"Spec for question answering model evaluation tasks.\"\"\"\n\n    task_name: str = \"question-answering\"\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextClassificationSpec","title":"EvaluationTextClassificationSpec  <code>dataclass</code>","text":"<p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for text classification model evaluation tasks.</p> PARAMETER DESCRIPTION <code>target_column_name</code> <p>Required. The label column in the dataset provided in <code>ground_truth_data</code>. Required when task_name='text-classification'.</p> <p> TYPE: <code>str</code> </p> <code>class_names</code> <p>Required. A list of all possible label names in your dataset. Required when task_name='text-classification'.</p> <p> TYPE: <code>List[str]</code> </p> Source code in <code>vertexai\\language_models\\_evaluatable_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationTextClassificationSpec(_EvaluationTaskSpec):\n    \"\"\"Spec for text classification model evaluation tasks.\n\n    Args:\n        target_column_name (str):\n            Required. The label column in the dataset provided in `ground_truth_data`. Required when task_name='text-classification'.\n        class_names (List[str]):\n            Required. A list of all possible label names in your dataset. Required when task_name='text-classification'.\n    \"\"\"\n\n    target_column_name: str\n    class_names: List[str]\n\n    @property\n    def task_name(self) -&gt; str:\n        return \"text-classification\"\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextGenerationSpec","title":"EvaluationTextGenerationSpec  <code>dataclass</code>","text":"<p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for text generation model evaluation tasks.</p> Source code in <code>vertexai\\language_models\\_evaluatable_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationTextGenerationSpec(_EvaluationTaskSpec):\n    \"\"\"Spec for text generation model evaluation tasks.\"\"\"\n\n    @property\n    def task_name(self) -&gt; str:\n        return \"text-generation\"\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextSummarizationSpec","title":"EvaluationTextSummarizationSpec  <code>dataclass</code>","text":"<p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for text summarization model evaluation tasks.</p> Source code in <code>vertexai\\language_models\\_evaluatable_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass EvaluationTextSummarizationSpec(_EvaluationTaskSpec):\n    \"\"\"Spec for text summarization model evaluation tasks.\"\"\"\n\n    task_name: str = \"summarization\"\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.InputOutputTextPair","title":"InputOutputTextPair  <code>dataclass</code>","text":"<p>InputOutputTextPair represents a pair of input and output texts.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass InputOutputTextPair:\n    \"\"\"InputOutputTextPair represents a pair of input and output texts.\"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    input_text: str\n    output_text: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbedding","title":"TextEmbedding  <code>dataclass</code>","text":"<p>Text embedding vector and statistics.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextEmbedding:\n    \"\"\"Text embedding vector and statistics.\"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    values: List[float]\n    statistics: Optional[TextEmbeddingStatistics] = None\n    _prediction_response: Optional[aiplatform.models.Prediction] = None\n\n    @classmethod\n    def _parse_text_embedding_response(\n        cls, prediction_response: aiplatform.models.Prediction, prediction_index: int\n    ) -&gt; \"TextEmbedding\":\n        \"\"\"Creates a `TextEmbedding` object from a prediction.\n\n        Args:\n            prediction_response: `aiplatform.models.Prediction` object.\n\n        Returns:\n            `TextEmbedding` object.\n        \"\"\"\n        prediction = prediction_response.predictions[prediction_index]\n        is_prediction_from_pretrained_models = isinstance(\n            prediction, collections.abc.Mapping\n        )\n        if is_prediction_from_pretrained_models:\n            embeddings = prediction[\"embeddings\"]\n            embedding_stats = embeddings[\"statistics\"]\n            return cls(\n                values=embeddings[\"values\"],\n                statistics=TextEmbeddingStatistics(\n                    token_count=embedding_stats[\"token_count\"],\n                    truncated=embedding_stats[\"truncated\"],\n                ),\n                _prediction_response=prediction_response,\n            )\n        else:\n            return cls(values=prediction, _prediction_response=prediction_response)\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbeddingInput","title":"TextEmbeddingInput  <code>dataclass</code>","text":"<p>Structural text embedding input.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The main text content to embed.</p> <p> TYPE: <code>str</code> </p> <code>task_type</code> <p>The name of the downstream task the embeddings will be used for. Valid values: RETRIEVAL_QUERY     Specifies the given text is a query in a search/retrieval setting. RETRIEVAL_DOCUMENT     Specifies the given text is a document from the corpus being searched. SEMANTIC_SIMILARITY     Specifies the given text will be used for STS. CLASSIFICATION     Specifies that the given text will be classified. CLUSTERING     Specifies that the embeddings will be used for clustering. QUESTION_ANSWERING     Specifies that the embeddings will be used for question answering. FACT_VERIFICATION     Specifies that the embeddings will be used for fact verification.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>title</code> <p>Optional identifier of the text content.</p> <p> TYPE: <code>Optional[str]</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextEmbeddingInput:\n    \"\"\"Structural text embedding input.\n\n    Attributes:\n        text: The main text content to embed.\n        task_type: The name of the downstream task the embeddings will be used for.\n            Valid values:\n            RETRIEVAL_QUERY\n                Specifies the given text is a query in a search/retrieval setting.\n            RETRIEVAL_DOCUMENT\n                Specifies the given text is a document from the corpus being searched.\n            SEMANTIC_SIMILARITY\n                Specifies the given text will be used for STS.\n            CLASSIFICATION\n                Specifies that the given text will be classified.\n            CLUSTERING\n                Specifies that the embeddings will be used for clustering.\n            QUESTION_ANSWERING\n                Specifies that the embeddings will be used for question answering.\n            FACT_VERIFICATION\n                Specifies that the embeddings will be used for fact verification.\n        title: Optional identifier of the text content.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    text: str\n    task_type: Optional[str] = None\n    title: Optional[str] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse","title":"TextGenerationResponse  <code>dataclass</code>","text":"<p>TextGenerationResponse represents a response of a language model. Attributes:     text: The generated text     is_blocked: Whether the the request was blocked.     errors: The error codes indicate why the response was blocked.         Learn more information about safety errors here:         this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors     safety_attributes: Scores for safety attributes.         Learn more about the safety attributes here:         https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions     grounding_metadata: Metadata for grounding.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TextGenerationResponse:\n    \"\"\"TextGenerationResponse represents a response of a language model.\n    Attributes:\n        text: The generated text\n        is_blocked: Whether the the request was blocked.\n        errors: The error codes indicate why the response was blocked.\n            Learn more information about safety errors here:\n            this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors\n        safety_attributes: Scores for safety attributes.\n            Learn more about the safety attributes here:\n            https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions\n        grounding_metadata: Metadata for grounding.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    text: str\n    _prediction_response: Any\n    is_blocked: bool = False\n    errors: Tuple[int] = tuple()\n    safety_attributes: Dict[str, float] = dataclasses.field(default_factory=dict)\n    grounding_metadata: Optional[GroundingMetadata] = None\n\n    def __repr__(self):\n        if self.text:\n            return self.text\n        # Falling back to the full representation\n        elif self.grounding_metadata is not None:\n            return (\n                \"TextGenerationResponse(\"\n                f\"text={self.text!r}\"\n                f\", is_blocked={self.is_blocked!r}\"\n                f\", errors={self.errors!r}\"\n                f\", safety_attributes={self.safety_attributes!r}\"\n                f\", grounding_metadata={self.grounding_metadata!r}\"\n                \")\"\n            )\n        else:\n            return (\n                \"TextGenerationResponse(\"\n                f\"text={self.text!r}\"\n                f\", is_blocked={self.is_blocked!r}\"\n                f\", errors={self.errors!r}\"\n                f\", safety_attributes={self.safety_attributes!r}\"\n                \")\"\n            )\n\n    @property\n    def raw_prediction_response(self) -&gt; aiplatform.models.Prediction:\n        \"\"\"Raw prediction response.\"\"\"\n        return self._prediction_response\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse.raw_prediction_response","title":"raw_prediction_response  <code>property</code>","text":"<pre><code>raw_prediction_response: Prediction\n</code></pre> <p>Raw prediction response.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec","title":"TuningEvaluationSpec  <code>dataclass</code>","text":"<p>Specification for model evaluation to perform during tuning.</p> ATTRIBUTE DESCRIPTION <code>evaluation_data</code> <p>GCS URI of the evaluation dataset. This will run model evaluation as part of the tuning job.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>evaluation_interval</code> <p>The evaluation will run at every evaluation_interval tuning steps. Default: 20.</p> <p> TYPE: <code>Optional[int]</code> </p> <code>enable_early_stopping</code> <p>If True, the tuning may stop early before completing all the tuning steps. Requires evaluation_data.</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>enable_checkpoint_selection</code> <p>If set to True, the tuning process returns the best model checkpoint (based on model evaluation). If set to False, the latest model checkpoint is returned. If unset, the selection is only enabled for <code>*-bison@001</code> models.</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>tensorboard</code> <p>Vertex Tensorboard where to write the evaluation metrics. The Tensorboard must be in the same location as the tuning job.</p> <p> TYPE: <code>Optional[Union[Tensorboard, str]]</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@dataclasses.dataclass\nclass TuningEvaluationSpec:\n    \"\"\"Specification for model evaluation to perform during tuning.\n\n    Attributes:\n        evaluation_data: GCS URI of the evaluation dataset. This will run\n            model evaluation as part of the tuning job.\n        evaluation_interval: The evaluation will run at every\n            evaluation_interval tuning steps. Default: 20.\n        enable_early_stopping: If True, the tuning may stop early before\n            completing all the tuning steps. Requires evaluation_data.\n        enable_checkpoint_selection: If set to True, the tuning process returns\n            the best model checkpoint (based on model evaluation).\n            If set to False, the latest model checkpoint is returned.\n            If unset, the selection is only enabled for `*-bison@001` models.\n        tensorboard: Vertex Tensorboard where to write the evaluation metrics.\n            The Tensorboard must be in the same location as the tuning job.\n    \"\"\"\n\n    __module__ = \"vertexai.language_models\"\n\n    evaluation_data: Optional[str] = None\n    evaluation_interval: Optional[int] = None\n    enable_early_stopping: Optional[bool] = None\n    enable_checkpoint_selection: Optional[bool] = None\n    tensorboard: Optional[Union[aiplatform.Tensorboard, str]] = None\n</code></pre>"},{"location":"vertexai/preview/vision_models/","title":"Vision models","text":"<p>Classes for working with vision models.</p>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage","title":"GeneratedImage","text":"<p>               Bases: <code>Image</code></p> <p>Generated image.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class GeneratedImage(Image):\n    \"\"\"Generated image.\"\"\"\n\n    __module__ = \"vertexai.preview.vision_models\"\n\n    def __init__(\n        self,\n        image_bytes: Optional[bytes],\n        generation_parameters: Dict[str, Any],\n        gcs_uri: Optional[str] = None,\n    ):\n        \"\"\"Creates a `GeneratedImage` object.\n\n        Args:\n            image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n            generation_parameters: Image generation parameter values.\n            gcs_uri: Image file Google Cloud Storage uri.\n        \"\"\"\n        super().__init__(image_bytes=image_bytes, gcs_uri=gcs_uri)\n        self._generation_parameters = generation_parameters\n\n    @property\n    def generation_parameters(self):\n        \"\"\"Image generation parameters as a dictionary.\"\"\"\n        return self._generation_parameters\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"GeneratedImage\":\n        \"\"\"Loads image from file.\n\n        Args:\n            location: Local path from where to load the image.\n\n        Returns:\n            Loaded image as a `GeneratedImage` object.\n        \"\"\"\n        base_image = Image.load_from_file(location=location)\n        exif = base_image._pil_image.getexif()  # pylint: disable=protected-access\n        exif_comment_dict = json.loads(exif[_EXIF_USER_COMMENT_TAG_IDX])\n        generation_parameters = exif_comment_dict[_IMAGE_GENERATION_PARAMETERS_EXIF_KEY]\n        return GeneratedImage(\n            image_bytes=base_image._image_bytes,  # pylint: disable=protected-access\n            generation_parameters=generation_parameters,\n            gcs_uri=base_image._gcs_uri,  # pylint: disable=protected-access\n        )\n\n    def save(self, location: str, include_generation_parameters: bool = True):\n        \"\"\"Saves image to a file.\n\n        Args:\n            location: Local path where to save the image.\n            include_generation_parameters: Whether to include the image\n                generation parameters in the image's EXIF metadata.\n        \"\"\"\n        if include_generation_parameters:\n            if not self._generation_parameters:\n                raise ValueError(\"Image does not have generation parameters.\")\n            if not PIL_Image:\n                raise ValueError(\n                    \"The PIL module is required for saving generation parameters.\"\n                )\n\n            exif = self._pil_image.getexif()\n            exif[_EXIF_USER_COMMENT_TAG_IDX] = json.dumps(\n                {_IMAGE_GENERATION_PARAMETERS_EXIF_KEY: self._generation_parameters}\n            )\n            self._pil_image.save(location, exif=exif)\n        else:\n            super().save(location=location)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.generation_parameters","title":"generation_parameters  <code>property</code>","text":"<pre><code>generation_parameters\n</code></pre> <p>Image generation parameters as a dictionary.</p>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.__init__","title":"__init__","text":"<pre><code>__init__(\n    image_bytes: Optional[bytes],\n    generation_parameters: Dict[str, Any],\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>image_bytes</code> <p>Image file bytes. Image can be in PNG or JPEG format.</p> <p> TYPE: <code>Optional[bytes]</code> </p> <code>generation_parameters</code> <p>Image generation parameter values.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>gcs_uri</code> <p>Image file Google Cloud Storage uri.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    image_bytes: Optional[bytes],\n    generation_parameters: Dict[str, Any],\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates a `GeneratedImage` object.\n\n    Args:\n        image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n        generation_parameters: Image generation parameter values.\n        gcs_uri: Image file Google Cloud Storage uri.\n    \"\"\"\n    super().__init__(image_bytes=image_bytes, gcs_uri=gcs_uri)\n    self._generation_parameters = generation_parameters\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; GeneratedImage\n</code></pre> <p>Loads image from file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>GeneratedImage</code> <p>Loaded image as a <code>GeneratedImage</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"GeneratedImage\":\n    \"\"\"Loads image from file.\n\n    Args:\n        location: Local path from where to load the image.\n\n    Returns:\n        Loaded image as a `GeneratedImage` object.\n    \"\"\"\n    base_image = Image.load_from_file(location=location)\n    exif = base_image._pil_image.getexif()  # pylint: disable=protected-access\n    exif_comment_dict = json.loads(exif[_EXIF_USER_COMMENT_TAG_IDX])\n    generation_parameters = exif_comment_dict[_IMAGE_GENERATION_PARAMETERS_EXIF_KEY]\n    return GeneratedImage(\n        image_bytes=base_image._image_bytes,  # pylint: disable=protected-access\n        generation_parameters=generation_parameters,\n        gcs_uri=base_image._gcs_uri,  # pylint: disable=protected-access\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.save","title":"save","text":"<pre><code>save(\n    location: str,\n    include_generation_parameters: bool = True,\n)\n</code></pre> <p>Saves image to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the image.</p> <p> TYPE: <code>str</code> </p> <code>include_generation_parameters</code> <p>Whether to include the image generation parameters in the image's EXIF metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str, include_generation_parameters: bool = True):\n    \"\"\"Saves image to a file.\n\n    Args:\n        location: Local path where to save the image.\n        include_generation_parameters: Whether to include the image\n            generation parameters in the image's EXIF metadata.\n    \"\"\"\n    if include_generation_parameters:\n        if not self._generation_parameters:\n            raise ValueError(\"Image does not have generation parameters.\")\n        if not PIL_Image:\n            raise ValueError(\n                \"The PIL module is required for saving generation parameters.\"\n            )\n\n        exif = self._pil_image.getexif()\n        exif[_EXIF_USER_COMMENT_TAG_IDX] = json.dumps(\n            {_IMAGE_GENERATION_PARAMETERS_EXIF_KEY: self._generation_parameters}\n        )\n        self._pil_image.save(location, exif=exif)\n    else:\n        super().save(location=location)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image","title":"Image","text":"<p>Image.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class Image:\n    \"\"\"Image.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _loaded_bytes: Optional[bytes] = None\n    _loaded_image: Optional[\"PIL_Image.Image\"] = None\n    _gcs_uri: Optional[str] = None\n\n    def __init__(\n        self,\n        image_bytes: Optional[bytes] = None,\n        gcs_uri: Optional[str] = None,\n    ):\n        \"\"\"Creates an `Image` object.\n\n        Args:\n            image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n            gcs_uri: Image URI in Google Cloud Storage.\n        \"\"\"\n        if bool(image_bytes) == bool(gcs_uri):\n            raise ValueError(\"Either image_bytes or gcs_uri must be provided.\")\n\n        self._image_bytes = image_bytes\n        self._gcs_uri = gcs_uri\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"Image\":\n        \"\"\"Loads image from local file or Google Cloud Storage.\n\n        Args:\n            location: Local path or Google Cloud Storage uri from where to load\n                the image.\n\n        Returns:\n            Loaded image as an `Image` object.\n        \"\"\"\n        parsed_url = urllib.parse.urlparse(location)\n        if (\n            parsed_url.scheme == \"https\"\n            and parsed_url.netloc == \"storage.googleapis.com\"\n        ):\n            parsed_url = parsed_url._replace(\n                scheme=\"gs\", netloc=\"\", path=f\"/{urllib.parse.unquote(parsed_url.path)}\"\n            )\n            location = urllib.parse.urlunparse(parsed_url)\n\n        if parsed_url.scheme == \"gs\":\n            return Image(gcs_uri=location)\n\n        # Load image from local path\n        image_bytes = pathlib.Path(location).read_bytes()\n        image = Image(image_bytes=image_bytes)\n        return image\n\n    @property\n    def _blob(self) -&gt; storage.Blob:\n        if self._gcs_uri is None:\n            raise AttributeError(\"_blob is only supported when gcs_uri is set.\")\n        storage_client = storage.Client(\n            credentials=aiplatform_initializer.global_config.credentials\n        )\n        blob = storage.Blob.from_string(uri=self._gcs_uri, client=storage_client)\n        # Needed to populate `blob.content_type`\n        blob.reload()\n        return blob\n\n    @property\n    def _image_bytes(self) -&gt; bytes:\n        if self._loaded_bytes is None:\n            self._loaded_bytes = self._blob.download_as_bytes()\n        return self._loaded_bytes\n\n    @_image_bytes.setter\n    def _image_bytes(self, value: bytes):\n        self._loaded_bytes = value\n\n    @property\n    def _pil_image(self) -&gt; \"PIL_Image.Image\":\n        if self._loaded_image is None:\n            if not PIL_Image:\n                raise RuntimeError(\n                    \"The PIL module is not available. Please install the Pillow package.\"\n                )\n            self._loaded_image = PIL_Image.open(io.BytesIO(self._image_bytes))\n        return self._loaded_image\n\n    @property\n    def _size(self):\n        return self._pil_image.size\n\n    @property\n    def _mime_type(self) -&gt; str:\n        \"\"\"Returns the MIME type of the image.\"\"\"\n        if self._gcs_uri:\n            return self._blob.content_type\n        if PIL_Image:\n            return PIL_Image.MIME.get(self._pil_image.format, \"image/jpeg\")\n        # Fall back to jpeg\n        return \"image/jpeg\"\n\n    def show(self):\n        \"\"\"Shows the image.\n\n        This method only works when in a notebook environment.\n        \"\"\"\n        if PIL_Image and IPython_display:\n            IPython_display.display(self._pil_image)\n\n    def save(self, location: str):\n        \"\"\"Saves image to a file.\n\n        Args:\n            location: Local path where to save the image.\n        \"\"\"\n        pathlib.Path(location).write_bytes(self._image_bytes)\n\n    def _as_base64_string(self) -&gt; str:\n        \"\"\"Encodes image using the base64 encoding.\n\n        Returns:\n            Base64 encoding of the image as a string.\n        \"\"\"\n        # ! b64encode returns `bytes` object, not `str`.\n        # We need to convert `bytes` to `str`, otherwise we get service error:\n        # \"received initial metadata size exceeds limit\"\n        return base64.b64encode(self._image_bytes).decode(\"ascii\")\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image.__init__","title":"__init__","text":"<pre><code>__init__(\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>image_bytes</code> <p>Image file bytes. Image can be in PNG or JPEG format.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(image_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either image_bytes or gcs_uri must be provided.\")\n\n    self._image_bytes = image_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(location)\n    if (\n        parsed_url.scheme == \"https\"\n        and parsed_url.netloc == \"storage.googleapis.com\"\n    ):\n        parsed_url = parsed_url._replace(\n            scheme=\"gs\", netloc=\"\", path=f\"/{urllib.parse.unquote(parsed_url.path)}\"\n        )\n        location = urllib.parse.urlunparse(parsed_url)\n\n    if parsed_url.scheme == \"gs\":\n        return Image(gcs_uri=location)\n\n    # Load image from local path\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image(image_bytes=image_bytes)\n    return image\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves image to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the image.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves image to a file.\n\n    Args:\n        location: Local path where to save the image.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._image_bytes)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image.show","title":"show","text":"<pre><code>show()\n</code></pre> <p>Shows the image.</p> <p>This method only works when in a notebook environment.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def show(self):\n    \"\"\"Shows the image.\n\n    This method only works when in a notebook environment.\n    \"\"\"\n    if PIL_Image and IPython_display:\n        IPython_display.display(self._pil_image)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageCaptioningModel","title":"ImageCaptioningModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates captions from image.</p> <p>Examples::</p> <pre><code>model = ImageCaptioningModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class ImageCaptioningModel(\n    _model_garden_models._ModelGardenModel  # pylint: disable=protected-access\n):\n    \"\"\"Generates captions from image.\n\n    Examples::\n\n        model = ImageCaptioningModel.from_pretrained(\"imagetext@001\")\n        image = Image.load_from_file(\"image.png\")\n        captions = model.get_captions(\n            image=image,\n            # Optional:\n            number_of_results=1,\n            language=\"en\",\n        )\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml\"\n\n    def get_captions(\n        self,\n        image: Image,\n        *,\n        number_of_results: int = 1,\n        language: str = \"en\",\n        output_gcs_uri: Optional[str] = None,\n    ) -&gt; List[str]:\n        \"\"\"Generates captions for a given image.\n\n        Args:\n            image: The image to get captions for. Size limit: 10 MB.\n            number_of_results: Number of captions to produce. Range: 1-3.\n            language: Language to use for captions.\n                Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n            output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n        Returns:\n            A list of image caption strings.\n        \"\"\"\n        instance = {}\n\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n        parameters = {\n            \"sampleCount\": number_of_results,\n            \"language\": language,\n        }\n        if output_gcs_uri is not None:\n            parameters[\"storageUri\"] = output_gcs_uri\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n        return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageCaptioningModel.get_captions","title":"get_captions","text":"<pre><code>get_captions(\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None\n) -&gt; List[str]\n</code></pre> <p>Generates captions for a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>language</code> <p>Language to use for captions. Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the captioned images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of image caption strings.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_captions(\n    self,\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None,\n) -&gt; List[str]:\n    \"\"\"Generates captions for a given image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        number_of_results: Number of captions to produce. Range: 1-3.\n        language: Language to use for captions.\n            Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n        output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n    Returns:\n        A list of image caption strings.\n    \"\"\"\n    instance = {}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n        \"language\": language,\n    }\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel","title":"ImageGenerationModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates images from text prompt.</p> <p>Examples::</p> <pre><code>model = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\nresponse = model.generate_images(\n    prompt=\"Astronaut riding a horse\",\n    # Optional:\n    number_of_images=1,\n    seed=0,\n)\nresponse[0].show()\nresponse[0].save(\"image1.png\")\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class ImageGenerationModel(\n    _model_garden_models._ModelGardenModel  # pylint: disable=protected-access\n):\n    \"\"\"Generates images from text prompt.\n\n    Examples::\n\n        model = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\n        response = model.generate_images(\n            prompt=\"Astronaut riding a horse\",\n            # Optional:\n            number_of_images=1,\n            seed=0,\n        )\n        response[0].show()\n        response[0].save(\"image1.png\")\n    \"\"\"\n\n    __module__ = \"vertexai.preview.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_generative_model_1.0.0.yaml\"\n\n    def _generate_images(\n        self,\n        prompt: str,\n        *,\n        negative_prompt: Optional[str] = None,\n        number_of_images: int = 1,\n        width: Optional[int] = None,\n        height: Optional[int] = None,\n        aspect_ratio: Optional[Literal[\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"]] = None,\n        guidance_scale: Optional[float] = None,\n        seed: Optional[int] = None,\n        base_image: Optional[\"Image\"] = None,\n        mask: Optional[\"Image\"] = None,\n        edit_mode: Optional[\n            Literal[\n                \"inpainting-insert\",\n                \"inpainting-remove\",\n                \"outpainting\",\n                \"product-image\",\n            ]\n        ] = None,\n        mask_mode: Optional[Literal[\"background\", \"foreground\", \"semantic\"]] = None,\n        segmentation_classes: Optional[List[str]] = None,\n        mask_dilation: Optional[float] = None,\n        product_position: Optional[Literal[\"fixed\", \"reposition\"]] = None,\n        output_mime_type: Optional[Literal[\"image/png\", \"image/jpeg\"]] = None,\n        compression_quality: Optional[float] = None,\n        language: Optional[str] = None,\n        output_gcs_uri: Optional[str] = None,\n        add_watermark: Optional[bool] = None,\n        safety_filter_level: Optional[\n            Literal[\"block_most\", \"block_some\", \"block_few\", \"block_fewest\"]\n        ] = None,\n        person_generation: Optional[\n            Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n        ] = None,\n    ) -&gt; \"ImageGenerationResponse\":\n        \"\"\"Generates images from text prompt.\n\n        Args:\n            prompt: Text prompt for the image.\n            negative_prompt: A description of what you want to omit in the generated\n              images.\n            number_of_images: Number of images to generate. Range: 1..8.\n            width: Width of the image. One of the sizes must be 256 or 1024.\n            height: Height of the image. One of the sizes must be 256 or 1024.\n            aspect_ratio: Aspect ratio for the image. Supported values are:\n                * 1:1 - Square image\n                * 9:16 - Portait image\n                * 16:9 - Landscape image\n                * 4:3 - Landscape, desktop ratio.\n                * 3:4 - Portrait, desktop ratio\n            guidance_scale: Controls the strength of the prompt. Suggested values\n              are - * 0-9 (low strength) * 10-20 (medium strength) * 21+ (high\n              strength)\n            seed: Image generation random seed.\n            base_image: Base image to use for the image generation.\n            mask: Mask for the base image.\n            edit_mode: Describes the editing mode for the request. Supported values\n              are - * inpainting-insert: fills the mask area based on the text\n              prompt (requires mask and text) * inpainting-remove: removes the\n              object(s) in the mask area. (requires mask)\n                * outpainting: extend the image based on the mask area. (Requires\n                  mask) * product-image: Changes the background for the predominant\n                  product or subject in the image\n            mask_mode: Solicits generation of the mask (v/s providing mask as an\n              input). Supported values are:\n                * background: Automatically generates a mask for all regions except\n                  the primary subject(s) of the image\n                * foreground: Automatically generates a mask for the primary\n                  subjects(s) of the image.\n                * semantic: Segment one or more of the segmentation classes using\n                  class ID\n            segmentation_classes: List of class IDs for segmentation. Max of 5 IDs\n            mask_dilation: Defines the dilation percentage of the mask provided.\n              Float between 0 and 1. Defaults to 0.03\n            product_position: Defines whether the product should stay fixed or be\n              repositioned. Supported Values:\n                * fixed: Fixed position\n                * reposition: Can be moved (default)\n            output_mime_type: Which image format should the output be saved as.\n              Supported values: * image/png: Save as a PNG image * image/jpeg: Save\n              as a JPEG image\n            compression_quality: Level of compression if the output mime type is\n              selected to be image/jpeg. Float between 0 to 100\n            language: Language of the text prompt for the image. Default: None.\n              Supported values are `\"en\"` for English, `\"hi\"` for Hindi, `\"ja\"` for\n              Japanese, `\"ko\"` for Korean, and `\"auto\"` for automatic language\n              detection.\n            output_gcs_uri: Google Cloud Storage uri to store the generated images.\n            add_watermark: Add a watermark to the generated image\n            safety_filter_level: Adds a filter level to Safety filtering. Supported\n              values are: * \"block_most\" : Strongest filtering level, most strict\n              blocking * \"block_some\" : Block some problematic prompts and responses\n              * \"block_few\" : Block fewer problematic prompts and responses *\n              \"block_fewest\" : Block very few problematic prompts and responses\n            person_generation: Allow generation of people by the model Supported\n              values are: * \"dont_allow\" : Block generation of people *\n              \"allow_adult\" : Generate adults, but not children * \"allow_all\" :\n              Generate adults and children\n\n        Returns:\n            An `ImageGenerationResponse` object.\n        \"\"\"\n        # Note: Only a single prompt is supported by the service.\n        instance = {\"prompt\": prompt}\n        shared_generation_parameters = {\n            \"prompt\": prompt,\n            # b/295946075 The service stopped supporting image sizes.\n            # \"width\": width,\n            # \"height\": height,\n            \"number_of_images_in_batch\": number_of_images,\n        }\n\n        if base_image:\n            if base_image._gcs_uri:  # pylint: disable=protected-access\n                instance[\"image\"] = {\n                    \"gcsUri\": base_image._gcs_uri  # pylint: disable=protected-access\n                }\n                shared_generation_parameters[\n                    \"base_image_uri\"\n                ] = base_image._gcs_uri  # pylint: disable=protected-access\n            else:\n                instance[\"image\"] = {\n                    \"bytesBase64Encoded\": base_image._as_base64_string()  # pylint: disable=protected-access\n                }\n                shared_generation_parameters[\"base_image_hash\"] = hashlib.sha1(\n                    base_image._image_bytes  # pylint: disable=protected-access\n                ).hexdigest()\n\n        if mask:\n            if mask._gcs_uri:  # pylint: disable=protected-access\n                instance[\"mask\"] = {\n                    \"image\": {\n                        \"gcsUri\": mask._gcs_uri  # pylint: disable=protected-access\n                    },\n                }\n                shared_generation_parameters[\n                    \"mask_uri\"\n                ] = mask._gcs_uri  # pylint: disable=protected-access\n            else:\n                instance[\"mask\"] = {\n                    \"image\": {\n                        \"bytesBase64Encoded\": mask._as_base64_string()  # pylint: disable=protected-access\n                    },\n                }\n                shared_generation_parameters[\"mask_hash\"] = hashlib.sha1(\n                    mask._image_bytes  # pylint: disable=protected-access\n                ).hexdigest()\n\n        parameters = {}\n        max_size = max(width or 0, height or 0) or None\n        if aspect_ratio is not None:\n            parameters[\"aspectRatio\"] = aspect_ratio\n        elif max_size:\n            # Note: The size needs to be a string\n            parameters[\"sampleImageSize\"] = str(max_size)\n            if height is not None and width is not None and height != width:\n                parameters[\"aspectRatio\"] = f\"{width}:{height}\"\n\n        parameters[\"sampleCount\"] = number_of_images\n        if negative_prompt:\n            parameters[\"negativePrompt\"] = negative_prompt\n            shared_generation_parameters[\"negative_prompt\"] = negative_prompt\n\n        if seed is not None:\n            # Note: String seed and numerical seed give different results\n            parameters[\"seed\"] = seed\n            shared_generation_parameters[\"seed\"] = seed\n\n        if guidance_scale is not None:\n            parameters[\"guidanceScale\"] = guidance_scale\n            shared_generation_parameters[\"guidance_scale\"] = guidance_scale\n\n        if language is not None:\n            parameters[\"language\"] = language\n            shared_generation_parameters[\"language\"] = language\n\n        if output_gcs_uri is not None:\n            parameters[\"storageUri\"] = output_gcs_uri\n            shared_generation_parameters[\"storage_uri\"] = output_gcs_uri\n\n        parameters[\"editConfig\"] = {}\n        if edit_mode is not None:\n            parameters[\"editConfig\"][\"editMode\"] = edit_mode\n            shared_generation_parameters[\"edit_mode\"] = edit_mode\n\n        if mask is None and edit_mode != \"product-image\":\n            parameters[\"editConfig\"][\"maskMode\"] = {}\n            if mask_mode is not None:\n                parameters[\"editConfig\"][\"maskMode\"][\"maskType\"] = mask_mode\n                shared_generation_parameters[\"mask_mode\"] = mask_mode\n\n            if segmentation_classes is not None:\n                parameters[\"editConfig\"][\"maskMode\"][\"classes\"] = segmentation_classes\n                shared_generation_parameters[\"classes\"] = segmentation_classes\n\n        if mask_dilation is not None:\n            parameters[\"editConfig\"][\"maskDilation\"] = mask_dilation\n            shared_generation_parameters[\"mask_dilation\"] = mask_dilation\n\n        if product_position is not None:\n            parameters[\"editConfig\"][\"productPosition\"] = product_position\n            shared_generation_parameters[\"product_position\"] = product_position\n\n        parameters[\"outputOptions\"] = {}\n        if output_mime_type is not None:\n            parameters[\"outputOptions\"][\"mimeType\"] = output_mime_type\n            shared_generation_parameters[\"mime_type\"] = output_mime_type\n\n        if compression_quality is not None:\n            parameters[\"outputOptions\"][\"compressionQuality\"] = compression_quality\n            shared_generation_parameters[\"compression_quality\"] = compression_quality\n\n        if add_watermark is not None:\n            parameters[\"addWatermark\"] = add_watermark\n            shared_generation_parameters[\"add_watermark\"] = add_watermark\n\n        if safety_filter_level is not None:\n            parameters[\"safetySetting\"] = safety_filter_level\n            shared_generation_parameters[\"safety_filter_level\"] = safety_filter_level\n\n        if person_generation is not None:\n            parameters[\"personGeneration\"] = person_generation\n            shared_generation_parameters[\"person_generation\"] = person_generation\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n\n        generated_images: List[\"GeneratedImage\"] = []\n        for idx, prediction in enumerate(response.predictions):\n            generation_parameters = dict(shared_generation_parameters)\n            generation_parameters[\"index_of_image_in_batch\"] = idx\n            encoded_bytes = prediction.get(\"bytesBase64Encoded\")\n            generated_image = GeneratedImage(\n                image_bytes=base64.b64decode(encoded_bytes) if encoded_bytes else None,\n                generation_parameters=generation_parameters,\n                gcs_uri=prediction.get(\"gcsUri\"),\n            )\n            generated_images.append(generated_image)\n\n        return ImageGenerationResponse(images=generated_images)\n\n    def generate_images(\n        self,\n        prompt: str,\n        *,\n        negative_prompt: Optional[str] = None,\n        number_of_images: int = 1,\n        aspect_ratio: Optional[Literal[\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"]] = None,\n        guidance_scale: Optional[float] = None,\n        language: Optional[str] = None,\n        seed: Optional[int] = None,\n        output_gcs_uri: Optional[str] = None,\n        add_watermark: Optional[bool] = True,\n        safety_filter_level: Optional[\n            Literal[\"block_most\", \"block_some\", \"block_few\", \"block_fewest\"]\n        ] = None,\n        person_generation: Optional[\n            Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n        ] = None,\n    ) -&gt; \"ImageGenerationResponse\":\n        \"\"\"Generates images from text prompt.\n\n        Args:\n            prompt: Text prompt for the image.\n            negative_prompt: A description of what you want to omit in the generated\n                images.\n            number_of_images: Number of images to generate. Range: 1..8.\n            aspect_ratio: Changes the aspect ratio of the generated image Supported\n                values are:\n                * \"1:1\" : 1:1 aspect ratio\n                * \"9:16\" : 9:16 aspect ratio\n                * \"16:9\" : 16:9 aspect ratio\n                * \"4:3\" : 4:3 aspect ratio\n                * \"3:4\" : 3:4 aspect_ratio\n            guidance_scale: Controls the strength of the prompt. Suggested values are:\n                * 0-9 (low strength)\n                * 10-20 (medium strength)\n                * 21+ (high strength)\n            language: Language of the text prompt for the image. Default: None.\n                Supported values are `\"en\"` for English, `\"hi\"` for Hindi, `\"ja\"`\n                for Japanese, `\"ko\"` for Korean, and `\"auto\"` for automatic language\n                detection.\n            seed: Image generation random seed.\n            output_gcs_uri: Google Cloud Storage uri to store the generated images.\n            add_watermark: Add a watermark to the generated image\n            safety_filter_level: Adds a filter level to Safety filtering. Supported\n                values are:\n                * \"block_most\" : Strongest filtering level, most strict\n                blocking\n                * \"block_some\" : Block some problematic prompts and responses\n                * \"block_few\" : Block fewer problematic prompts and responses\n                * \"block_fewest\" : Block very few problematic prompts and responses\n            person_generation: Allow generation of people by the model Supported\n                values are:\n                * \"dont_allow\" : Block generation of people\n                * \"allow_adult\" : Generate adults, but not children\n                * \"allow_all\" : Generate adults and children\n        Returns:\n            An `ImageGenerationResponse` object.\n        \"\"\"\n        return self._generate_images(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            number_of_images=number_of_images,\n            aspect_ratio=aspect_ratio,\n            guidance_scale=guidance_scale,\n            language=language,\n            seed=seed,\n            output_gcs_uri=output_gcs_uri,\n            add_watermark=add_watermark,\n            safety_filter_level=safety_filter_level,\n            person_generation=person_generation,\n        )\n\n    def edit_image(\n        self,\n        *,\n        prompt: str,\n        base_image: \"Image\",\n        mask: Optional[\"Image\"] = None,\n        negative_prompt: Optional[str] = None,\n        number_of_images: int = 1,\n        guidance_scale: Optional[float] = None,\n        edit_mode: Optional[\n            Literal[\n                \"inpainting-insert\", \"inpainting-remove\", \"outpainting\", \"product-image\"\n            ]\n        ] = None,\n        mask_mode: Optional[Literal[\"background\", \"foreground\", \"semantic\"]] = None,\n        segmentation_classes: Optional[List[str]] = None,\n        mask_dilation: Optional[float] = None,\n        product_position: Optional[Literal[\"fixed\", \"reposition\"]] = None,\n        output_mime_type: Optional[Literal[\"image/png\", \"image/jpeg\"]] = None,\n        compression_quality: Optional[float] = None,\n        language: Optional[str] = None,\n        seed: Optional[int] = None,\n        output_gcs_uri: Optional[str] = None,\n        safety_filter_level: Optional[\n            Literal[\"block_most\", \"block_some\", \"block_few\", \"block_fewest\"]\n        ] = None,\n        person_generation: Optional[\n            Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n        ] = None,\n    ) -&gt; \"ImageGenerationResponse\":\n        \"\"\"Edits an existing image based on text prompt.\n\n        Args:\n            prompt: Text prompt for the image.\n            base_image: Base image from which to generate the new image.\n            mask: Mask for the base image.\n            negative_prompt: A description of what you want to omit in\n                the generated images.\n            number_of_images: Number of images to generate. Range: 1..8.\n            guidance_scale: Controls the strength of the prompt.\n                Suggested values are:\n                * 0-9 (low strength)\n                * 10-20 (medium strength)\n                * 21+ (high strength)\n            edit_mode: Describes the editing mode for the request. Supported values are:\n                * inpainting-insert: fills the mask area based on the text prompt\n                (requires mask and text)\n                * inpainting-remove: removes the object(s) in the mask area.\n                (requires mask)\n                * outpainting: extend the image based on the mask area.\n                (Requires mask)\n                * product-image: Changes the background for the predominant product\n                or subject in the image\n            mask_mode: Solicits generation of the mask (v/s providing mask as an\n                input). Supported values are:\n                * background: Automatically generates a mask for all regions except\n                the primary subject(s) of the image\n                * foreground: Automatically generates a mask for the primary\n                subjects(s) of the image.\n                * semantic: Segment one or more of the segmentation classes using\n                class ID\n            segmentation_classes: List of class IDs for segmentation. Max of 5 IDs\n            mask_dilation: Defines the dilation percentage of the mask provided.\n                Float between 0 and 1. Defaults to 0.03\n            product_position: Defines whether the product should stay fixed or be\n                repositioned. Supported Values:\n                * fixed: Fixed position\n                * reposition: Can be moved (default)\n            output_mime_type: Which image format should the output be saved as.\n                Supported values:\n                * image/png: Save as a PNG image\n                * image/jpeg: Save as a JPEG image\n            compression_quality: Level of compression if the output mime type is\n              selected to be image/jpeg. Float between 0 to 100\n            language: Language of the text prompt for the image. Default: None.\n                Supported values are `\"en\"` for English, `\"hi\"` for Hindi,\n                `\"ja\"` for Japanese, `\"ko\"` for Korean, and `\"auto\"` for\n                automatic language detection.\n            seed: Image generation random seed.\n            output_gcs_uri: Google Cloud Storage uri to store the edited images.\n            safety_filter_level: Adds a filter level to Safety filtering. Supported\n                values are:\n                * \"block_most\" : Strongest filtering level, most strict\n                blocking\n                * \"block_some\" : Block some problematic prompts and responses\n                * \"block_few\" : Block fewer problematic prompts and responses\n                * \"block_fewest\" : Block very few problematic prompts and responses\n            person_generation: Allow generation of people by the model Supported\n                values are:\n                * \"dont_allow\" : Block generation of people\n                * \"allow_adult\" : Generate adults, but not children\n                * \"allow_all\" : Generate adults and children\n\n        Returns:\n            An `ImageGenerationResponse` object.\n        \"\"\"\n        return self._generate_images(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            number_of_images=number_of_images,\n            guidance_scale=guidance_scale,\n            seed=seed,\n            base_image=base_image,\n            mask=mask,\n            edit_mode=edit_mode,\n            mask_mode=mask_mode,\n            segmentation_classes=segmentation_classes,\n            mask_dilation=mask_dilation,\n            product_position=product_position,\n            output_mime_type=output_mime_type,\n            compression_quality=compression_quality,\n            language=language,\n            output_gcs_uri=output_gcs_uri,\n            add_watermark=False,  # Not supported for editing yet\n            safety_filter_level=safety_filter_level,\n            person_generation=person_generation,\n        )\n\n    def upscale_image(\n        self,\n        image: Union[\"Image\", \"GeneratedImage\"],\n        new_size: Optional[int] = 2048,\n        output_gcs_uri: Optional[str] = None,\n    ) -&gt; \"Image\":\n        \"\"\"Upscales an image.\n\n        This supports upscaling images generated through the `generate_images()` method,\n        or upscaling a new image that is 1024x1024.\n\n        Examples::\n\n            # Upscale a generated image\n            model = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\n            response = model.generate_images(\n                prompt=\"Astronaut riding a horse\",\n            )\n            model.upscale_image(image=response[0])\n\n            # Upscale a new 1024x1024 image\n            my_image = Image.load_from_file(\"my-image.png\")\n            model.upscale_image(image=my_image)\n\n        Args:\n            image (Union[GeneratedImage, Image]):\n                Required. The generated image to upscale.\n            new_size (int):\n                The size of the biggest dimension of the upscaled image. Only 2048 and 4096 are currently\n                supported. Results in a 2048x2048 or 4096x4096 image. Defaults to 2048 if not provided.\n            output_gcs_uri: Google Cloud Storage uri to store the upscaled images.\n\n        Returns:\n            An `Image` object.\n        \"\"\"\n\n        # Currently this method only supports 1024x1024 images\n        if image._size[0] != 1024 and image._size[1] != 1024:\n            raise ValueError(\n                \"Upscaling is currently only supported on images that are 1024x1024.\"\n            )\n\n        if new_size not in _SUPPORTED_UPSCALING_SIZES:\n            raise ValueError(\n                f\"Only the folowing square upscaling sizes are currently supported: {_SUPPORTED_UPSCALING_SIZES}.\"\n            )\n\n        instance = {\"prompt\": \"\"}\n\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n\n        parameters = {\n            \"sampleImageSize\": str(new_size),\n            \"sampleCount\": 1,\n            \"mode\": \"upscale\",\n        }\n\n        if output_gcs_uri is not None:\n            parameters[\"storageUri\"] = output_gcs_uri\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n\n        upscaled_image = response.predictions[0]\n\n        if isinstance(image, GeneratedImage):\n            generation_parameters = image.generation_parameters\n\n        else:\n            generation_parameters = {}\n\n        generation_parameters[\"upscaled_image_size\"] = new_size\n\n        encoded_bytes = upscaled_image.get(\"bytesBase64Encoded\")\n        return GeneratedImage(\n            image_bytes=base64.b64decode(encoded_bytes) if encoded_bytes else None,\n            generation_parameters=generation_parameters,\n            gcs_uri=upscaled_image.get(\"gcsUri\"),\n        )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel.edit_image","title":"edit_image","text":"<pre><code>edit_image(\n    *,\n    prompt: str,\n    base_image: Image,\n    mask: Optional[Image] = None,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    guidance_scale: Optional[float] = None,\n    edit_mode: Optional[\n        Literal[\n            \"inpainting-insert\",\n            \"inpainting-remove\",\n            \"outpainting\",\n            \"product-image\",\n        ]\n    ] = None,\n    mask_mode: Optional[\n        Literal[\"background\", \"foreground\", \"semantic\"]\n    ] = None,\n    segmentation_classes: Optional[List[str]] = None,\n    mask_dilation: Optional[float] = None,\n    product_position: Optional[\n        Literal[\"fixed\", \"reposition\"]\n    ] = None,\n    output_mime_type: Optional[\n        Literal[\"image/png\", \"image/jpeg\"]\n    ] = None,\n    compression_quality: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    safety_filter_level: Optional[\n        Literal[\n            \"block_most\",\n            \"block_some\",\n            \"block_few\",\n            \"block_fewest\",\n        ]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None\n) -&gt; ImageGenerationResponse\n</code></pre> <p>Edits an existing image based on text prompt.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Text prompt for the image.</p> <p> TYPE: <code>str</code> </p> <code>base_image</code> <p>Base image from which to generate the new image.</p> <p> TYPE: <code>Image</code> </p> <code>mask</code> <p>Mask for the base image.</p> <p> TYPE: <code>Optional[Image]</code> DEFAULT: <code>None</code> </p> <code>negative_prompt</code> <p>A description of what you want to omit in the generated images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>number_of_images</code> <p>Number of images to generate. Range: 1..8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>guidance_scale</code> <p>Controls the strength of the prompt. Suggested values are: * 0-9 (low strength) * 10-20 (medium strength) * 21+ (high strength)</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>edit_mode</code> <p>Describes the editing mode for the request. Supported values are: * inpainting-insert: fills the mask area based on the text prompt (requires mask and text) * inpainting-remove: removes the object(s) in the mask area. (requires mask) * outpainting: extend the image based on the mask area. (Requires mask) * product-image: Changes the background for the predominant product or subject in the image</p> <p> TYPE: <code>Optional[Literal['inpainting-insert', 'inpainting-remove', 'outpainting', 'product-image']]</code> DEFAULT: <code>None</code> </p> <code>mask_mode</code> <p>Solicits generation of the mask (v/s providing mask as an input). Supported values are: * background: Automatically generates a mask for all regions except the primary subject(s) of the image * foreground: Automatically generates a mask for the primary subjects(s) of the image. * semantic: Segment one or more of the segmentation classes using class ID</p> <p> TYPE: <code>Optional[Literal['background', 'foreground', 'semantic']]</code> DEFAULT: <code>None</code> </p> <code>segmentation_classes</code> <p>List of class IDs for segmentation. Max of 5 IDs</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>mask_dilation</code> <p>Defines the dilation percentage of the mask provided. Float between 0 and 1. Defaults to 0.03</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>product_position</code> <p>Defines whether the product should stay fixed or be repositioned. Supported Values: * fixed: Fixed position * reposition: Can be moved (default)</p> <p> TYPE: <code>Optional[Literal['fixed', 'reposition']]</code> DEFAULT: <code>None</code> </p> <code>output_mime_type</code> <p>Which image format should the output be saved as. Supported values: * image/png: Save as a PNG image * image/jpeg: Save as a JPEG image</p> <p> TYPE: <code>Optional[Literal['image/png', 'image/jpeg']]</code> DEFAULT: <code>None</code> </p> <code>compression_quality</code> <p>Level of compression if the output mime type is selected to be image/jpeg. Float between 0 to 100</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language of the text prompt for the image. Default: None. Supported values are <code>\"en\"</code> for English, <code>\"hi\"</code> for Hindi, <code>\"ja\"</code> for Japanese, <code>\"ko\"</code> for Korean, and <code>\"auto\"</code> for automatic language detection.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Image generation random seed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the edited images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>safety_filter_level</code> <p>Adds a filter level to Safety filtering. Supported values are: * \"block_most\" : Strongest filtering level, most strict blocking * \"block_some\" : Block some problematic prompts and responses * \"block_few\" : Block fewer problematic prompts and responses * \"block_fewest\" : Block very few problematic prompts and responses</p> <p> TYPE: <code>Optional[Literal['block_most', 'block_some', 'block_few', 'block_fewest']]</code> DEFAULT: <code>None</code> </p> <code>person_generation</code> <p>Allow generation of people by the model Supported values are: * \"dont_allow\" : Block generation of people * \"allow_adult\" : Generate adults, but not children * \"allow_all\" : Generate adults and children</p> <p> TYPE: <code>Optional[Literal['dont_allow', 'allow_adult', 'allow_all']]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ImageGenerationResponse</code> <p>An <code>ImageGenerationResponse</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def edit_image(\n    self,\n    *,\n    prompt: str,\n    base_image: \"Image\",\n    mask: Optional[\"Image\"] = None,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    guidance_scale: Optional[float] = None,\n    edit_mode: Optional[\n        Literal[\n            \"inpainting-insert\", \"inpainting-remove\", \"outpainting\", \"product-image\"\n        ]\n    ] = None,\n    mask_mode: Optional[Literal[\"background\", \"foreground\", \"semantic\"]] = None,\n    segmentation_classes: Optional[List[str]] = None,\n    mask_dilation: Optional[float] = None,\n    product_position: Optional[Literal[\"fixed\", \"reposition\"]] = None,\n    output_mime_type: Optional[Literal[\"image/png\", \"image/jpeg\"]] = None,\n    compression_quality: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    safety_filter_level: Optional[\n        Literal[\"block_most\", \"block_some\", \"block_few\", \"block_fewest\"]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None,\n) -&gt; \"ImageGenerationResponse\":\n    \"\"\"Edits an existing image based on text prompt.\n\n    Args:\n        prompt: Text prompt for the image.\n        base_image: Base image from which to generate the new image.\n        mask: Mask for the base image.\n        negative_prompt: A description of what you want to omit in\n            the generated images.\n        number_of_images: Number of images to generate. Range: 1..8.\n        guidance_scale: Controls the strength of the prompt.\n            Suggested values are:\n            * 0-9 (low strength)\n            * 10-20 (medium strength)\n            * 21+ (high strength)\n        edit_mode: Describes the editing mode for the request. Supported values are:\n            * inpainting-insert: fills the mask area based on the text prompt\n            (requires mask and text)\n            * inpainting-remove: removes the object(s) in the mask area.\n            (requires mask)\n            * outpainting: extend the image based on the mask area.\n            (Requires mask)\n            * product-image: Changes the background for the predominant product\n            or subject in the image\n        mask_mode: Solicits generation of the mask (v/s providing mask as an\n            input). Supported values are:\n            * background: Automatically generates a mask for all regions except\n            the primary subject(s) of the image\n            * foreground: Automatically generates a mask for the primary\n            subjects(s) of the image.\n            * semantic: Segment one or more of the segmentation classes using\n            class ID\n        segmentation_classes: List of class IDs for segmentation. Max of 5 IDs\n        mask_dilation: Defines the dilation percentage of the mask provided.\n            Float between 0 and 1. Defaults to 0.03\n        product_position: Defines whether the product should stay fixed or be\n            repositioned. Supported Values:\n            * fixed: Fixed position\n            * reposition: Can be moved (default)\n        output_mime_type: Which image format should the output be saved as.\n            Supported values:\n            * image/png: Save as a PNG image\n            * image/jpeg: Save as a JPEG image\n        compression_quality: Level of compression if the output mime type is\n          selected to be image/jpeg. Float between 0 to 100\n        language: Language of the text prompt for the image. Default: None.\n            Supported values are `\"en\"` for English, `\"hi\"` for Hindi,\n            `\"ja\"` for Japanese, `\"ko\"` for Korean, and `\"auto\"` for\n            automatic language detection.\n        seed: Image generation random seed.\n        output_gcs_uri: Google Cloud Storage uri to store the edited images.\n        safety_filter_level: Adds a filter level to Safety filtering. Supported\n            values are:\n            * \"block_most\" : Strongest filtering level, most strict\n            blocking\n            * \"block_some\" : Block some problematic prompts and responses\n            * \"block_few\" : Block fewer problematic prompts and responses\n            * \"block_fewest\" : Block very few problematic prompts and responses\n        person_generation: Allow generation of people by the model Supported\n            values are:\n            * \"dont_allow\" : Block generation of people\n            * \"allow_adult\" : Generate adults, but not children\n            * \"allow_all\" : Generate adults and children\n\n    Returns:\n        An `ImageGenerationResponse` object.\n    \"\"\"\n    return self._generate_images(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        number_of_images=number_of_images,\n        guidance_scale=guidance_scale,\n        seed=seed,\n        base_image=base_image,\n        mask=mask,\n        edit_mode=edit_mode,\n        mask_mode=mask_mode,\n        segmentation_classes=segmentation_classes,\n        mask_dilation=mask_dilation,\n        product_position=product_position,\n        output_mime_type=output_mime_type,\n        compression_quality=compression_quality,\n        language=language,\n        output_gcs_uri=output_gcs_uri,\n        add_watermark=False,  # Not supported for editing yet\n        safety_filter_level=safety_filter_level,\n        person_generation=person_generation,\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel.generate_images","title":"generate_images","text":"<pre><code>generate_images(\n    prompt: str,\n    *,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    aspect_ratio: Optional[\n        Literal[\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"]\n    ] = None,\n    guidance_scale: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    add_watermark: Optional[bool] = True,\n    safety_filter_level: Optional[\n        Literal[\n            \"block_most\",\n            \"block_some\",\n            \"block_few\",\n            \"block_fewest\",\n        ]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None\n) -&gt; ImageGenerationResponse\n</code></pre> <p>Generates images from text prompt.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Text prompt for the image.</p> <p> TYPE: <code>str</code> </p> <code>negative_prompt</code> <p>A description of what you want to omit in the generated images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>number_of_images</code> <p>Number of images to generate. Range: 1..8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>aspect_ratio</code> <p>Changes the aspect ratio of the generated image Supported values are: * \"1:1\" : 1:1 aspect ratio * \"9:16\" : 9:16 aspect ratio * \"16:9\" : 16:9 aspect ratio * \"4:3\" : 4:3 aspect ratio * \"3:4\" : 3:4 aspect_ratio</p> <p> TYPE: <code>Optional[Literal['1:1', '9:16', '16:9', '4:3', '3:4']]</code> DEFAULT: <code>None</code> </p> <code>guidance_scale</code> <p>Controls the strength of the prompt. Suggested values are: * 0-9 (low strength) * 10-20 (medium strength) * 21+ (high strength)</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language of the text prompt for the image. Default: None. Supported values are <code>\"en\"</code> for English, <code>\"hi\"</code> for Hindi, <code>\"ja\"</code> for Japanese, <code>\"ko\"</code> for Korean, and <code>\"auto\"</code> for automatic language detection.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Image generation random seed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the generated images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>add_watermark</code> <p>Add a watermark to the generated image</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>True</code> </p> <code>safety_filter_level</code> <p>Adds a filter level to Safety filtering. Supported values are: * \"block_most\" : Strongest filtering level, most strict blocking * \"block_some\" : Block some problematic prompts and responses * \"block_few\" : Block fewer problematic prompts and responses * \"block_fewest\" : Block very few problematic prompts and responses</p> <p> TYPE: <code>Optional[Literal['block_most', 'block_some', 'block_few', 'block_fewest']]</code> DEFAULT: <code>None</code> </p> <code>person_generation</code> <p>Allow generation of people by the model Supported values are: * \"dont_allow\" : Block generation of people * \"allow_adult\" : Generate adults, but not children * \"allow_all\" : Generate adults and children</p> <p> TYPE: <code>Optional[Literal['dont_allow', 'allow_adult', 'allow_all']]</code> DEFAULT: <code>None</code> </p> <p>Returns:     An <code>ImageGenerationResponse</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def generate_images(\n    self,\n    prompt: str,\n    *,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    aspect_ratio: Optional[Literal[\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"]] = None,\n    guidance_scale: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    add_watermark: Optional[bool] = True,\n    safety_filter_level: Optional[\n        Literal[\"block_most\", \"block_some\", \"block_few\", \"block_fewest\"]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None,\n) -&gt; \"ImageGenerationResponse\":\n    \"\"\"Generates images from text prompt.\n\n    Args:\n        prompt: Text prompt for the image.\n        negative_prompt: A description of what you want to omit in the generated\n            images.\n        number_of_images: Number of images to generate. Range: 1..8.\n        aspect_ratio: Changes the aspect ratio of the generated image Supported\n            values are:\n            * \"1:1\" : 1:1 aspect ratio\n            * \"9:16\" : 9:16 aspect ratio\n            * \"16:9\" : 16:9 aspect ratio\n            * \"4:3\" : 4:3 aspect ratio\n            * \"3:4\" : 3:4 aspect_ratio\n        guidance_scale: Controls the strength of the prompt. Suggested values are:\n            * 0-9 (low strength)\n            * 10-20 (medium strength)\n            * 21+ (high strength)\n        language: Language of the text prompt for the image. Default: None.\n            Supported values are `\"en\"` for English, `\"hi\"` for Hindi, `\"ja\"`\n            for Japanese, `\"ko\"` for Korean, and `\"auto\"` for automatic language\n            detection.\n        seed: Image generation random seed.\n        output_gcs_uri: Google Cloud Storage uri to store the generated images.\n        add_watermark: Add a watermark to the generated image\n        safety_filter_level: Adds a filter level to Safety filtering. Supported\n            values are:\n            * \"block_most\" : Strongest filtering level, most strict\n            blocking\n            * \"block_some\" : Block some problematic prompts and responses\n            * \"block_few\" : Block fewer problematic prompts and responses\n            * \"block_fewest\" : Block very few problematic prompts and responses\n        person_generation: Allow generation of people by the model Supported\n            values are:\n            * \"dont_allow\" : Block generation of people\n            * \"allow_adult\" : Generate adults, but not children\n            * \"allow_all\" : Generate adults and children\n    Returns:\n        An `ImageGenerationResponse` object.\n    \"\"\"\n    return self._generate_images(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        number_of_images=number_of_images,\n        aspect_ratio=aspect_ratio,\n        guidance_scale=guidance_scale,\n        language=language,\n        seed=seed,\n        output_gcs_uri=output_gcs_uri,\n        add_watermark=add_watermark,\n        safety_filter_level=safety_filter_level,\n        person_generation=person_generation,\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel.upscale_image","title":"upscale_image","text":"<pre><code>upscale_image(\n    image: Union[Image, GeneratedImage],\n    new_size: Optional[int] = 2048,\n    output_gcs_uri: Optional[str] = None,\n) -&gt; Image\n</code></pre> <p>Upscales an image.</p> <p>This supports upscaling images generated through the <code>generate_images()</code> method, or upscaling a new image that is 1024x1024.</p> <p>Examples::</p> <pre><code># Upscale a generated image\nmodel = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\nresponse = model.generate_images(\n    prompt=\"Astronaut riding a horse\",\n)\nmodel.upscale_image(image=response[0])\n\n# Upscale a new 1024x1024 image\nmy_image = Image.load_from_file(\"my-image.png\")\nmodel.upscale_image(image=my_image)\n</code></pre> PARAMETER DESCRIPTION <code>image</code> <p>Required. The generated image to upscale.</p> <p> TYPE: <code>Union[GeneratedImage, Image]</code> </p> <code>new_size</code> <p>The size of the biggest dimension of the upscaled image. Only 2048 and 4096 are currently supported. Results in a 2048x2048 or 4096x4096 image. Defaults to 2048 if not provided.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2048</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the upscaled images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>An <code>Image</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def upscale_image(\n    self,\n    image: Union[\"Image\", \"GeneratedImage\"],\n    new_size: Optional[int] = 2048,\n    output_gcs_uri: Optional[str] = None,\n) -&gt; \"Image\":\n    \"\"\"Upscales an image.\n\n    This supports upscaling images generated through the `generate_images()` method,\n    or upscaling a new image that is 1024x1024.\n\n    Examples::\n\n        # Upscale a generated image\n        model = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\n        response = model.generate_images(\n            prompt=\"Astronaut riding a horse\",\n        )\n        model.upscale_image(image=response[0])\n\n        # Upscale a new 1024x1024 image\n        my_image = Image.load_from_file(\"my-image.png\")\n        model.upscale_image(image=my_image)\n\n    Args:\n        image (Union[GeneratedImage, Image]):\n            Required. The generated image to upscale.\n        new_size (int):\n            The size of the biggest dimension of the upscaled image. Only 2048 and 4096 are currently\n            supported. Results in a 2048x2048 or 4096x4096 image. Defaults to 2048 if not provided.\n        output_gcs_uri: Google Cloud Storage uri to store the upscaled images.\n\n    Returns:\n        An `Image` object.\n    \"\"\"\n\n    # Currently this method only supports 1024x1024 images\n    if image._size[0] != 1024 and image._size[1] != 1024:\n        raise ValueError(\n            \"Upscaling is currently only supported on images that are 1024x1024.\"\n        )\n\n    if new_size not in _SUPPORTED_UPSCALING_SIZES:\n        raise ValueError(\n            f\"Only the folowing square upscaling sizes are currently supported: {_SUPPORTED_UPSCALING_SIZES}.\"\n        )\n\n    instance = {\"prompt\": \"\"}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n\n    parameters = {\n        \"sampleImageSize\": str(new_size),\n        \"sampleCount\": 1,\n        \"mode\": \"upscale\",\n    }\n\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n\n    upscaled_image = response.predictions[0]\n\n    if isinstance(image, GeneratedImage):\n        generation_parameters = image.generation_parameters\n\n    else:\n        generation_parameters = {}\n\n    generation_parameters[\"upscaled_image_size\"] = new_size\n\n    encoded_bytes = upscaled_image.get(\"bytesBase64Encoded\")\n    return GeneratedImage(\n        image_bytes=base64.b64decode(encoded_bytes) if encoded_bytes else None,\n        generation_parameters=generation_parameters,\n        gcs_uri=upscaled_image.get(\"gcsUri\"),\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationResponse","title":"ImageGenerationResponse  <code>dataclass</code>","text":"<p>Image generation response.</p> ATTRIBUTE DESCRIPTION <code>images</code> <p>The list of generated images.</p> <p> TYPE: <code>List[GeneratedImage]</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@dataclasses.dataclass\nclass ImageGenerationResponse:\n    \"\"\"Image generation response.\n\n    Attributes:\n        images: The list of generated images.\n    \"\"\"\n\n    __module__ = \"vertexai.preview.vision_models\"\n\n    images: List[\"GeneratedImage\"]\n\n    def __iter__(self) -&gt; typing.Iterator[\"GeneratedImage\"]:\n        \"\"\"Iterates through the generated images.\"\"\"\n        yield from self.images\n\n    def __getitem__(self, idx: int) -&gt; \"GeneratedImage\":\n        \"\"\"Gets the generated image by index.\"\"\"\n        return self.images[idx]\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationResponse.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; GeneratedImage\n</code></pre> <p>Gets the generated image by index.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; \"GeneratedImage\":\n    \"\"\"Gets the generated image by index.\"\"\"\n    return self.images[idx]\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationResponse.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; typing.Iterator[GeneratedImage]\n</code></pre> <p>Iterates through the generated images.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __iter__(self) -&gt; typing.Iterator[\"GeneratedImage\"]:\n    \"\"\"Iterates through the generated images.\"\"\"\n    yield from self.images\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageQnAModel","title":"ImageQnAModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Answers questions about an image.</p> <p>Examples::</p> <pre><code>model = ImageQnAModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class ImageQnAModel(\n    _model_garden_models._ModelGardenModel  # pylint: disable=protected-access\n):\n    \"\"\"Answers questions about an image.\n\n    Examples::\n\n        model = ImageQnAModel.from_pretrained(\"imagetext@001\")\n        image = Image.load_from_file(\"image.png\")\n        answers = model.ask_question(\n            image=image,\n            question=\"What color is the car in this image?\",\n            # Optional:\n            number_of_results=1,\n        )\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml\"\n\n    def ask_question(\n        self,\n        image: Image,\n        question: str,\n        *,\n        number_of_results: int = 1,\n    ) -&gt; List[str]:\n        \"\"\"Answers questions about an image.\n\n        Args:\n            image: The image to get captions for. Size limit: 10 MB.\n            question: Question to ask about the image.\n            number_of_results: Number of captions to produce. Range: 1-3.\n\n        Returns:\n            A list of answers.\n        \"\"\"\n        instance = {\"prompt\": question}\n\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n        parameters = {\n            \"sampleCount\": number_of_results,\n        }\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n        return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageQnAModel.ask_question","title":"ask_question","text":"<pre><code>ask_question(\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1\n) -&gt; List[str]\n</code></pre> <p>Answers questions about an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>question</code> <p>Question to ask about the image.</p> <p> TYPE: <code>str</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of answers.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def ask_question(\n    self,\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1,\n) -&gt; List[str]:\n    \"\"\"Answers questions about an image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        question: Question to ask about the image.\n        number_of_results: Number of captions to produce. Range: 1-3.\n\n    Returns:\n        A list of answers.\n    \"\"\"\n    instance = {\"prompt\": question}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n    }\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageTextModel","title":"ImageTextModel","text":"<p>               Bases: <code>ImageCaptioningModel</code>, <code>ImageQnAModel</code></p> <p>Generates text from images.</p> <p>Examples::</p> <pre><code>model = ImageTextModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\n\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class ImageTextModel(ImageCaptioningModel, ImageQnAModel):\n    \"\"\"Generates text from images.\n\n    Examples::\n\n        model = ImageTextModel.from_pretrained(\"imagetext@001\")\n        image = Image.load_from_file(\"image.png\")\n\n        captions = model.get_captions(\n            image=image,\n            # Optional:\n            number_of_results=1,\n            language=\"en\",\n        )\n\n        answers = model.ask_question(\n            image=image,\n            question=\"What color is the car in this image?\",\n            # Optional:\n            number_of_results=1,\n        )\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    # NOTE: Using this ImageTextModel class is recommended over using ImageQnAModel or ImageCaptioningModel,\n    # since SDK Model Garden classes should follow the design pattern of exactly 1 SDK class to 1 Model Garden schema URI\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml\"\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingModel","title":"MultiModalEmbeddingModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates embedding vectors from images and videos.</p> <p>Examples::</p> <pre><code>model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\nimage = Image.load_from_file(\"image.png\")\nvideo = Video.load_from_file(\"video.mp4\")\n\nembeddings = model.get_embeddings(\n    image=image,\n    video=video,\n    contextual_text=\"Hello world\",\n)\nimage_embedding = embeddings.image_embedding\nvideo_embeddings = embeddings.video_embeddings\ntext_embedding = embeddings.text_embedding\n</code></pre> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class MultiModalEmbeddingModel(_model_garden_models._ModelGardenModel):\n    \"\"\"Generates embedding vectors from images and videos.\n\n    Examples::\n\n        model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\n        image = Image.load_from_file(\"image.png\")\n        video = Video.load_from_file(\"video.mp4\")\n\n        embeddings = model.get_embeddings(\n            image=image,\n            video=video,\n            contextual_text=\"Hello world\",\n        )\n        image_embedding = embeddings.image_embedding\n        video_embeddings = embeddings.video_embeddings\n        text_embedding = embeddings.text_embedding\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/vision_embedding_model_1.0.0.yaml\"\n\n    def get_embeddings(\n        self,\n        image: Optional[Image] = None,\n        video: Optional[Video] = None,\n        contextual_text: Optional[str] = None,\n        dimension: Optional[int] = None,\n        video_segment_config: Optional[VideoSegmentConfig] = None,\n    ) -&gt; \"MultiModalEmbeddingResponse\":\n        \"\"\"Gets embedding vectors from the provided image.\n\n        Args:\n            image (Image): Optional. The image to generate embeddings for. One of\n              `image`, `video`, or `contextual_text` is required.\n            video (Video): Optional. The video to generate embeddings for. One of\n              `image`, `video` or `contextual_text` is required.\n            contextual_text (str): Optional. Contextual text for your input image or video.\n              If provided, the model will also generate an embedding vector for the\n              provided contextual text. The returned image and text embedding\n              vectors are in the same semantic space with the same dimensionality,\n              and the vectors can be used interchangeably for use cases like\n              searching image by text or searching text by image. One of `image`, `video` or\n              `contextual_text` is required.\n            dimension (int): Optional. The number of embedding dimensions. Lower\n              values offer decreased latency when using these embeddings for\n              subsequent tasks, while higher values offer better accuracy.\n              Available values: `128`, `256`, `512`, and `1408` (default).\n            video_segment_config (VideoSegmentConfig): Optional. The specific\n              video segments (in seconds) the embeddings are generated for.\n\n        Returns:\n            MultiModalEmbeddingResponse:\n                The image and text embedding vectors.\n        \"\"\"\n\n        if not image and not video and not contextual_text:\n            raise ValueError(\n                \"One of `image`, `video`, or `contextual_text` is required.\"\n            )\n\n        instance = {}\n\n        if image:\n            if image._gcs_uri:  # pylint: disable=protected-access\n                instance[\"image\"] = {\n                    \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n                }\n            else:\n                instance[\"image\"] = {\n                    \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n                }\n\n        if video:\n            if video._gcs_uri:  # pylint: disable=protected-access\n                instance[\"video\"] = {\n                    \"gcsUri\": video._gcs_uri  # pylint: disable=protected-access\n                }\n            else:\n                instance[\"video\"] = {\n                    \"bytesBase64Encoded\": video._as_base64_string()  # pylint: disable=protected-access\n                }  # pylint: disable=protected-access\n\n            if video_segment_config:\n                instance[\"video\"][\"videoSegmentConfig\"] = {\n                    \"startOffsetSec\": video_segment_config.start_offset_sec,\n                    \"endOffsetSec\": video_segment_config.end_offset_sec,\n                    \"intervalSec\": video_segment_config.interval_sec,\n                }\n\n        if contextual_text:\n            instance[\"text\"] = contextual_text\n\n        parameters = {}\n        if dimension:\n            parameters[\"dimension\"] = dimension\n\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n        image_embedding = response.predictions[0].get(\"imageEmbedding\")\n        video_embeddings = []\n        for video_embedding in response.predictions[0].get(\"videoEmbeddings\", []):\n            video_embeddings.append(\n                VideoEmbedding(\n                    embedding=video_embedding[\"embedding\"],\n                    start_offset_sec=video_embedding[\"startOffsetSec\"],\n                    end_offset_sec=video_embedding[\"endOffsetSec\"],\n                )\n            )\n        text_embedding = (\n            response.predictions[0].get(\"textEmbedding\")\n            if \"textEmbedding\" in response.predictions[0]\n            else None\n        )\n        return MultiModalEmbeddingResponse(\n            image_embedding=image_embedding,\n            video_embeddings=video_embeddings,\n            _prediction_response=response,\n            text_embedding=text_embedding,\n        )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[\n        VideoSegmentConfig\n    ] = None,\n) -&gt; MultiModalEmbeddingResponse\n</code></pre> <p>Gets embedding vectors from the provided image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Optional. The image to generate embeddings for. One of <code>image</code>, <code>video</code>, or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Image</code> DEFAULT: <code>None</code> </p> <code>video</code> <p>Optional. The video to generate embeddings for. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Video</code> DEFAULT: <code>None</code> </p> <code>contextual_text</code> <p>Optional. Contextual text for your input image or video. If provided, the model will also generate an embedding vector for the provided contextual text. The returned image and text embedding vectors are in the same semantic space with the same dimensionality, and the vectors can be used interchangeably for use cases like searching image by text or searching text by image. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>dimension</code> <p>Optional. The number of embedding dimensions. Lower values offer decreased latency when using these embeddings for subsequent tasks, while higher values offer better accuracy. Available values: <code>128</code>, <code>256</code>, <code>512</code>, and <code>1408</code> (default).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>video_segment_config</code> <p>Optional. The specific video segments (in seconds) the embeddings are generated for.</p> <p> TYPE: <code>VideoSegmentConfig</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiModalEmbeddingResponse</code> <p>The image and text embedding vectors.</p> <p> TYPE: <code>MultiModalEmbeddingResponse</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_embeddings(\n    self,\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[VideoSegmentConfig] = None,\n) -&gt; \"MultiModalEmbeddingResponse\":\n    \"\"\"Gets embedding vectors from the provided image.\n\n    Args:\n        image (Image): Optional. The image to generate embeddings for. One of\n          `image`, `video`, or `contextual_text` is required.\n        video (Video): Optional. The video to generate embeddings for. One of\n          `image`, `video` or `contextual_text` is required.\n        contextual_text (str): Optional. Contextual text for your input image or video.\n          If provided, the model will also generate an embedding vector for the\n          provided contextual text. The returned image and text embedding\n          vectors are in the same semantic space with the same dimensionality,\n          and the vectors can be used interchangeably for use cases like\n          searching image by text or searching text by image. One of `image`, `video` or\n          `contextual_text` is required.\n        dimension (int): Optional. The number of embedding dimensions. Lower\n          values offer decreased latency when using these embeddings for\n          subsequent tasks, while higher values offer better accuracy.\n          Available values: `128`, `256`, `512`, and `1408` (default).\n        video_segment_config (VideoSegmentConfig): Optional. The specific\n          video segments (in seconds) the embeddings are generated for.\n\n    Returns:\n        MultiModalEmbeddingResponse:\n            The image and text embedding vectors.\n    \"\"\"\n\n    if not image and not video and not contextual_text:\n        raise ValueError(\n            \"One of `image`, `video`, or `contextual_text` is required.\"\n        )\n\n    instance = {}\n\n    if image:\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n\n    if video:\n        if video._gcs_uri:  # pylint: disable=protected-access\n            instance[\"video\"] = {\n                \"gcsUri\": video._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"video\"] = {\n                \"bytesBase64Encoded\": video._as_base64_string()  # pylint: disable=protected-access\n            }  # pylint: disable=protected-access\n\n        if video_segment_config:\n            instance[\"video\"][\"videoSegmentConfig\"] = {\n                \"startOffsetSec\": video_segment_config.start_offset_sec,\n                \"endOffsetSec\": video_segment_config.end_offset_sec,\n                \"intervalSec\": video_segment_config.interval_sec,\n            }\n\n    if contextual_text:\n        instance[\"text\"] = contextual_text\n\n    parameters = {}\n    if dimension:\n        parameters[\"dimension\"] = dimension\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    image_embedding = response.predictions[0].get(\"imageEmbedding\")\n    video_embeddings = []\n    for video_embedding in response.predictions[0].get(\"videoEmbeddings\", []):\n        video_embeddings.append(\n            VideoEmbedding(\n                embedding=video_embedding[\"embedding\"],\n                start_offset_sec=video_embedding[\"startOffsetSec\"],\n                end_offset_sec=video_embedding[\"endOffsetSec\"],\n            )\n        )\n    text_embedding = (\n        response.predictions[0].get(\"textEmbedding\")\n        if \"textEmbedding\" in response.predictions[0]\n        else None\n    )\n    return MultiModalEmbeddingResponse(\n        image_embedding=image_embedding,\n        video_embeddings=video_embeddings,\n        _prediction_response=response,\n        text_embedding=text_embedding,\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingResponse","title":"MultiModalEmbeddingResponse  <code>dataclass</code>","text":"<p>The multimodal embedding response.</p> ATTRIBUTE DESCRIPTION <code>image_embedding</code> <p>Optional. The embedding vector generated from your image.</p> <p> TYPE: <code>List[float]</code> </p> <code>video_embeddings</code> <p>Optional. The embedding vectors generated from your video.</p> <p> TYPE: <code>List[VideoEmbedding]</code> </p> <code>text_embedding</code> <p>Optional. The embedding vector generated from the contextual text provided for your image or video.</p> <p> TYPE: <code>List[float]</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@dataclasses.dataclass\nclass MultiModalEmbeddingResponse:\n    \"\"\"The multimodal embedding response.\n\n    Attributes:\n        image_embedding (List[float]):\n            Optional. The embedding vector generated from your image.\n        video_embeddings (List[VideoEmbedding]):\n            Optional. The embedding vectors generated from your video.\n        text_embedding (List[float]):\n            Optional. The embedding vector generated from the contextual text provided for your image or video.\n    \"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _prediction_response: Any\n    image_embedding: Optional[List[float]] = None\n    video_embeddings: Optional[List[VideoEmbedding]] = None\n    text_embedding: Optional[List[float]] = None\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Video","title":"Video","text":"<p>Video.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class Video:\n    \"\"\"Video.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    _loaded_bytes: Optional[bytes] = None\n    _gcs_uri: Optional[str] = None\n\n    def __init__(\n        self,\n        video_bytes: Optional[bytes] = None,\n        gcs_uri: Optional[str] = None,\n    ):\n        \"\"\"Creates an `Image` object.\n\n        Args:\n            video_bytes: Video file bytes. Video can be in AVI, FLV, MKV, MOV,\n                MP4, MPEG, MPG, WEBM, and WMV formats.\n            gcs_uri: Image URI in Google Cloud Storage.\n        \"\"\"\n        if bool(video_bytes) == bool(gcs_uri):\n            raise ValueError(\"Either video_bytes or gcs_uri must be provided.\")\n\n        self._video_bytes = video_bytes\n        self._gcs_uri = gcs_uri\n\n    @staticmethod\n    def load_from_file(location: str) -&gt; \"Video\":\n        \"\"\"Loads video from local file or Google Cloud Storage.\n\n        Args:\n            location: Local path or Google Cloud Storage uri from where to load\n                the video.\n\n        Returns:\n            Loaded video as an `Video` object.\n        \"\"\"\n        if location.startswith(\"gs://\"):\n            return Video(gcs_uri=location)\n\n        video_bytes = pathlib.Path(location).read_bytes()\n        video = Video(video_bytes=video_bytes)\n        return video\n\n    @property\n    def _blob(self) -&gt; storage.Blob:\n        if self._gcs_uri is None:\n            raise AttributeError(\"_blob is only supported when gcs_uri is set.\")\n        storage_client = storage.Client(\n            credentials=aiplatform_initializer.global_config.credentials\n        )\n        blob = storage.Blob.from_string(uri=self._gcs_uri, client=storage_client)\n        # Needed to populate `blob.content_type`\n        blob.reload()\n        return blob\n\n    @property\n    def _video_bytes(self) -&gt; bytes:\n        if self._loaded_bytes is None:\n            self._loaded_bytes = self._blob.download_as_bytes()\n        return self._loaded_bytes\n\n    @_video_bytes.setter\n    def _video_bytes(self, value: bytes):\n        self._loaded_bytes = value\n\n    @property\n    def _mime_type(self) -&gt; str:\n        \"\"\"Returns the MIME type of the video.\"\"\"\n        if self._gcs_uri:\n            return self._blob.content_type\n        # Fall back to mp4\n        return \"video/mp4\"\n\n    def save(self, location: str):\n        \"\"\"Saves video to a file.\n\n        Args:\n            location: Local path where to save the video.\n        \"\"\"\n        pathlib.Path(location).write_bytes(self._video_bytes)\n\n    def _as_base64_string(self) -&gt; str:\n        \"\"\"Encodes video using the base64 encoding.\n\n        Returns:\n            Base64 encoding of the video as a string.\n        \"\"\"\n        # ! b64encode returns `bytes` object, not `str`.\n        # We need to convert `bytes` to `str`, otherwise we get service error:\n        # \"received initial metadata size exceeds limit\"\n        return base64.b64encode(self._video_bytes).decode(\"ascii\")\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Video.__init__","title":"__init__","text":"<pre><code>__init__(\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>video_bytes</code> <p>Video file bytes. Video can be in AVI, FLV, MKV, MOV, MP4, MPEG, MPG, WEBM, and WMV formats.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        video_bytes: Video file bytes. Video can be in AVI, FLV, MKV, MOV,\n            MP4, MPEG, MPG, WEBM, and WMV formats.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(video_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either video_bytes or gcs_uri must be provided.\")\n\n    self._video_bytes = video_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Video.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Video\n</code></pre> <p>Loads video from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the video.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Video</code> <p>Loaded video as an <code>Video</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Video\":\n    \"\"\"Loads video from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the video.\n\n    Returns:\n        Loaded video as an `Video` object.\n    \"\"\"\n    if location.startswith(\"gs://\"):\n        return Video(gcs_uri=location)\n\n    video_bytes = pathlib.Path(location).read_bytes()\n    video = Video(video_bytes=video_bytes)\n    return video\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Video.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves video to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the video.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves video to a file.\n\n    Args:\n        location: Local path where to save the video.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._video_bytes)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoEmbedding","title":"VideoEmbedding","text":"<p>Embeddings generated from video with offset times.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class VideoEmbedding:\n    \"\"\"Embeddings generated from video with offset times.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    start_offset_sec: int\n    end_offset_sec: int\n    embedding: List[float]\n\n    def __init__(\n        self, start_offset_sec: int, end_offset_sec: int, embedding: List[float]\n    ):\n        \"\"\"Creates a `VideoEmbedding` object.\n\n        Args:\n            start_offset_sec: Start time offset (in seconds) of generated embeddings.\n            end_offset_sec: End time offset (in seconds) of generated embeddings.\n            embedding: Generated embedding for interval.\n        \"\"\"\n        self.start_offset_sec = start_offset_sec\n        self.end_offset_sec = end_offset_sec\n        self.embedding = embedding\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoEmbedding.__init__","title":"__init__","text":"<pre><code>__init__(\n    start_offset_sec: int,\n    end_offset_sec: int,\n    embedding: List[float],\n)\n</code></pre> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>embedding</code> <p>Generated embedding for interval.</p> <p> TYPE: <code>List[float]</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self, start_offset_sec: int, end_offset_sec: int, embedding: List[float]\n):\n    \"\"\"Creates a `VideoEmbedding` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) of generated embeddings.\n        end_offset_sec: End time offset (in seconds) of generated embeddings.\n        embedding: Generated embedding for interval.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.embedding = embedding\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoSegmentConfig","title":"VideoSegmentConfig","text":"<p>The specific video segments (in seconds) the embeddings are generated for.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class VideoSegmentConfig:\n    \"\"\"The specific video segments (in seconds) the embeddings are generated for.\"\"\"\n\n    __module__ = \"vertexai.vision_models\"\n\n    start_offset_sec: int\n    end_offset_sec: int\n    interval_sec: int\n\n    def __init__(\n        self,\n        start_offset_sec: int = 0,\n        end_offset_sec: int = 120,\n        interval_sec: int = 16,\n    ):\n        \"\"\"Creates a `VideoSegmentConfig` object.\n\n        Args:\n            start_offset_sec: Start time offset (in seconds) to generate embeddings for.\n            end_offset_sec: End time offset (in seconds) to generate embeddings for.\n            interval_sec: Interval to divide video for generated embeddings.\n        \"\"\"\n        self.start_offset_sec = start_offset_sec\n        self.end_offset_sec = end_offset_sec\n        self.interval_sec = interval_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoSegmentConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n)\n</code></pre> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>120</code> </p> <code>interval_sec</code> <p>Interval to divide video for generated embeddings.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n):\n    \"\"\"Creates a `VideoSegmentConfig` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) to generate embeddings for.\n        end_offset_sec: End time offset (in seconds) to generate embeddings for.\n        interval_sec: Interval to divide video for generated embeddings.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.interval_sec = interval_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.WatermarkVerificationModel","title":"WatermarkVerificationModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Verifies if an image has a watermark.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>class WatermarkVerificationModel(_model_garden_models._ModelGardenModel):\n    \"\"\"Verifies if an image has a watermark.\"\"\"\n\n    __module__ = \"vertexai.preview.vision_models\"\n\n    _INSTANCE_SCHEMA_URI = \"gs://google-cloud-aiplatform/schema/predict/instance/watermark_verification_model_1.0.0.yaml\"\n\n    def verify_image(self, image: Image) -&gt; WatermarkVerificationResponse:\n        \"\"\"Verifies the watermark of an image.\n\n        Args:\n            image: The image to verify.\n\n        Returns:\n            A WatermarkVerificationResponse, containing the confidence level of\n            the image being watermarked.\n        \"\"\"\n        if not image:\n            raise ValueError(\"Image is required.\")\n\n        instance = {}\n\n        if image._gcs_uri:\n            instance[\"image\"] = {\"gcsUri\": image._gcs_uri}\n        else:\n            instance[\"image\"] = {\"bytesBase64Encoded\": image._as_base64_string()}\n\n        parameters = {}\n        response = self._endpoint.predict(\n            instances=[instance],\n            parameters=parameters,\n        )\n\n        verification_likelihood = response.predictions[0].get(\"decision\")\n        return WatermarkVerificationResponse(\n            _prediction_response=response,\n            watermark_verification_result=verification_likelihood,\n        )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.WatermarkVerificationModel.verify_image","title":"verify_image","text":"<pre><code>verify_image(image: Image) -&gt; WatermarkVerificationResponse\n</code></pre> <p>Verifies the watermark of an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to verify.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>WatermarkVerificationResponse</code> <p>A WatermarkVerificationResponse, containing the confidence level of</p> <code>WatermarkVerificationResponse</code> <p>the image being watermarked.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def verify_image(self, image: Image) -&gt; WatermarkVerificationResponse:\n    \"\"\"Verifies the watermark of an image.\n\n    Args:\n        image: The image to verify.\n\n    Returns:\n        A WatermarkVerificationResponse, containing the confidence level of\n        the image being watermarked.\n    \"\"\"\n    if not image:\n        raise ValueError(\"Image is required.\")\n\n    instance = {}\n\n    if image._gcs_uri:\n        instance[\"image\"] = {\"gcsUri\": image._gcs_uri}\n    else:\n        instance[\"image\"] = {\"bytesBase64Encoded\": image._as_base64_string()}\n\n    parameters = {}\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n\n    verification_likelihood = response.predictions[0].get(\"decision\")\n    return WatermarkVerificationResponse(\n        _prediction_response=response,\n        watermark_verification_result=verification_likelihood,\n    )\n</code></pre>"}]}