{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vertex Generative AI SDK for Python","text":"<p>The Vertex Generative AI SDK helps developers use Google's generative AI Gemini models and PaLM language models to build AI-powered features and applications. The SDKs support use cases like the following:</p> <ul> <li>Generate text from texts, images and videos (multimodal generation)</li> <li>Build stateful multi-turn conversations (chat)</li> <li>Function calling</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install the google-cloud-aiplatform Python package, run the following command:</p> <pre><code>pip3 install --upgrade --user \"google-cloud-aiplatform&gt;=1.38\"\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>For detailed instructions, see quickstart and Introduction to multimodal classes in the Vertex AI SDK.</p>"},{"location":"#imports","title":"Imports:","text":"<pre><code>from vertexai.generative_models import GenerativeModel, Image, Content, Part, Tool, FunctionDeclaration, GenerationConfig\n</code></pre>"},{"location":"#basic-generation","title":"Basic generation:","text":"<pre><code>from vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\"Why is sky blue?\"))\n</code></pre>"},{"location":"#using-images-and-videos","title":"Using images and videos","text":"<pre><code>from vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-pro-vision\")\n\n# Local image\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_model.generate_content([\"What is shown in this image?\", image]))\n\n# Image from Cloud Storage\nimage_part = generative_models.Part.from_uri(\"gs://download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg\", mime_type=\"image/jpeg\")\nprint(vision_model.generate_content([image_part, \"Describe this image?\"]))\n\n# Text and video\nvideo_part = Part.from_uri(\"gs://cloud-samples-data/video/animals.mp4\", mime_type=\"video/mp4\")\nprint(vision_model.generate_content([\"What is in the video? \", video_part]))\n</code></pre>"},{"location":"#chat","title":"Chat","text":"<pre><code>from vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-ultra-vision\")\nvision_chat = vision_model.start_chat()\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_chat.send_message([\"I like this image.\", image]))\nprint(vision_chat.send_message(\"What things do I like?.\"))\n</code></pre>"},{"location":"#system-instructions","title":"System instructions","text":"<pre><code>from vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\n    \"gemini-1.0-pro\",\n    system_instruction=[\n        \"Talk like a pirate.\",\n        \"Don't use rude words.\",\n    ],\n)\nprint(model.generate_content(\"Why is sky blue?\"))\n</code></pre>"},{"location":"#function-calling","title":"Function calling","text":"<pre><code># First, create tools that the model is can use to answer your questions.\n# Describe a function by specifying it's schema (JsonSchema format)\nget_current_weather_func = generative_models.FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"celsius\",\n                    \"fahrenheit\",\n                ]\n            }\n        },\n        \"required\": [\n            \"location\"\n        ]\n    },\n)\n# Tool is a collection of related functions\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n\n# Use tools in chat:\nmodel = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\n# Send a message to the model. The model will respond with a function call.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\n# Then send a function response to the model. The model will use it to answer.\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather\": \"super nice\"},\n        }\n    ),\n))\n</code></pre>"},{"location":"#automatic-function-calling","title":"Automatic Function calling","text":"<pre><code>from vertexai.preview.generative_models import GenerativeModel, Tool, FunctionDeclaration, AutomaticFunctionCallingResponder\n\n# First, create functions that the model can use to answer your questions.\ndef get_current_weather(location: str, unit: str = \"centigrade\"):\n    \"\"\"Gets weather in the specified location.\n\n    Args:\n        location: The location for which to get the weather.\n        unit: Optional. Temperature unit. Can be Centigrade or Fahrenheit. Defaults to Centigrade.\n    \"\"\"\n    return dict(\n        location=location,\n        unit=unit,\n        weather=\"Super nice, but maybe a bit hot.\",\n    )\n\n# Infer function schema\nget_current_weather_func = FunctionDeclaration.from_func(get_current_weather)\n# Tool is a collection of related functions\nweather_tool = Tool(\n    function_declarations=[get_current_weather_func],\n)\n\n# Use tools in chat:\nmodel = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\n\n# Activate automatic function calling:\nafc_responder = AutomaticFunctionCallingResponder(\n    # Optional:\n    max_automatic_function_calls=5,\n)\nchat = model.start_chat(responder=afc_responder)\n# Send a message to the model. The model will respond with a function call.\n# The SDK will automatically call the requested function and respond to the model.\n# The model will use the function call response to answer the original question.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>You can find complete documentation for the Vertex AI SDKs and the Gemini model in the Google Cloud documentation</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See Contributing for more information on contributing to the Vertex AI Python SDK.</p>"},{"location":"#license","title":"License","text":"<p>The contents of this repository are licensed under the Apache License, version 2.0.</p>"},{"location":"vertexai/","title":"Index","text":""},{"location":"vertexai/#vertexai","title":"vertexai","text":"<p>The vertexai module.</p>"},{"location":"vertexai/generative_models/","title":"Generative models","text":""},{"location":"vertexai/generative_models/#vertexai.generative_models","title":"vertexai.generative_models","text":"<p>Classes for working with the Gemini models.</p>"},{"location":"vertexai/generative_models/#vertexai.generative_models.FinishReason","title":"FinishReason  <code>module-attribute</code>","text":"<pre><code>FinishReason = FinishReason\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.HarmCategory","title":"HarmCategory  <code>module-attribute</code>","text":"<pre><code>HarmCategory = HarmCategory\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.HarmBlockThreshold","title":"HarmBlockThreshold  <code>module-attribute</code>","text":"<pre><code>HarmBlockThreshold = HarmBlockThreshold\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerativeModel","title":"GenerativeModel","text":"<pre><code>GenerativeModel(\n    model_name: str,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    tool_config: Optional[ToolConfig] = None,\n    system_instruction: Optional[PartsType] = None\n)\n</code></pre> <p>               Bases: <code>_GenerativeModel</code></p> Usage <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\"Hello\"))\n</code></pre> PARAMETER DESCRIPTION <code>model_name</code> <p>Model Garden model resource name. Alternatively, a tuned model endpoint resource name can be provided.</p> <p> TYPE: <code>str</code> </p> <code>generation_config</code> <p>Default generation config to use in generate_content.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Default safety settings to use in generate_content.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>Default tools to use in generate_content.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>tool_config</code> <p>Default tool config to use in generate_content.</p> <p> TYPE: <code>Optional[ToolConfig]</code> DEFAULT: <code>None</code> </p> <code>system_instruction</code> <p>Default system instruction to use in generate_content. Note: Only text should be used in parts. Content of each part will become a separate paragraph.</p> <p> TYPE: <code>Optional[PartsType]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    tool_config: Optional[\"ToolConfig\"] = None,\n    system_instruction: Optional[PartsType] = None,\n):\n    r\"\"\"Initializes GenerativeModel.\n\n    Usage:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\"Hello\"))\n        ```\n\n    Args:\n        model_name: Model Garden model resource name.\n            Alternatively, a tuned model endpoint resource name can be provided.\n        generation_config: Default generation config to use in generate_content.\n        safety_settings: Default safety settings to use in generate_content.\n        tools: Default tools to use in generate_content.\n        tool_config: Default tool config to use in generate_content.\n        system_instruction: Default system instruction to use in generate_content.\n            Note: Only text should be used in parts.\n            Content of each part will become a separate paragraph.\n    \"\"\"\n    if not model_name:\n        raise ValueError(\"model_name must not be empty\")\n    if \"/\" not in model_name:\n        model_name = \"publishers/google/models/\" + model_name\n    if model_name.startswith(\"models/\"):\n        model_name = \"publishers/google/\" + model_name\n\n    project = aiplatform_initializer.global_config.project\n    location = aiplatform_initializer.global_config.location\n\n    if model_name.startswith(\"publishers/\"):\n        prediction_resource_name = (\n            f\"projects/{project}/locations/{location}/{model_name}\"\n        )\n    elif model_name.startswith(\"projects/\"):\n        prediction_resource_name = model_name\n    else:\n        raise ValueError(\n            \"model_name must be either a Model Garden model ID or a full resource name.\"\n        )\n\n    location = aiplatform_utils.extract_project_and_location_from_parent(\n        prediction_resource_name\n    )[\"location\"]\n\n    self._model_name = model_name\n    self._prediction_resource_name = prediction_resource_name\n    self._location = location\n    self._generation_config = generation_config\n    self._safety_settings = safety_settings\n    self._tools = tools\n    self._tool_config = tool_config\n    self._system_instruction = system_instruction\n\n    # Validating the parameters\n    self._prepare_request(\n        contents=\"test\",\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n        tools=tools,\n        tool_config=tool_config,\n        system_instruction=system_instruction,\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerativeModel.generate_content","title":"generate_content","text":"<pre><code>generate_content(\n    contents: ContentsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    tool_config: Optional[ToolConfig] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, Iterable[GenerationResponse]\n]\n</code></pre> <p>Generates content.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>tool_config</code> <p>Config shared for all tools provided in the request.</p> <p> TYPE: <code>Optional[ToolConfig]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A stream of GenerationResponse objects if stream == True</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def generate_content(\n    self,\n    contents: ContentsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    tool_config: Optional[\"ToolConfig\"] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"],]:\n    \"\"\"Generates content.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        tool_config: Config shared for all tools provided in the request.\n        stream: Whether to stream the response.\n\n    Returns:\n        A single GenerationResponse object if stream == False\n        A stream of GenerationResponse objects if stream == True\n    \"\"\"\n    if stream:\n        # TODO(b/315810992): Surface prompt_feedback on the returned stream object\n        return self._generate_content_streaming(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n    else:\n        return self._generate_content(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerativeModel.generate_content_async","title":"generate_content_async  <code>async</code>","text":"<pre><code>generate_content_async(\n    contents: ContentsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    tool_config: Optional[ToolConfig] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, AsyncIterable[GenerationResponse]\n]\n</code></pre> <p>Generates content asynchronously.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>tool_config</code> <p>Config shared for all tools provided in the request.</p> <p> TYPE: <code>Optional[ToolConfig]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, AsyncIterable[GenerationResponse]]</code> <p>An awaitable for a single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, AsyncIterable[GenerationResponse]]</code> <p>An awaitable for a stream of GenerationResponse objects if stream == True</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>async def generate_content_async(\n    self,\n    contents: ContentsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    tool_config: Optional[\"ToolConfig\"] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", AsyncIterable[\"GenerationResponse\"],]:\n    \"\"\"Generates content asynchronously.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        tool_config: Config shared for all tools provided in the request.\n        stream: Whether to stream the response.\n\n    Returns:\n        An awaitable for a single GenerationResponse object if stream == False\n        An awaitable for a stream of GenerationResponse objects if stream == True\n    \"\"\"\n    if stream:\n        return await self._generate_content_streaming_async(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n    else:\n        return await self._generate_content_async(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerativeModel.count_tokens","title":"count_tokens","text":"<pre><code>count_tokens(contents: ContentsType) -&gt; CountTokensResponse\n</code></pre> <p>Counts tokens.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> RETURNS DESCRIPTION <code>CountTokensResponse</code> <p>A CountTokensResponse object that has the following attributes: total_tokens: The total number of tokens counted across all instances from the request. total_billable_characters: The total number of billable characters counted across all instances from the request.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def count_tokens(\n    self, contents: ContentsType\n) -&gt; gapic_prediction_service_types.CountTokensResponse:\n    \"\"\"Counts tokens.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n\n    Returns:\n        A CountTokensResponse object that has the following attributes:\n            total_tokens: The total number of tokens counted across all instances from the request.\n            total_billable_characters: The total number of billable characters counted across all instances from the request.\n    \"\"\"\n    return self._prediction_client.count_tokens(\n        request=gapic_prediction_service_types.CountTokensRequest(\n            endpoint=self._prediction_resource_name,\n            model=self._prediction_resource_name,\n            contents=self._prepare_request(contents=contents).contents,\n        )\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerativeModel.count_tokens_async","title":"count_tokens_async  <code>async</code>","text":"<pre><code>count_tokens_async(\n    contents: ContentsType,\n) -&gt; CountTokensResponse\n</code></pre> <p>Counts tokens asynchronously.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> RETURNS DESCRIPTION <code>CountTokensResponse</code> <p>And awaitable for a CountTokensResponse object that has the following attributes: total_tokens: The total number of tokens counted across all instances from the request. total_billable_characters: The total number of billable characters counted across all instances from the request.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>async def count_tokens_async(\n    self, contents: ContentsType\n) -&gt; gapic_prediction_service_types.CountTokensResponse:\n    \"\"\"Counts tokens asynchronously.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n\n    Returns:\n        And awaitable for a CountTokensResponse object that has the following attributes:\n            total_tokens: The total number of tokens counted across all instances from the request.\n            total_billable_characters: The total number of billable characters counted across all instances from the request.\n    \"\"\"\n    return await self._prediction_async_client.count_tokens(\n        request=gapic_prediction_service_types.CountTokensRequest(\n            endpoint=self._prediction_resource_name,\n            model=self._prediction_resource_name,\n            contents=self._prepare_request(contents=contents).contents,\n        )\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerativeModel.start_chat","title":"start_chat","text":"<pre><code>start_chat(\n    *,\n    history: Optional[List[Content]] = None,\n    response_validation: bool = True\n) -&gt; ChatSession\n</code></pre> <p>Creates a stateful chat session.</p> PARAMETER DESCRIPTION <code>history</code> <p>Previous history to initialize the chat session.</p> <p> TYPE: <code>Optional[List[Content]]</code> DEFAULT: <code>None</code> </p> <code>response_validation</code> <p>Whether to validate responses before adding them to chat history. By default, <code>send_message</code> will raise error if the request or response is blocked or if the response is incomplete due to going over the max token limit. If set to <code>False</code>, the chat session history will always accumulate the request and response messages even if the reponse if blocked or incomplete. This can result in an unusable chat session state.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ChatSession</code> <p>A ChatSession object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def start_chat(\n    self,\n    *,\n    history: Optional[List[\"Content\"]] = None,\n    response_validation: bool = True,\n) -&gt; \"ChatSession\":\n    \"\"\"Creates a stateful chat session.\n\n    Args:\n        history: Previous history to initialize the chat session.\n        response_validation: Whether to validate responses before adding\n            them to chat history. By default, `send_message` will raise\n            error if the request or response is blocked or if the response\n            is incomplete due to going over the max token limit.\n            If set to `False`, the chat session history will always\n            accumulate the request and response messages even if the\n            reponse if blocked or incomplete. This can result in an unusable\n            chat session state.\n\n    Returns:\n        A ChatSession object.\n    \"\"\"\n    return ChatSession(\n        model=self,\n        history=history,\n        response_validation=response_validation,\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationConfig","title":"GenerationConfig","text":"<pre><code>GenerationConfig(\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None\n)\n</code></pre> <p>Parameters for the generation.</p> PARAMETER DESCRIPTION <code>temperature</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>If specified, top-k sampling will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to generate.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>The maximum number of output tokens to generate per message.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>A list of stop sequences.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>response_mime_type</code> <p>Output response mimetype of the generated candidate text. Supported mimetypes:</p> <ul> <li><code>text/plain</code>: (default) Text output.</li> <li><code>application/json</code>: JSON response in the candidates.</li> </ul> <p>The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>response_schema</code> <p>Output response schema of the genreated candidate text. Only valid when response_mime_type is application/json.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Usage <pre><code>response = model.generate_content(\n    \"Why is sky blue?\",\n    generation_config=GenerationConfig(\n        temperature=0.1,\n        top_p=0.95,\n        top_k=20,\n        candidate_count=1,\n        max_output_tokens=100,\n        stop_sequences=[\"\\n\\n\\n\"],\n    )\n)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None,\n):\n    r\"\"\"Constructs a GenerationConfig object.\n\n    Args:\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n        top_k: If specified, top-k sampling will be used.\n        candidate_count: Number of candidates to generate.\n        max_output_tokens: The maximum number of output tokens to generate per message.\n        stop_sequences: A list of stop sequences.\n        presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n        frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n        response_mime_type: Output response mimetype of the generated\n            candidate text. Supported mimetypes:\n\n            -  ``text/plain``: (default) Text output.\n            -  ``application/json``: JSON response in the candidates.\n\n            The model needs to be prompted to output the appropriate\n            response type, otherwise the behavior is undefined.\n        response_schema: Output response schema of the genreated candidate text. Only valid when\n            response_mime_type is application/json.\n\n    Usage:\n        ```\n        response = model.generate_content(\n            \"Why is sky blue?\",\n            generation_config=GenerationConfig(\n                temperature=0.1,\n                top_p=0.95,\n                top_k=20,\n                candidate_count=1,\n                max_output_tokens=100,\n                stop_sequences=[\"\\n\\n\\n\"],\n            )\n        )\n        ```\n    \"\"\"\n    if response_schema is None:\n        raw_schema = None\n    else:\n        gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_generation_config = gapic_content_types.GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        candidate_count=candidate_count,\n        max_output_tokens=max_output_tokens,\n        stop_sequences=stop_sequences,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        response_mime_type=response_mime_type,\n        response_schema=raw_schema,\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    generation_config_dict: Dict[str, Any]\n) -&gt; GenerationConfig\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, generation_config_dict: Dict[str, Any]) -&gt; \"GenerationConfig\":\n    raw_generation_config = gapic_content_types.GenerationConfig(\n        generation_config_dict\n    )\n    return cls._from_gapic(raw_generation_config=raw_generation_config)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_generation_config)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse","title":"GenerationResponse","text":"<pre><code>GenerationResponse()\n</code></pre> <p>The response from the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raw_response = gapic_prediction_service_types.GenerateContentResponse()\n    self._raw_response = raw_response\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse.candidates","title":"candidates  <code>property</code>","text":"<pre><code>candidates: List[Candidate]\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse.prompt_feedback","title":"prompt_feedback  <code>property</code>","text":"<pre><code>prompt_feedback: PromptFeedback\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse.usage_metadata","title":"usage_metadata  <code>property</code>","text":"<pre><code>usage_metadata: UsageMetadata\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    response_dict: Dict[str, Any]\n) -&gt; GenerationResponse\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, response_dict: Dict[str, Any]) -&gt; \"GenerationResponse\":\n    raw_response = gapic_prediction_service_types.GenerateContentResponse()\n    json_format.ParseDict(response_dict, raw_response._pb)\n    return cls._from_gapic(raw_response=raw_response)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.GenerationResponse.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_response)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate","title":"Candidate","text":"<pre><code>Candidate()\n</code></pre> <p>A response candidate generated by the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raw_candidate = gapic_content_types.Candidate()\n    self._raw_candidate = raw_candidate\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.content","title":"content  <code>property</code>","text":"<pre><code>content: Content\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.finish_reason","title":"finish_reason  <code>property</code>","text":"<pre><code>finish_reason: FinishReason\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.finish_message","title":"finish_message  <code>property</code>","text":"<pre><code>finish_message: str\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.index","title":"index  <code>property</code>","text":"<pre><code>index: int\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.safety_ratings","title":"safety_ratings  <code>property</code>","text":"<pre><code>safety_ratings: Sequence[SafetyRating]\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.citation_metadata","title":"citation_metadata  <code>property</code>","text":"<pre><code>citation_metadata: CitationMetadata\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.grounding_metadata","title":"grounding_metadata  <code>property</code>","text":"<pre><code>grounding_metadata: GroundingMetadata\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.function_calls","title":"function_calls  <code>property</code>","text":"<pre><code>function_calls: Sequence[FunctionCall]\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(candidate_dict: Dict[str, Any]) -&gt; Candidate\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, candidate_dict: Dict[str, Any]) -&gt; \"Candidate\":\n    raw_candidate = gapic_content_types.Candidate()\n    json_format.ParseDict(candidate_dict, raw_candidate._pb)\n    return cls._from_gapic(raw_candidate=raw_candidate)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Candidate.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_candidate)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ChatSession","title":"ChatSession","text":"<pre><code>ChatSession(\n    model: _GenerativeModel,\n    *,\n    history: Optional[List[Content]] = None,\n    response_validation: bool = True\n)\n</code></pre> <p>Chat session holds the chat history.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    model: _GenerativeModel,\n    *,\n    history: Optional[List[\"Content\"]] = None,\n    response_validation: bool = True,\n):\n    if history:\n        if not all(isinstance(item, Content) for item in history):\n            raise ValueError(\"history must be a list of Content objects.\")\n\n    self._model = model\n    self._history = history or []\n    self._response_validator = _validate_response if response_validation else None\n    # _responder is currently only set by PreviewChatSession\n    self._responder: Optional[\"AutomaticFunctionCallingResponder\"] = None\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ChatSession.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Content]\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, Iterable[GenerationResponse]\n]\n</code></pre> <p>Generates content.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"]]:\n    \"\"\"Generates content.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        A single GenerationResponse object if stream == False\n        A stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ChatSession.send_message_async","title":"send_message_async","text":"<pre><code>send_message_async(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    Awaitable[GenerationResponse],\n    Awaitable[AsyncIterable[GenerationResponse]],\n]\n</code></pre> <p>Generates content asynchronously.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a single GenerationResponse object if stream == False</p> <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message_async(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\n    Awaitable[\"GenerationResponse\"],\n    Awaitable[AsyncIterable[\"GenerationResponse\"]],\n]:\n    \"\"\"Generates content asynchronously.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        An awaitable for a single GenerationResponse object if stream == False\n        An awaitable for a stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Content","title":"Content","text":"<pre><code>Content(\n    *, parts: List[Part] = None, role: Optional[str] = None\n)\n</code></pre> <p>The multi-part content of a message.</p> Usage <pre><code>response = model.generate_content(contents=[\n    Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n])\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    parts: List[\"Part\"] = None,\n    role: Optional[str] = None,\n):\n    raw_parts = [part._raw_part for part in parts or []]\n    self._raw_content = gapic_content_types.Content(parts=raw_parts, role=role)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Content.parts","title":"parts  <code>property</code>","text":"<pre><code>parts: List[Part]\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Content.role","title":"role  <code>property</code> <code>writable</code>","text":"<pre><code>role: str\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Content.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Content.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(content_dict: Dict[str, Any]) -&gt; Content\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, content_dict: Dict[str, Any]) -&gt; \"Content\":\n    raw_content = gapic_content_types.Content()\n    json_format.ParseDict(content_dict, raw_content._pb)\n    return cls._from_gapic(raw_content=raw_content)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Content.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_content)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.FunctionDeclaration","title":"FunctionDeclaration","text":"<pre><code>FunctionDeclaration(\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None\n)\n</code></pre> <p>A representation of a function declaration.</p> Usage <p>Create function declaration and tool: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"celsius\",\n                    \"fahrenheit\",\n                ]\n            }\n        },\n        \"required\": [\n            \"location\"\n        ]\n    },\n)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the function that the model can call.</p> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Describes the parameters to this function in JSON Schema Object format.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>description</code> <p>Description and purpose of the function. Model uses it to decide how and whether to call the function.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None,\n):\n    \"\"\"Constructs a FunctionDeclaration.\n\n    Args:\n        name: The name of the function that the model can call.\n        parameters: Describes the parameters to this function in JSON Schema Object format.\n        description: Description and purpose of the function.\n            Model uses it to decide how and whether to call the function.\n    \"\"\"\n    gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n    raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n        name=name, description=description, parameters=raw_schema\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.FunctionDeclaration.from_func","title":"from_func  <code>classmethod</code>","text":"<pre><code>from_func(\n    func: Callable[..., Any]\n) -&gt; CallableFunctionDeclaration\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n    return CallableFunctionDeclaration.from_func(func)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.FunctionDeclaration.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_function_declaration)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image","title":"Image","text":"<p>The image that can be sent to a generative model.</p>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image.data","title":"data  <code>property</code>","text":"<pre><code>data: bytes\n</code></pre> <p>Returns the image data.</p>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from file.\n\n    Args:\n        location: Local path from where to load the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image()\n    image._image_bytes = image_bytes\n    return image\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Image.from_bytes","title":"from_bytes  <code>staticmethod</code>","text":"<pre><code>from_bytes(data: bytes) -&gt; Image\n</code></pre> <p>Loads image from image bytes.</p> PARAMETER DESCRIPTION <code>data</code> <p>Image bytes.</p> <p> TYPE: <code>bytes</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_bytes(data: bytes) -&gt; \"Image\":\n    \"\"\"Loads image from image bytes.\n\n    Args:\n        data: Image bytes.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image = Image()\n    image._image_bytes = data\n    return image\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part","title":"Part","text":"<pre><code>Part()\n</code></pre> <p>A part of a multi-part Content message.</p> Usage <pre><code>text_part = Part.from_text(\"Why is sky blue?\")\nimage_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\nvideo_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\nfunction_response_part = Part.from_function_response(\n    name=\"get_current_weather\",\n    response={\n        \"content\": {\"weather_there\": \"super nice\"},\n    }\n)\n\nresponse1 = model.generate_content([text_part, image_part])\nresponse2 = model.generate_content(video_part)\nresponse3 = chat.send_message(function_response_part)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raw_part = gapic_content_types.Part()\n    self._raw_part = raw_part\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.mime_type","title":"mime_type  <code>property</code>","text":"<pre><code>mime_type: Optional[str]\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.inline_data","title":"inline_data  <code>property</code>","text":"<pre><code>inline_data: Blob\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.file_data","title":"file_data  <code>property</code>","text":"<pre><code>file_data: FileData\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.function_call","title":"function_call  <code>property</code>","text":"<pre><code>function_call: FunctionCall\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.function_response","title":"function_response  <code>property</code>","text":"<pre><code>function_response: FunctionResponse\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(part_dict: Dict[str, Any]) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, part_dict: Dict[str, Any]) -&gt; \"Part\":\n    raw_part = gapic_content_types.Part()\n    json_format.ParseDict(part_dict, raw_part._pb)\n    return cls._from_gapic(raw_part=raw_part)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.from_data","title":"from_data  <code>staticmethod</code>","text":"<pre><code>from_data(data: bytes, mime_type: str) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_data(data: bytes, mime_type: str) -&gt; \"Part\":\n    return Part._from_gapic(\n        raw_part=gapic_content_types.Part(\n            inline_data=gapic_content_types.Blob(data=data, mime_type=mime_type)\n        )\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.from_uri","title":"from_uri  <code>staticmethod</code>","text":"<pre><code>from_uri(uri: str, mime_type: str) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_uri(uri: str, mime_type: str) -&gt; \"Part\":\n    return Part._from_gapic(\n        raw_part=gapic_content_types.Part(\n            file_data=gapic_content_types.FileData(\n                file_uri=uri, mime_type=mime_type\n            )\n        )\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.from_text","title":"from_text  <code>staticmethod</code>","text":"<pre><code>from_text(text: str) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_text(text: str) -&gt; \"Part\":\n    return Part._from_gapic(raw_part=gapic_content_types.Part(text=text))\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.from_image","title":"from_image  <code>staticmethod</code>","text":"<pre><code>from_image(image: Image) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_image(image: \"Image\") -&gt; \"Part\":\n    return Part.from_data(data=image.data, mime_type=image._mime_type)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.from_function_response","title":"from_function_response  <code>staticmethod</code>","text":"<pre><code>from_function_response(\n    name: str, response: Dict[str, Any]\n) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_function_response(name: str, response: Dict[str, Any]) -&gt; \"Part\":\n    return Part._from_gapic(\n        raw_part=gapic_content_types.Part(\n            function_response=gapic_tool_types.FunctionResponse(\n                name=name,\n                response=response,\n            )\n        )\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Part.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_part)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.ResponseValidationError","title":"ResponseValidationError","text":"<pre><code>ResponseValidationError(\n    message: str,\n    request_contents: List[Content],\n    responses: List[GenerationResponse],\n)\n</code></pre> <p>               Bases: <code>ResponseBlockedError</code></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    request_contents: List[\"Content\"],\n    responses: List[\"GenerationResponse\"],\n):\n    super().__init__(message)\n    self.request = request_contents\n    self.responses = responses\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting","title":"SafetySetting","text":"<pre><code>SafetySetting(\n    *,\n    category: HarmCategory,\n    threshold: HarmBlockThreshold,\n    method: Optional[HarmBlockMethod] = None\n)\n</code></pre> <p>Parameters for the generation.</p> PARAMETER DESCRIPTION <code>category</code> <p>Harm category.</p> <p> TYPE: <code>HarmCategory</code> </p> <code>threshold</code> <p>The harm block threshold.</p> <p> TYPE: <code>HarmBlockThreshold</code> </p> <code>method</code> <p>Specify if the threshold is used for probability or severity score. If not specified, the threshold is used for probability score.</p> <p> TYPE: <code>Optional[HarmBlockMethod]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    category: \"SafetySetting.HarmCategory\",\n    threshold: \"SafetySetting.HarmBlockThreshold\",\n    method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n):\n    r\"\"\"Safety settings.\n\n    Args:\n        category: Harm category.\n        threshold: The harm block threshold.\n        method: Specify if the threshold is used for probability or severity\n            score. If not specified, the threshold is used for probability\n            score.\n    \"\"\"\n    self._raw_safety_setting = gapic_content_types.SafetySetting(\n        category=category,\n        threshold=threshold,\n        method=method,\n    )\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting.HarmCategory","title":"HarmCategory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HarmCategory = HarmCategory\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting.HarmBlockMethod","title":"HarmBlockMethod  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HarmBlockMethod = HarmBlockMethod\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting.HarmBlockThreshold","title":"HarmBlockThreshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HarmBlockThreshold = HarmBlockThreshold\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    safety_setting_dict: Dict[str, Any]\n) -&gt; SafetySetting\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, safety_setting_dict: Dict[str, Any]) -&gt; \"SafetySetting\":\n    raw_safety_setting = gapic_content_types.SafetySetting(safety_setting_dict)\n    return cls._from_gapic(raw_safety_setting=raw_safety_setting)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.SafetySetting.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_safety_setting)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Tool","title":"Tool","text":"<pre><code>Tool(function_declarations: List[FunctionDeclaration])\n</code></pre> <p>A collection of functions that the model may use to generate response.</p> Usage <p>Create tool from function declarations: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(...)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    function_declarations: List[\"FunctionDeclaration\"],\n):\n    gapic_function_declarations = [\n        function_declaration._raw_function_declaration\n        for function_declaration in function_declarations\n    ]\n    self._raw_tool = gapic_tool_types.Tool(\n        function_declarations=gapic_function_declarations\n    )\n    callable_functions = {\n        function_declaration._raw_function_declaration.name: function_declaration\n        for function_declaration in function_declarations\n        if isinstance(function_declaration, CallableFunctionDeclaration)\n    }\n    self._callable_functions = callable_functions\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Tool.from_function_declarations","title":"from_function_declarations  <code>classmethod</code>","text":"<pre><code>from_function_declarations(\n    function_declarations: List[FunctionDeclaration],\n) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_function_declarations(\n    cls,\n    function_declarations: List[\"FunctionDeclaration\"],\n) -&gt; \"Tool\":\n    return Tool(function_declarations=function_declarations)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Tool.from_retrieval","title":"from_retrieval  <code>classmethod</code>","text":"<pre><code>from_retrieval(\n    retrieval: Union[Retrieval, Retrieval]\n) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_retrieval(\n    cls,\n    retrieval: Union[\"grounding.Retrieval\", \"rag.Retrieval\"],\n) -&gt; \"Tool\":\n    raw_tool = gapic_tool_types.Tool(retrieval=retrieval._raw_retrieval)\n    return cls._from_gapic(raw_tool=raw_tool)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Tool.from_google_search_retrieval","title":"from_google_search_retrieval  <code>classmethod</code>","text":"<pre><code>from_google_search_retrieval(\n    google_search_retrieval: GoogleSearchRetrieval,\n) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_google_search_retrieval(\n    cls,\n    google_search_retrieval: \"grounding.GoogleSearchRetrieval\",\n) -&gt; \"Tool\":\n    raw_tool = gapic_tool_types.Tool(\n        google_search_retrieval=google_search_retrieval._raw_google_search_retrieval\n    )\n    return cls._from_gapic(raw_tool=raw_tool)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Tool.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(tool_dict: Dict[str, Any]) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, tool_dict: Dict[str, Any]) -&gt; \"Tool\":\n    tool_dict = copy.deepcopy(tool_dict)\n    function_declarations = tool_dict[\"function_declarations\"]\n    for function_declaration in function_declarations:\n        function_declaration[\"parameters\"] = _convert_schema_dict_to_gapic(\n            function_declaration[\"parameters\"]\n        )\n    raw_tool = gapic_tool_types.Tool(tool_dict)\n    return cls._from_gapic(raw_tool=raw_tool)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.Tool.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_tool)\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.grounding","title":"grounding","text":"<pre><code>grounding()\n</code></pre> <p>Grounding namespace.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raise RuntimeError(\"This class must not be instantiated.\")\n</code></pre>"},{"location":"vertexai/generative_models/#vertexai.generative_models.grounding.GoogleSearchRetrieval","title":"GoogleSearchRetrieval","text":"<pre><code>GoogleSearchRetrieval()\n</code></pre> <p>Tool to retrieve public web data for grounding, powered by Google Search.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes a Google Search Retrieval tool.\"\"\"\n    self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval()\n</code></pre>"},{"location":"vertexai/language_models/","title":"Language models","text":""},{"location":"vertexai/language_models/#vertexai.language_models","title":"vertexai.language_models","text":"<p>Classes for working with language models.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatMessage","title":"ChatMessage  <code>dataclass</code>","text":"<pre><code>ChatMessage(\n    __module__=\"vertexai.language_models\",\n    content: str,\n    author: str,\n)\n</code></pre> <p>A chat message.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>Content of the message.</p> <p> TYPE: <code>str</code> </p> <code>author</code> <p>Author of the message.</p> <p> TYPE: <code>str</code> </p>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatMessage.author","title":"author  <code>instance-attribute</code>","text":"<pre><code>author: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatModel","title":"ChatModel","text":"<pre><code>ChatModel(\n    model_id: str, endpoint_name: Optional[str] = None\n)\n</code></pre> <p>               Bases: <code>_ChatModelBase</code>, <code>_TunableChatModelMixin</code>, <code>_RlhfTunableModelMixin</code></p> <p>ChatModel represents a language model that is capable of chat.</p> <p>Examples::</p> <pre><code>chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n\nchat = chat_model.start_chat(\n    context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\n    examples=[\n        InputOutputTextPair(\n            input_text=\"Who do you work for?\",\n            output_text=\"I work for Ned.\",\n        ),\n        InputOutputTextPair(\n            input_text=\"What do I like?\",\n            output_text=\"Ned likes watching movies.\",\n        ),\n    ],\n    temperature=0.3,\n)\n\nchat.send_message(\"Do you know any cool events this weekend?\")\n</code></pre> <p>This constructor should not be called directly. Use <code>LanguageModel.from_pretrained(model_name=...)</code> instead.</p> PARAMETER DESCRIPTION <code>model_id</code> <p>Identifier of a Vertex LLM. Example: \"text-bison@001\"</p> <p> TYPE: <code>str</code> </p> <code>endpoint_name</code> <p>Vertex Endpoint resource name for the model</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def __init__(self, model_id: str, endpoint_name: Optional[str] = None):\n    \"\"\"Creates a LanguageModel.\n\n    This constructor should not be called directly.\n    Use `LanguageModel.from_pretrained(model_name=...)` instead.\n\n    Args:\n        model_id: Identifier of a Vertex LLM. Example: \"text-bison@001\"\n        endpoint_name: Vertex Endpoint resource name for the model\n    \"\"\"\n\n    super().__init__(\n        model_id=model_id,\n        endpoint_name=endpoint_name,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatModel.list_tuned_model_names","title":"list_tuned_model_names","text":"<pre><code>list_tuned_model_names() -&gt; Sequence[str]\n</code></pre> <p>Lists the names of tuned models.</p> RETURNS DESCRIPTION <code>Sequence[str]</code> <p>A list of tuned models that can be used with the <code>get_tuned_model</code> method.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def list_tuned_model_names(self) -&gt; Sequence[str]:\n    \"\"\"Lists the names of tuned models.\n\n    Returns:\n        A list of tuned models that can be used with the `get_tuned_model` method.\n    \"\"\"\n    model_info = _model_garden_models._get_model_info(\n        model_id=self._model_id,\n        schema_to_class_map={self._INSTANCE_SCHEMA_URI: type(self)},\n    )\n    return _list_tuned_model_names(model_id=model_info.tuning_model_id)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatModel.get_tuned_model","title":"get_tuned_model  <code>classmethod</code>","text":"<pre><code>get_tuned_model(tuned_model_name: str) -&gt; _LanguageModel\n</code></pre> <p>Loads the specified tuned language model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@classmethod\ndef get_tuned_model(cls, tuned_model_name: str) -&gt; \"_LanguageModel\":\n    \"\"\"Loads the specified tuned language model.\"\"\"\n\n    tuned_vertex_model = aiplatform.Model(tuned_model_name)\n    tuned_model_labels = tuned_vertex_model.labels\n\n    if _TUNING_BASE_MODEL_ID_LABEL_KEY not in tuned_model_labels:\n        raise ValueError(\n            f\"The provided model {tuned_model_name} does not have a base model ID.\"\n        )\n\n    tuning_model_id = tuned_vertex_model.labels[_TUNING_BASE_MODEL_ID_LABEL_KEY]\n\n    tuned_model_deployments = tuned_vertex_model.gca_resource.deployed_models\n    if len(tuned_model_deployments) == 0:\n        # Deploying the model\n        endpoint_name = tuned_vertex_model.deploy().resource_name\n    else:\n        endpoint_name = tuned_model_deployments[0].endpoint\n\n    base_model_id = _get_model_id_from_tuning_model_id(tuning_model_id)\n    model_info = _model_garden_models._get_model_info(\n        model_id=base_model_id,\n        schema_to_class_map={cls._INSTANCE_SCHEMA_URI: cls},\n    )\n    model = model_info.interface_class(\n        model_id=base_model_id,\n        endpoint_name=endpoint_name,\n    )\n    return model\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatModel.tune_model_rlhf","title":"tune_model_rlhf","text":"<pre><code>tune_model_rlhf(\n    *,\n    prompt_data: Union[str, DataFrame],\n    preference_data: Union[str, DataFrame],\n    model_display_name: Optional[str] = None,\n    prompt_sequence_length: Optional[int] = None,\n    target_sequence_length: Optional[int] = None,\n    reward_model_learning_rate_multiplier: Optional[\n        float\n    ] = None,\n    reinforcement_learning_rate_multiplier: Optional[\n        float\n    ] = None,\n    reward_model_train_steps: Optional[int] = None,\n    reinforcement_learning_train_steps: Optional[\n        int\n    ] = None,\n    kl_coeff: Optional[float] = None,\n    default_context: Optional[str] = None,\n    tuning_job_location: Optional[str] = None,\n    accelerator_type: Optional[\n        _ACCELERATOR_TYPE_TYPE\n    ] = None,\n    tuning_evaluation_spec: Optional[\n        TuningEvaluationSpec\n    ] = None\n) -&gt; _LanguageModelTuningJob\n</code></pre> <p>Tunes a model using reinforcement learning from human feedback.</p> <p>This method launches and returns an asynchronous model tuning job. Usage: <pre><code>tuning_job = model.tune_model_rlhf(...)\n... do some other work\ntuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n</code></pre></p> PARAMETER DESCRIPTION <code>prompt_data</code> <p>A Pandas DataFrame or a URI pointing to data in JSON lines format. The dataset schema is model-specific. See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset</p> <p> TYPE: <code>Union[str, DataFrame]</code> </p> <code>preference_data</code> <p>A Pandas DataFrame or a URI pointing to data in JSON lines format. The dataset schema is model-specific. See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset</p> <p> TYPE: <code>Union[str, DataFrame]</code> </p> <code>model_display_name</code> <p>Custom display name for the tuned model. If not provided, a default name will be created.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>prompt_sequence_length</code> <p>Maximum tokenized sequence length for input text. Higher values increase memory overhead. This value should be at most 8192. Default value is 512.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>target_sequence_length</code> <p>Maximum tokenized sequence length for target text. Higher values increase memory overhead. This value should be at most 1024. Default value is 64.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>reward_model_learning_rate_multiplier</code> <p>Constant used to adjust the base learning rate used when training a reward model. Multiply by a number &gt; 1 to increase the magnitude of updates applied at each training step or multiply by a number &lt; 1 to decrease the magnitude of updates. Default value is 1.0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>reinforcement_learning_rate_multiplier</code> <p>Constant used to adjust the base learning rate used during reinforcement learning. Multiply by a number &gt; 1 to increase the magnitude of updates applied at each training step or multiply by a number &lt; 1 to decrease the magnitude of updates. Default value is 1.0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>reward_model_train_steps</code> <p>Number of steps to use when training a reward model. Default value is 1000.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>reinforcement_learning_train_steps</code> <p>Number of reinforcement learning steps to perform when tuning a base model. Default value is 1000.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>kl_coeff</code> <p>Coefficient for KL penalty. This regularizes the policy model and penalizes if it diverges from its initial distribution. If set to 0, the reference language model is not loaded into memory. Default value is 0.1.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>default_context</code> <p>This field lets the model know what task to perform. Base models have been trained over a large set of varied instructions. You can give a simple and intuitive description of the task and the model will follow it, e.g. \"Classify this movie review as positive or negative\" or \"Translate this sentence to Danish\". Do not specify this if your dataset already prepends the instruction to the inputs field.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tuning_job_location</code> <p>GCP location where the tuning job should be run.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>accelerator_type</code> <p>Type of accelerator to use. Can be \"TPU\" or \"GPU\".</p> <p> TYPE: <code>Optional[_ACCELERATOR_TYPE_TYPE]</code> DEFAULT: <code>None</code> </p> <code>tuning_evaluation_spec</code> <p>Evaluation settings to use during tuning.</p> <p> TYPE: <code>Optional[TuningEvaluationSpec]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>_LanguageModelTuningJob</code> <p>A <code>LanguageModelTuningJob</code> object that represents the tuning job.</p> <code>_LanguageModelTuningJob</code> <p>Calling <code>job.result()</code> blocks until the tuning is complete and returns a <code>LanguageModel</code> object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the \"tuning_job_location\" value is not supported</p> <code>RuntimeError</code> <p>If the model does not support tuning</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def tune_model_rlhf(\n    self,\n    *,\n    prompt_data: Union[str, \"pandas.core.frame.DataFrame\"],\n    preference_data: Union[str, \"pandas.core.frame.DataFrame\"],\n    model_display_name: Optional[str] = None,\n    prompt_sequence_length: Optional[int] = None,\n    target_sequence_length: Optional[int] = None,\n    reward_model_learning_rate_multiplier: Optional[float] = None,\n    reinforcement_learning_rate_multiplier: Optional[float] = None,\n    reward_model_train_steps: Optional[int] = None,\n    reinforcement_learning_train_steps: Optional[int] = None,\n    kl_coeff: Optional[float] = None,\n    default_context: Optional[str] = None,\n    tuning_job_location: Optional[str] = None,\n    accelerator_type: Optional[_ACCELERATOR_TYPE_TYPE] = None,\n    tuning_evaluation_spec: Optional[\"TuningEvaluationSpec\"] = None,\n) -&gt; \"_LanguageModelTuningJob\":\n    \"\"\"Tunes a model using reinforcement learning from human feedback.\n\n    This method launches and returns an asynchronous model tuning job.\n    Usage:\n    ```\n    tuning_job = model.tune_model_rlhf(...)\n    ... do some other work\n    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n    ```\n\n    Args:\n        prompt_data: A Pandas DataFrame or a URI pointing to data in JSON lines\n            format. The dataset schema is model-specific.\n            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset\n        preference_data: A Pandas DataFrame or a URI pointing to data in JSON lines\n            format. The dataset schema is model-specific.\n            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset\n        model_display_name: Custom display name for the tuned model.\n            If not provided, a default name will be created.\n        prompt_sequence_length: Maximum tokenized sequence length for input text.\n            Higher values increase memory overhead.\n            This value should be at most 8192. Default value is 512.\n        target_sequence_length: Maximum tokenized sequence length for target text.\n            Higher values increase memory overhead.\n            This value should be at most 1024. Default value is 64.\n        reward_model_learning_rate_multiplier: Constant used to adjust the base\n            learning rate used when training a reward model. Multiply by a\n            number &gt; 1 to increase the magnitude of updates applied at each\n            training step or multiply by a number &lt; 1 to decrease the magnitude\n            of updates. Default value is 1.0.\n        reinforcement_learning_rate_multiplier: Constant used to adjust the base\n            learning rate used during reinforcement learning. Multiply by a\n            number &gt; 1 to increase the magnitude of updates applied at each\n            training step or multiply by a number &lt; 1 to decrease the magnitude\n            of updates. Default value is 1.0.\n        reward_model_train_steps: Number of steps to use when training a reward\n            model. Default value is 1000.\n        reinforcement_learning_train_steps: Number of reinforcement learning steps\n            to perform when tuning a base model. Default value is 1000.\n        kl_coeff: Coefficient for KL penalty. This regularizes the policy model and\n            penalizes if it diverges from its initial distribution. If set to 0,\n            the reference language model is not loaded into memory. Default value\n            is 0.1.\n        default_context: This field lets the model know what task to perform.\n            Base models have been trained over a large set of varied instructions.\n            You can give a simple and intuitive description of the task and the\n            model will follow it, e.g. \"Classify this movie review as positive or\n            negative\" or \"Translate this sentence to Danish\". Do not specify this\n            if your dataset already prepends the instruction to the inputs field.\n        tuning_job_location: GCP location where the tuning job should be run.\n        accelerator_type: Type of accelerator to use. Can be \"TPU\" or \"GPU\".\n        tuning_evaluation_spec: Evaluation settings to use during tuning.\n\n    Returns:\n        A `LanguageModelTuningJob` object that represents the tuning job.\n        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.\n\n    Raises:\n        ValueError: If the \"tuning_job_location\" value is not supported\n        RuntimeError: If the model does not support tuning\n    \"\"\"\n    tuning_job_location = (\n        tuning_job_location or aiplatform_initializer.global_config.location\n    )\n    eval_dataset = None\n    tensorboard_resource_id = None\n    if tuning_evaluation_spec is not None:\n        _check_unused_rlhf_eval_specs(tuning_evaluation_spec)\n\n        eval_dataset = tuning_evaluation_spec.evaluation_data\n        if eval_dataset is not None and not eval_dataset.startswith(\"gs://\"):\n            raise ValueError(\n                \"evaluation_data must be a GCS URI that starts with gs://\"\n            )\n\n        tensorboard_resource_id = _get_tensorboard_resource_id_from_evaluation_spec(\n            tuning_evaluation_spec, tuning_job_location\n        )\n    prompt_dataset_uri = _maybe_upload_training_data(\n        training_data=prompt_data,\n        model_id=self._model_id,\n    )\n    preference_dataset_uri = _maybe_upload_training_data(\n        training_data=preference_data,\n        model_id=self._model_id,\n    )\n\n    if accelerator_type:\n        if accelerator_type not in _ACCELERATOR_TYPES:\n            raise ValueError(\n                f\"Unsupported accelerator type: {accelerator_type}.\"\n                f\" Supported types: {_ACCELERATOR_TYPES}\"\n            )\n\n    tuning_parameters = _RlhfTuningParameters(\n        prompt_dataset=prompt_dataset_uri,\n        preference_dataset=preference_dataset_uri,\n        large_model_reference=self._model_id.rsplit(\"/\", 1)[-1],\n        location=tuning_job_location,\n        model_display_name=model_display_name,\n        prompt_sequence_length=prompt_sequence_length,\n        target_sequence_length=target_sequence_length,\n        reward_model_learning_rate_multiplier=reward_model_learning_rate_multiplier,\n        reinforcement_learning_rate_multiplier=reinforcement_learning_rate_multiplier,\n        reward_model_train_steps=reward_model_train_steps,\n        reinforcement_learning_train_steps=reinforcement_learning_train_steps,\n        kl_coeff=kl_coeff,\n        instruction=default_context,\n        eval_dataset=eval_dataset,\n        accelerator_type=accelerator_type,\n        tensorboard_resource_id=tensorboard_resource_id,\n    )\n\n    return self._tune_model_rlhf(\n        tuning_parameters=tuning_parameters,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatModel.tune_model","title":"tune_model","text":"<pre><code>tune_model(\n    training_data: Union[str, DataFrame],\n    *,\n    train_steps: Optional[int] = None,\n    learning_rate_multiplier: Optional[float] = None,\n    tuning_job_location: Optional[str] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    default_context: Optional[str] = None,\n    accelerator_type: Optional[\n        _ACCELERATOR_TYPE_TYPE\n    ] = None,\n    tuning_evaluation_spec: Optional[\n        TuningEvaluationSpec\n    ] = None\n) -&gt; _LanguageModelTuningJob\n</code></pre> <p>Tunes a model based on training data.</p> <p>This method launches and returns an asynchronous model tuning job. Usage: <pre><code>tuning_job = model.tune_model(...)\n... do some other work\ntuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n</code></pre></p> PARAMETER DESCRIPTION <code>training_data</code> <p>A Pandas DataFrame or a URI pointing to data in JSON lines format. The dataset schema is model-specific. See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</p> <p> TYPE: <code>Union[str, DataFrame]</code> </p> <code>train_steps</code> <p>Number of training batches to tune on (batch size is 8 samples).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>learning_rate</code> <p>Deprecated. Use learning_rate_multiplier instead. Learning rate to use in tuning.</p> <p> </p> <code>learning_rate_multiplier</code> <p>Learning rate multiplier to use in tuning.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>tuning_job_location</code> <p>GCP location where the tuning job should be run.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tuned_model_location</code> <p>GCP location where the tuned model should be deployed.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>model_display_name</code> <p>Custom display name for the tuned model.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>default_context</code> <p>The context to use for all training samples by default.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>accelerator_type</code> <p>Type of accelerator to use. Can be \"TPU\" or \"GPU\".</p> <p> TYPE: <code>Optional[_ACCELERATOR_TYPE_TYPE]</code> DEFAULT: <code>None</code> </p> <code>tuning_evaluation_spec</code> <p>Specification for the model evaluation during tuning.</p> <p> TYPE: <code>Optional[TuningEvaluationSpec]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>_LanguageModelTuningJob</code> <p>A <code>LanguageModelTuningJob</code> object that represents the tuning job.</p> <code>_LanguageModelTuningJob</code> <p>Calling <code>job.result()</code> blocks until the tuning is complete and returns a <code>LanguageModel</code> object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the \"tuning_job_location\" value is not supported</p> <code>ValueError</code> <p>If the \"tuned_model_location\" value is not supported</p> <code>RuntimeError</code> <p>If the model does not support tuning</p> <code>AttributeError</code> <p>If any attribute in the \"tuning_evaluation_spec\" is not supported</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def tune_model(\n    self,\n    training_data: Union[str, \"pandas.core.frame.DataFrame\"],\n    *,\n    train_steps: Optional[int] = None,\n    learning_rate_multiplier: Optional[float] = None,\n    tuning_job_location: Optional[str] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    default_context: Optional[str] = None,\n    accelerator_type: Optional[_ACCELERATOR_TYPE_TYPE] = None,\n    tuning_evaluation_spec: Optional[\"TuningEvaluationSpec\"] = None,\n) -&gt; \"_LanguageModelTuningJob\":\n    \"\"\"Tunes a model based on training data.\n\n    This method launches and returns an asynchronous model tuning job.\n    Usage:\n    ```\n    tuning_job = model.tune_model(...)\n    ... do some other work\n    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n    ```\n\n    Args:\n        training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.\n            The dataset schema is model-specific.\n            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format\n        train_steps: Number of training batches to tune on (batch size is 8 samples).\n        learning_rate: Deprecated. Use learning_rate_multiplier instead.\n            Learning rate to use in tuning.\n        learning_rate_multiplier: Learning rate multiplier to use in tuning.\n        tuning_job_location: GCP location where the tuning job should be run.\n        tuned_model_location: GCP location where the tuned model should be deployed.\n        model_display_name: Custom display name for the tuned model.\n        default_context: The context to use for all training samples by default.\n        accelerator_type: Type of accelerator to use. Can be \"TPU\" or \"GPU\".\n        tuning_evaluation_spec: Specification for the model evaluation during tuning.\n\n    Returns:\n        A `LanguageModelTuningJob` object that represents the tuning job.\n        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.\n\n    Raises:\n        ValueError: If the \"tuning_job_location\" value is not supported\n        ValueError: If the \"tuned_model_location\" value is not supported\n        RuntimeError: If the model does not support tuning\n        AttributeError: If any attribute in the \"tuning_evaluation_spec\" is not supported\n    \"\"\"\n\n    if tuning_evaluation_spec is not None:\n        unsupported_chat_model_tuning_eval_spec = {\n            \"evaluation_data\": tuning_evaluation_spec.evaluation_data,\n            \"evaluation_interval\": tuning_evaluation_spec.evaluation_interval,\n            \"enable_early_stopping\": tuning_evaluation_spec.enable_early_stopping,\n            \"enable_checkpoint_selection\": tuning_evaluation_spec.enable_checkpoint_selection,\n        }\n\n        for att_name, att_value in unsupported_chat_model_tuning_eval_spec.items():\n            if att_value is not None:\n                raise AttributeError(\n                    (\n                        f\"ChatModel and CodeChatModel only support tensorboard as attribute for TuningEvaluationSpec\"\n                        f\"found attribute name {att_name} with value {att_value}, please leave {att_name} to None\"\n                    )\n                )\n    return super().tune_model(\n        training_data=training_data,\n        train_steps=train_steps,\n        learning_rate_multiplier=learning_rate_multiplier,\n        tuning_job_location=tuning_job_location,\n        tuned_model_location=tuned_model_location,\n        model_display_name=model_display_name,\n        default_context=default_context,\n        accelerator_type=accelerator_type,\n        tuning_evaluation_spec=tuning_evaluation_spec,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatModel.start_chat","title":"start_chat","text":"<pre><code>start_chat(\n    *,\n    context: Optional[str] = None,\n    examples: Optional[List[InputOutputTextPair]] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; ChatSession\n</code></pre> <p>Starts a chat session with the model.</p> PARAMETER DESCRIPTION <code>context</code> <p>Context shapes how the model responds throughout the conversation. For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>examples</code> <p>List of structured messages to the model to learn how to respond to the conversation. A list of <code>InputOutputTextPair</code> objects.</p> <p> TYPE: <code>Optional[List[InputOutputTextPair]]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024].</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>message_history</code> <p>A list of previously sent and received messages.</p> <p> TYPE: <code>Optional[List[ChatMessage]]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ChatSession</code> <p>A <code>ChatSession</code> object.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def start_chat(\n    self,\n    *,\n    context: Optional[str] = None,\n    examples: Optional[List[InputOutputTextPair]] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; \"ChatSession\":\n    \"\"\"Starts a chat session with the model.\n\n    Args:\n        context: Context shapes how the model responds throughout the conversation.\n            For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style\n        examples: List of structured messages to the model to learn how to respond to the conversation.\n            A list of `InputOutputTextPair` objects.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n        message_history: A list of previously sent and received messages.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n\n    Returns:\n        A `ChatSession` object.\n    \"\"\"\n    return ChatSession(\n        model=self,\n        context=context,\n        examples=examples,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        message_history=message_history,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession","title":"ChatSession","text":"<pre><code>ChatSession(\n    model: ChatModel,\n    context: Optional[str] = None,\n    examples: Optional[List[InputOutputTextPair]] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n)\n</code></pre> <p>               Bases: <code>_ChatSessionBase</code></p> <p>ChatSession represents a chat session with a language model.</p> <p>Within a chat session, the model keeps context and remembers the previous conversation.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def __init__(\n    self,\n    model: ChatModel,\n    context: Optional[str] = None,\n    examples: Optional[List[InputOutputTextPair]] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n):\n    super().__init__(\n        model=model,\n        context=context,\n        examples=examples,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        message_history=message_history,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession.USER_AUTHOR","title":"USER_AUTHOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USER_AUTHOR = 'user'\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession.MODEL_AUTHOR","title":"MODEL_AUTHOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL_AUTHOR = 'bot'\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession.message_history","title":"message_history  <code>property</code>","text":"<pre><code>message_history: List[ChatMessage]\n</code></pre> <p>List of previous messages.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[WebSearch, VertexAISearch, InlineContext]\n    ] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Sends message to the language model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>grounding_source</code> <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p> <p> TYPE: <code>Optional[Union[WebSearch, VertexAISearch, InlineContext]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the</p> <code>MultiCandidateTextGenerationResponse</code> <p>text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[\n            GroundingSource.WebSearch,\n            GroundingSource.VertexAISearch,\n            GroundingSource.InlineContext,\n        ]\n    ] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Sends message to the language model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        candidate_count: Number of candidates to return.\n        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the\n        text produced by the model.\n    \"\"\"\n    prediction_request = self._prepare_request(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n        candidate_count=candidate_count,\n        grounding_source=grounding_source,\n    )\n\n    prediction_response = self._model._endpoint.predict(\n        instances=[prediction_request.instance],\n        parameters=prediction_request.parameters,\n    )\n    response_obj = self._parse_chat_prediction_response(\n        prediction_response=prediction_response\n    )\n    response_text = response_obj.text\n\n    self._message_history.append(\n        ChatMessage(content=message, author=self.USER_AUTHOR)\n    )\n    self._message_history.append(\n        ChatMessage(content=response_text, author=self.MODEL_AUTHOR)\n    )\n\n    return response_obj\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession.send_message_async","title":"send_message_async  <code>async</code>","text":"<pre><code>send_message_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[WebSearch, VertexAISearch, InlineContext]\n    ] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Asynchronously sends message to the language model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>grounding_source</code> <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p> <p> TYPE: <code>Optional[Union[WebSearch, VertexAISearch, InlineContext]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains</p> <code>MultiCandidateTextGenerationResponse</code> <p>the text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def send_message_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[\n            GroundingSource.WebSearch,\n            GroundingSource.VertexAISearch,\n            GroundingSource.InlineContext,\n        ]\n    ] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Asynchronously sends message to the language model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        candidate_count: Number of candidates to return.\n        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains\n        the text produced by the model.\n    \"\"\"\n    prediction_request = self._prepare_request(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n        candidate_count=candidate_count,\n        grounding_source=grounding_source,\n    )\n\n    prediction_response = await self._model._endpoint.predict_async(\n        instances=[prediction_request.instance],\n        parameters=prediction_request.parameters,\n    )\n    response_obj = self._parse_chat_prediction_response(\n        prediction_response=prediction_response\n    )\n    response_text = response_obj.text\n\n    self._message_history.append(\n        ChatMessage(content=message, author=self.USER_AUTHOR)\n    )\n    self._message_history.append(\n        ChatMessage(content=response_text, author=self.MODEL_AUTHOR)\n    )\n\n    return response_obj\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession.send_message_streaming","title":"send_message_streaming","text":"<pre><code>send_message_streaming(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; Iterator[TextGenerationResponse]\n</code></pre> <p>Sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>TextGenerationResponse</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>TextGenerationResponse</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message_streaming(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; Iterator[TextGenerationResponse]:\n    \"\"\"Sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Yields:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    prediction_request = self._prepare_request(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n    )\n\n    prediction_service_client = self._model._endpoint._prediction_client\n\n    full_response_text = \"\"\n\n    for (\n        prediction_dict\n    ) in _streaming_prediction.predict_stream_of_dicts_from_single_dict(\n        prediction_service_client=prediction_service_client,\n        endpoint_name=self._model._endpoint_name,\n        instance=prediction_request.instance,\n        parameters=prediction_request.parameters,\n    ):\n        prediction_response = aiplatform.models.Prediction(\n            predictions=[prediction_dict],\n            deployed_model_id=\"\",\n        )\n        text_generation_response = self._parse_chat_prediction_response(\n            prediction_response=prediction_response\n        )\n        full_response_text += text_generation_response.text\n        yield text_generation_response\n\n    # We only add the question and answer to the history if/when the answer\n    # was read fully. Otherwise, the answer would have been truncated.\n    self._message_history.append(\n        ChatMessage(content=message, author=self.USER_AUTHOR)\n    )\n    self._message_history.append(\n        ChatMessage(content=full_response_text, author=self.MODEL_AUTHOR)\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.ChatSession.send_message_streaming_async","title":"send_message_streaming_async  <code>async</code>","text":"<pre><code>send_message_streaming_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; AsyncIterator[TextGenerationResponse]\n</code></pre> <p>Asynchronously sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>AsyncIterator[TextGenerationResponse]</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>AsyncIterator[TextGenerationResponse]</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def send_message_streaming_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; AsyncIterator[TextGenerationResponse]:\n    \"\"\"Asynchronously sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Yields:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    prediction_request = self._prepare_request(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n    )\n\n    prediction_service_async_client = self._model._endpoint._prediction_async_client\n\n    full_response_text = \"\"\n\n    async for prediction_dict in _streaming_prediction.predict_stream_of_dicts_from_single_dict_async(\n        prediction_service_async_client=prediction_service_async_client,\n        endpoint_name=self._model._endpoint_name,\n        instance=prediction_request.instance,\n        parameters=prediction_request.parameters,\n    ):\n        prediction_response = aiplatform.models.Prediction(\n            predictions=[prediction_dict],\n            deployed_model_id=\"\",\n        )\n        text_generation_response = self._parse_chat_prediction_response(\n            prediction_response=prediction_response\n        )\n        full_response_text += text_generation_response.text\n        yield text_generation_response\n\n    # We only add the question and answer to the history if/when the answer\n    # was read fully. Otherwise, the answer would have been truncated.\n    self._message_history.append(\n        ChatMessage(content=message, author=self.USER_AUTHOR)\n    )\n    self._message_history.append(\n        ChatMessage(content=full_response_text, author=self.MODEL_AUTHOR)\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatModel","title":"CodeChatModel","text":"<pre><code>CodeChatModel(\n    model_id: str, endpoint_name: Optional[str] = None\n)\n</code></pre> <p>               Bases: <code>_ChatModelBase</code>, <code>_TunableChatModelMixin</code></p> <p>CodeChatModel represents a model that is capable of completing code.</p> <p>Examples:</p> <p>code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")</p> <p>code_chat = code_chat_model.start_chat(     context=\"I'm writing a large-scale enterprise application.\",     max_output_tokens=128,     temperature=0.2, )</p> <p>code_chat.send_message(\"Please help write a function to calculate the min of two numbers\")</p> <p>This constructor should not be called directly. Use <code>LanguageModel.from_pretrained(model_name=...)</code> instead.</p> PARAMETER DESCRIPTION <code>model_id</code> <p>Identifier of a Vertex LLM. Example: \"text-bison@001\"</p> <p> TYPE: <code>str</code> </p> <code>endpoint_name</code> <p>Vertex Endpoint resource name for the model</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def __init__(self, model_id: str, endpoint_name: Optional[str] = None):\n    \"\"\"Creates a LanguageModel.\n\n    This constructor should not be called directly.\n    Use `LanguageModel.from_pretrained(model_name=...)` instead.\n\n    Args:\n        model_id: Identifier of a Vertex LLM. Example: \"text-bison@001\"\n        endpoint_name: Vertex Endpoint resource name for the model\n    \"\"\"\n\n    super().__init__(\n        model_id=model_id,\n        endpoint_name=endpoint_name,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatModel.list_tuned_model_names","title":"list_tuned_model_names","text":"<pre><code>list_tuned_model_names() -&gt; Sequence[str]\n</code></pre> <p>Lists the names of tuned models.</p> RETURNS DESCRIPTION <code>Sequence[str]</code> <p>A list of tuned models that can be used with the <code>get_tuned_model</code> method.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def list_tuned_model_names(self) -&gt; Sequence[str]:\n    \"\"\"Lists the names of tuned models.\n\n    Returns:\n        A list of tuned models that can be used with the `get_tuned_model` method.\n    \"\"\"\n    model_info = _model_garden_models._get_model_info(\n        model_id=self._model_id,\n        schema_to_class_map={self._INSTANCE_SCHEMA_URI: type(self)},\n    )\n    return _list_tuned_model_names(model_id=model_info.tuning_model_id)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatModel.get_tuned_model","title":"get_tuned_model  <code>classmethod</code>","text":"<pre><code>get_tuned_model(tuned_model_name: str) -&gt; _LanguageModel\n</code></pre> <p>Loads the specified tuned language model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@classmethod\ndef get_tuned_model(cls, tuned_model_name: str) -&gt; \"_LanguageModel\":\n    \"\"\"Loads the specified tuned language model.\"\"\"\n\n    tuned_vertex_model = aiplatform.Model(tuned_model_name)\n    tuned_model_labels = tuned_vertex_model.labels\n\n    if _TUNING_BASE_MODEL_ID_LABEL_KEY not in tuned_model_labels:\n        raise ValueError(\n            f\"The provided model {tuned_model_name} does not have a base model ID.\"\n        )\n\n    tuning_model_id = tuned_vertex_model.labels[_TUNING_BASE_MODEL_ID_LABEL_KEY]\n\n    tuned_model_deployments = tuned_vertex_model.gca_resource.deployed_models\n    if len(tuned_model_deployments) == 0:\n        # Deploying the model\n        endpoint_name = tuned_vertex_model.deploy().resource_name\n    else:\n        endpoint_name = tuned_model_deployments[0].endpoint\n\n    base_model_id = _get_model_id_from_tuning_model_id(tuning_model_id)\n    model_info = _model_garden_models._get_model_info(\n        model_id=base_model_id,\n        schema_to_class_map={cls._INSTANCE_SCHEMA_URI: cls},\n    )\n    model = model_info.interface_class(\n        model_id=base_model_id,\n        endpoint_name=endpoint_name,\n    )\n    return model\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatModel.tune_model","title":"tune_model","text":"<pre><code>tune_model(\n    training_data: Union[str, DataFrame],\n    *,\n    train_steps: Optional[int] = None,\n    learning_rate_multiplier: Optional[float] = None,\n    tuning_job_location: Optional[str] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    default_context: Optional[str] = None,\n    accelerator_type: Optional[\n        _ACCELERATOR_TYPE_TYPE\n    ] = None,\n    tuning_evaluation_spec: Optional[\n        TuningEvaluationSpec\n    ] = None\n) -&gt; _LanguageModelTuningJob\n</code></pre> <p>Tunes a model based on training data.</p> <p>This method launches and returns an asynchronous model tuning job. Usage: <pre><code>tuning_job = model.tune_model(...)\n... do some other work\ntuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n</code></pre></p> PARAMETER DESCRIPTION <code>training_data</code> <p>A Pandas DataFrame or a URI pointing to data in JSON lines format. The dataset schema is model-specific. See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</p> <p> TYPE: <code>Union[str, DataFrame]</code> </p> <code>train_steps</code> <p>Number of training batches to tune on (batch size is 8 samples).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>learning_rate</code> <p>Deprecated. Use learning_rate_multiplier instead. Learning rate to use in tuning.</p> <p> </p> <code>learning_rate_multiplier</code> <p>Learning rate multiplier to use in tuning.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>tuning_job_location</code> <p>GCP location where the tuning job should be run.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tuned_model_location</code> <p>GCP location where the tuned model should be deployed.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>model_display_name</code> <p>Custom display name for the tuned model.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>default_context</code> <p>The context to use for all training samples by default.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>accelerator_type</code> <p>Type of accelerator to use. Can be \"TPU\" or \"GPU\".</p> <p> TYPE: <code>Optional[_ACCELERATOR_TYPE_TYPE]</code> DEFAULT: <code>None</code> </p> <code>tuning_evaluation_spec</code> <p>Specification for the model evaluation during tuning.</p> <p> TYPE: <code>Optional[TuningEvaluationSpec]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>_LanguageModelTuningJob</code> <p>A <code>LanguageModelTuningJob</code> object that represents the tuning job.</p> <code>_LanguageModelTuningJob</code> <p>Calling <code>job.result()</code> blocks until the tuning is complete and returns a <code>LanguageModel</code> object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the \"tuning_job_location\" value is not supported</p> <code>ValueError</code> <p>If the \"tuned_model_location\" value is not supported</p> <code>RuntimeError</code> <p>If the model does not support tuning</p> <code>AttributeError</code> <p>If any attribute in the \"tuning_evaluation_spec\" is not supported</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def tune_model(\n    self,\n    training_data: Union[str, \"pandas.core.frame.DataFrame\"],\n    *,\n    train_steps: Optional[int] = None,\n    learning_rate_multiplier: Optional[float] = None,\n    tuning_job_location: Optional[str] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    default_context: Optional[str] = None,\n    accelerator_type: Optional[_ACCELERATOR_TYPE_TYPE] = None,\n    tuning_evaluation_spec: Optional[\"TuningEvaluationSpec\"] = None,\n) -&gt; \"_LanguageModelTuningJob\":\n    \"\"\"Tunes a model based on training data.\n\n    This method launches and returns an asynchronous model tuning job.\n    Usage:\n    ```\n    tuning_job = model.tune_model(...)\n    ... do some other work\n    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n    ```\n\n    Args:\n        training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.\n            The dataset schema is model-specific.\n            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format\n        train_steps: Number of training batches to tune on (batch size is 8 samples).\n        learning_rate: Deprecated. Use learning_rate_multiplier instead.\n            Learning rate to use in tuning.\n        learning_rate_multiplier: Learning rate multiplier to use in tuning.\n        tuning_job_location: GCP location where the tuning job should be run.\n        tuned_model_location: GCP location where the tuned model should be deployed.\n        model_display_name: Custom display name for the tuned model.\n        default_context: The context to use for all training samples by default.\n        accelerator_type: Type of accelerator to use. Can be \"TPU\" or \"GPU\".\n        tuning_evaluation_spec: Specification for the model evaluation during tuning.\n\n    Returns:\n        A `LanguageModelTuningJob` object that represents the tuning job.\n        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.\n\n    Raises:\n        ValueError: If the \"tuning_job_location\" value is not supported\n        ValueError: If the \"tuned_model_location\" value is not supported\n        RuntimeError: If the model does not support tuning\n        AttributeError: If any attribute in the \"tuning_evaluation_spec\" is not supported\n    \"\"\"\n\n    if tuning_evaluation_spec is not None:\n        unsupported_chat_model_tuning_eval_spec = {\n            \"evaluation_data\": tuning_evaluation_spec.evaluation_data,\n            \"evaluation_interval\": tuning_evaluation_spec.evaluation_interval,\n            \"enable_early_stopping\": tuning_evaluation_spec.enable_early_stopping,\n            \"enable_checkpoint_selection\": tuning_evaluation_spec.enable_checkpoint_selection,\n        }\n\n        for att_name, att_value in unsupported_chat_model_tuning_eval_spec.items():\n            if att_value is not None:\n                raise AttributeError(\n                    (\n                        f\"ChatModel and CodeChatModel only support tensorboard as attribute for TuningEvaluationSpec\"\n                        f\"found attribute name {att_name} with value {att_value}, please leave {att_name} to None\"\n                    )\n                )\n    return super().tune_model(\n        training_data=training_data,\n        train_steps=train_steps,\n        learning_rate_multiplier=learning_rate_multiplier,\n        tuning_job_location=tuning_job_location,\n        tuned_model_location=tuned_model_location,\n        model_display_name=model_display_name,\n        default_context=default_context,\n        accelerator_type=accelerator_type,\n        tuning_evaluation_spec=tuning_evaluation_spec,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatModel.start_chat","title":"start_chat","text":"<pre><code>start_chat(\n    *,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; CodeChatSession\n</code></pre> <p>Starts a chat session with the code chat model.</p> PARAMETER DESCRIPTION <code>context</code> <p>Context shapes how the model responds throughout the conversation. For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000].</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CodeChatSession</code> <p>A <code>ChatSession</code> object.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def start_chat(\n    self,\n    *,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; \"CodeChatSession\":\n    \"\"\"Starts a chat session with the code chat model.\n\n    Args:\n        context: Context shapes how the model responds throughout the conversation.\n            For example, you can use context to specify words the model can or\n            cannot use, topics to focus on or avoid, or the response format or style.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n        stop_sequences: Customized stop sequences to stop the decoding process.\n\n    Returns:\n        A `ChatSession` object.\n    \"\"\"\n    return CodeChatSession(\n        model=self,\n        context=context,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        message_history=message_history,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession","title":"CodeChatSession","text":"<pre><code>CodeChatSession(\n    model: CodeChatModel,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n)\n</code></pre> <p>               Bases: <code>_ChatSessionBase</code></p> <p>CodeChatSession represents a chat session with code chat language model.</p> <p>Within a code chat session, the model keeps context and remembers the previous converstion.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def __init__(\n    self,\n    model: CodeChatModel,\n    context: Optional[str] = None,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    message_history: Optional[List[ChatMessage]] = None,\n    stop_sequences: Optional[List[str]] = None,\n):\n    super().__init__(\n        model=model,\n        context=context,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        message_history=message_history,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.USER_AUTHOR","title":"USER_AUTHOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>USER_AUTHOR = 'user'\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.MODEL_AUTHOR","title":"MODEL_AUTHOR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL_AUTHOR = 'bot'\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.message_history","title":"message_history  <code>property</code>","text":"<pre><code>message_history: List[ChatMessage]\n</code></pre> <p>List of previous messages.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Sends message to the code chat model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000]. Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].  Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the</p> <code>MultiCandidateTextGenerationResponse</code> <p>text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Sends message to the code chat model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n             Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        candidate_count: Number of candidates to return.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the\n        text produced by the model.\n    \"\"\"\n    return super().send_message(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n        candidate_count=candidate_count,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message_async","title":"send_message_async  <code>async</code>","text":"<pre><code>send_message_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    candidate_count: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Asynchronously sends message to the code chat model and gets a response.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1000]. Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1].  Uses the value specified when calling <code>CodeChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the</p> <code>MultiCandidateTextGenerationResponse</code> <p>text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def send_message_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    candidate_count: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Asynchronously sends message to the code chat model and gets a response.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].\n            Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1].\n             Uses the value specified when calling `CodeChatModel.start_chat` by default.\n        candidate_count: Number of candidates to return.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the\n        text produced by the model.\n    \"\"\"\n    response = await super().send_message_async(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        candidate_count=candidate_count,\n    )\n    return response\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message_streaming","title":"send_message_streaming","text":"<pre><code>send_message_streaming(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; Iterator[TextGenerationResponse]\n</code></pre> <p>Sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TextGenerationResponse</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>TextGenerationResponse</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message_streaming(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; Iterator[TextGenerationResponse]:\n    \"\"\"Sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Returns:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    return super().send_message_streaming(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeChatSession.send_message_streaming_async","title":"send_message_streaming_async","text":"<pre><code>send_message_streaming_async(\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None\n) -&gt; AsyncIterator[TextGenerationResponse]\n</code></pre> <p>Asynchronously sends message to the language model and gets a streamed response.</p> <p>The response is only added to the history once it's fully read.</p> PARAMETER DESCRIPTION <code>message</code> <p>Message to send to the model</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024]. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process. Uses the value specified when calling <code>ChatModel.start_chat</code> by default.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AsyncIterator[TextGenerationResponse]</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>AsyncIterator[TextGenerationResponse]</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def send_message_streaming_async(\n    self,\n    message: str,\n    *,\n    max_output_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n) -&gt; AsyncIterator[TextGenerationResponse]:\n    \"\"\"Asynchronously sends message to the language model and gets a streamed response.\n\n    The response is only added to the history once it's fully read.\n\n    Args:\n        message: Message to send to the model\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n            Uses the value specified when calling `ChatModel.start_chat` by default.\n\n    Returns:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    return super().send_message_streaming_async(\n        message=message,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        stop_sequences=stop_sequences,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.CodeGenerationModel","title":"CodeGenerationModel","text":"<pre><code>CodeGenerationModel(\n    model_id: str, endpoint_name: Optional[str] = None\n)\n</code></pre> <p>               Bases: <code>_CodeGenerationModel</code>, <code>_TunableTextModelMixin</code>, <code>_ModelWithBatchPredict</code></p> <p>This constructor should not be called directly. Use <code>LanguageModel.from_pretrained(model_name=...)</code> instead.</p> PARAMETER DESCRIPTION <code>model_id</code> <p>Identifier of a Vertex LLM. Example: \"text-bison@001\"</p> <p> TYPE: <code>str</code> </p> <code>endpoint_name</code> <p>Vertex Endpoint resource name for the model</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def __init__(self, model_id: str, endpoint_name: Optional[str] = None):\n    \"\"\"Creates a LanguageModel.\n\n    This constructor should not be called directly.\n    Use `LanguageModel.from_pretrained(model_name=...)` instead.\n\n    Args:\n        model_id: Identifier of a Vertex LLM. Example: \"text-bison@001\"\n        endpoint_name: Vertex Endpoint resource name for the model\n    \"\"\"\n\n    super().__init__(\n        model_id=model_id,\n        endpoint_name=endpoint_name,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.InputOutputTextPair","title":"InputOutputTextPair  <code>dataclass</code>","text":"<pre><code>InputOutputTextPair(\n    __module__=\"vertexai.language_models\",\n    input_text: str,\n    output_text: str,\n)\n</code></pre> <p>InputOutputTextPair represents a pair of input and output texts.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.InputOutputTextPair.input_text","title":"input_text  <code>instance-attribute</code>","text":"<pre><code>input_text: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.InputOutputTextPair.output_text","title":"output_text  <code>instance-attribute</code>","text":"<pre><code>output_text: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbedding","title":"TextEmbedding  <code>dataclass</code>","text":"<pre><code>TextEmbedding(\n    __module__=\"vertexai.language_models\",\n    values: List[float],\n    statistics: Optional[TextEmbeddingStatistics] = None,\n    _prediction_response: Optional[Prediction] = None,\n)\n</code></pre> <p>Text embedding vector and statistics.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbedding.values","title":"values  <code>instance-attribute</code>","text":"<pre><code>values: List[float]\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbedding.statistics","title":"statistics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>statistics: Optional[TextEmbeddingStatistics] = None\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingInput","title":"TextEmbeddingInput  <code>dataclass</code>","text":"<pre><code>TextEmbeddingInput(\n    __module__=\"vertexai.language_models\",\n    text: str,\n    task_type: Optional[str] = None,\n    title: Optional[str] = None,\n)\n</code></pre> <p>Structural text embedding input.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The main text content to embed.</p> <p> TYPE: <code>str</code> </p> <code>task_type</code> <p>The name of the downstream task the embeddings will be used for. Valid values: RETRIEVAL_QUERY     Specifies the given text is a query in a search/retrieval setting. RETRIEVAL_DOCUMENT     Specifies the given text is a document from the corpus being searched. SEMANTIC_SIMILARITY     Specifies the given text will be used for STS. CLASSIFICATION     Specifies that the given text will be classified. CLUSTERING     Specifies that the embeddings will be used for clustering. QUESTION_ANSWERING     Specifies that the embeddings will be used for question answering. FACT_VERIFICATION     Specifies that the embeddings will be used for fact verification.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>title</code> <p>Optional identifier of the text content.</p> <p> TYPE: <code>Optional[str]</code> </p>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingInput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingInput.task_type","title":"task_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>task_type: Optional[str] = None\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingInput.title","title":"title  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>title: Optional[str] = None\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingModel","title":"TextEmbeddingModel","text":"<pre><code>TextEmbeddingModel(\n    model_id: str, endpoint_name: Optional[str] = None\n)\n</code></pre> <p>               Bases: <code>_TextEmbeddingModel</code>, <code>_TunableTextEmbeddingModelMixin</code></p> <p>This constructor should not be called directly. Use <code>LanguageModel.from_pretrained(model_name=...)</code> instead.</p> PARAMETER DESCRIPTION <code>model_id</code> <p>Identifier of a Vertex LLM. Example: \"text-bison@001\"</p> <p> TYPE: <code>str</code> </p> <code>endpoint_name</code> <p>Vertex Endpoint resource name for the model</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def __init__(self, model_id: str, endpoint_name: Optional[str] = None):\n    \"\"\"Creates a LanguageModel.\n\n    This constructor should not be called directly.\n    Use `LanguageModel.from_pretrained(model_name=...)` instead.\n\n    Args:\n        model_id: Identifier of a Vertex LLM. Example: \"text-bison@001\"\n        endpoint_name: Vertex Endpoint resource name for the model\n    \"\"\"\n\n    super().__init__(\n        model_id=model_id,\n        endpoint_name=endpoint_name,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingModel.list_tuned_model_names","title":"list_tuned_model_names","text":"<pre><code>list_tuned_model_names() -&gt; Sequence[str]\n</code></pre> <p>Lists the names of tuned models.</p> RETURNS DESCRIPTION <code>Sequence[str]</code> <p>A list of tuned models that can be used with the <code>get_tuned_model</code> method.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def list_tuned_model_names(self) -&gt; Sequence[str]:\n    \"\"\"Lists the names of tuned models.\n\n    Returns:\n        A list of tuned models that can be used with the `get_tuned_model` method.\n    \"\"\"\n    model_info = _model_garden_models._get_model_info(\n        model_id=self._model_id,\n        schema_to_class_map={self._INSTANCE_SCHEMA_URI: type(self)},\n    )\n    return _list_tuned_model_names(model_id=model_info.tuning_model_id)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingModel.get_tuned_model","title":"get_tuned_model  <code>classmethod</code>","text":"<pre><code>get_tuned_model(*args, **kwargs)\n</code></pre> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@classmethod\ndef get_tuned_model(cls, *args, **kwargs):\n    del args, kwargs  # Unused.\n    raise NotImplementedError(\n        \"Use deploy_tuned_model instead to get the tuned model.\"\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingModel.tune_model","title":"tune_model","text":"<pre><code>tune_model(\n    *,\n    training_data: Optional[str] = None,\n    corpus_data: Optional[str] = None,\n    queries_data: Optional[str] = None,\n    test_data: Optional[str] = None,\n    validation_data: Optional[str] = None,\n    batch_size: Optional[int] = None,\n    train_steps: Optional[int] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    task_type: Optional[str] = None,\n    machine_type: Optional[str] = None,\n    accelerator: Optional[str] = None,\n    accelerator_count: Optional[int] = None\n) -&gt; _TextEmbeddingModelTuningJob\n</code></pre> <p>Tunes a model based on training data.</p> <p>This method launches and returns an asynchronous model tuning job. Usage: ``` tuning_job = model.tune_model(...) ... do some other work tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</p> <p>Args:     training_data: URI pointing to training data in TSV format.     corpus_data: URI pointing to data in JSON lines format.     queries_data: URI pointing to data in JSON lines format.     test_data: URI pointing to data in TSV format.     validation_data: URI pointing to data in TSV format.     batch_size: The training batch size.     train_steps: The number of steps to perform model tuning. Must         be greater than 30.     tuned_model_location: GCP location where the tuned model should be deployed.     model_display_name: Custom display name for the tuned model.     task_type: The task type expected to be used during inference.         Valid values are <code>DEFAULT</code>, <code>RETRIEVAL_QUERY</code>, <code>RETRIEVAL_DOCUMENT</code>,         <code>SEMANTIC_SIMILARITY</code>, <code>CLASSIFICATION</code>, <code>CLUSTERING</code>,         <code>FACT_VERIFICATION</code>, and <code>QUESTION_ANSWERING</code>.     machine_type: The machine type to use for training. For information         about selecting the machine type that matches the accelerator         type and count you have selected, see         https://cloud.google.com/compute/docs/gpus.     accelerator: The accelerator type to use for tuning, for example         <code>NVIDIA_TESLA_V100</code>. For possible values, see         https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#using-accelerators.     accelerator_count: The number of accelerators to use when training.         Using a greater number of accelerators may make training faster,         but has no effect on quality. Returns:     A <code>LanguageModelTuningJob</code> object that represents the tuning job.     Calling <code>job.result()</code> blocks until the tuning is complete and     returns a <code>LanguageModel</code> object.</p> <p>Raises:     ValueError: If the provided parameter combinations or values are not         supported.     RuntimeError: If the model does not support tuning</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def tune_model(\n    self,\n    *,\n    training_data: Optional[str] = None,\n    corpus_data: Optional[str] = None,\n    queries_data: Optional[str] = None,\n    test_data: Optional[str] = None,\n    validation_data: Optional[str] = None,\n    batch_size: Optional[int] = None,\n    train_steps: Optional[int] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    task_type: Optional[str] = None,\n    machine_type: Optional[str] = None,\n    accelerator: Optional[str] = None,\n    accelerator_count: Optional[int] = None,\n) -&gt; \"_TextEmbeddingModelTuningJob\":\n    \"\"\"Tunes a model based on training data.\n\n    This method launches and returns an asynchronous model tuning job.\n    Usage:\n    ```\n    tuning_job = model.tune_model(...)\n    ... do some other work\n    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n\n    Args:\n        training_data: URI pointing to training data in TSV format.\n        corpus_data: URI pointing to data in JSON lines format.\n        queries_data: URI pointing to data in JSON lines format.\n        test_data: URI pointing to data in TSV format.\n        validation_data: URI pointing to data in TSV format.\n        batch_size: The training batch size.\n        train_steps: The number of steps to perform model tuning. Must\n            be greater than 30.\n        tuned_model_location: GCP location where the tuned model should be deployed.\n        model_display_name: Custom display name for the tuned model.\n        task_type: The task type expected to be used during inference.\n            Valid values are `DEFAULT`, `RETRIEVAL_QUERY`, `RETRIEVAL_DOCUMENT`,\n            `SEMANTIC_SIMILARITY`, `CLASSIFICATION`, `CLUSTERING`,\n            `FACT_VERIFICATION`, and `QUESTION_ANSWERING`.\n        machine_type: The machine type to use for training. For information\n            about selecting the machine type that matches the accelerator\n            type and count you have selected, see\n            https://cloud.google.com/compute/docs/gpus.\n        accelerator: The accelerator type to use for tuning, for example\n            `NVIDIA_TESLA_V100`. For possible values, see\n            https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#using-accelerators.\n        accelerator_count: The number of accelerators to use when training.\n            Using a greater number of accelerators may make training faster,\n            but has no effect on quality.\n    Returns:\n        A `LanguageModelTuningJob` object that represents the tuning job.\n        Calling `job.result()` blocks until the tuning is complete and\n        returns a `LanguageModel` object.\n\n    Raises:\n        ValueError: If the provided parameter combinations or values are not\n            supported.\n        RuntimeError: If the model does not support tuning\n    \"\"\"\n\n    return super().tune_model(\n        training_data=training_data,\n        corpus_data=corpus_data,\n        queries_data=queries_data,\n        test_data=test_data,\n        validation_data=validation_data,\n        task_type=task_type,\n        batch_size=batch_size,\n        train_steps=train_steps,\n        tuned_model_location=tuned_model_location,\n        model_display_name=model_display_name,\n        machine_type=machine_type,\n        accelerator=accelerator,\n        accelerator_count=accelerator_count,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingModel.deploy_tuned_model","title":"deploy_tuned_model  <code>classmethod</code>","text":"<pre><code>deploy_tuned_model(\n    tuned_model_name: str,\n    machine_type: Optional[str] = None,\n    accelerator: Optional[str] = None,\n    accelerator_count: Optional[int] = None,\n) -&gt; _LanguageModel\n</code></pre> <p>Loads the specified tuned language model.</p> PARAMETER DESCRIPTION <code>machine_type</code> <p>Machine type. E.g., \"a2-highgpu-1g\". See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>accelerator</code> <p>Kind of accelerator. E.g., \"NVIDIA_TESLA_A100\". See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>accelerator_count</code> <p>Count of accelerators.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>_LanguageModel</code> <p>Tuned <code>LanguageModel</code> object.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@classmethod\ndef deploy_tuned_model(\n    cls,\n    tuned_model_name: str,\n    machine_type: Optional[str] = None,\n    accelerator: Optional[str] = None,\n    accelerator_count: Optional[int] = None,\n) -&gt; \"_LanguageModel\":\n    \"\"\"Loads the specified tuned language model.\n\n    Args:\n        machine_type: Machine type. E.g., \"a2-highgpu-1g\". See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.\n        accelerator: Kind of accelerator. E.g., \"NVIDIA_TESLA_A100\". See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.\n        accelerator_count: Count of accelerators.\n\n    Returns:\n        Tuned `LanguageModel` object.\n    \"\"\"\n    tuned_vertex_model = aiplatform.Model(tuned_model_name)\n    tuned_model_labels = tuned_vertex_model.labels\n\n    if _TUNING_BASE_MODEL_ID_LABEL_KEY not in tuned_model_labels:\n        raise ValueError(\n            f\"The provided model {tuned_model_name} does not have a base model ID.\"\n        )\n\n    tuning_model_id = tuned_vertex_model.labels[_TUNING_BASE_MODEL_ID_LABEL_KEY]\n    tuned_model_deployments = tuned_vertex_model.gca_resource.deployed_models\n    if len(tuned_model_deployments) == 0:\n        # Deploying a model to an endpoint requires a resource quota.\n        endpoint_name = tuned_vertex_model.deploy(\n            machine_type=machine_type,\n            accelerator_type=accelerator,\n            accelerator_count=accelerator_count,\n        ).resource_name\n    else:\n        endpoint_name = tuned_model_deployments[0].endpoint\n\n    base_model_id = _get_model_id_from_tuning_model_id(tuning_model_id)\n    model_info = _model_garden_models._get_model_info(\n        model_id=base_model_id,\n        schema_to_class_map={cls._INSTANCE_SCHEMA_URI: cls},\n    )\n    model = model_info.interface_class(\n        model_id=base_model_id,\n        endpoint_name=endpoint_name,\n    )\n    return model\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    texts: List[Union[str, TextEmbeddingInput]],\n    *,\n    auto_truncate: bool = True,\n    output_dimensionality: Optional[int] = None\n) -&gt; List[TextEmbedding]\n</code></pre> <p>Calculates embeddings for the given texts.</p> PARAMETER DESCRIPTION <code>texts</code> <p>A list of texts or <code>TextEmbeddingInput</code> objects to embed.</p> <p> TYPE: <code>List[Union[str, TextEmbeddingInput]]</code> </p> <code>auto_truncate</code> <p>Whether to automatically truncate long texts. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>output_dimensionality</code> <p>Optional dimensions of embeddings. Range: [1, 768]. Default: None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextEmbedding]</code> <p>A list of <code>TextEmbedding</code> objects.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def get_embeddings(\n    self,\n    texts: List[Union[str, TextEmbeddingInput]],\n    *,\n    auto_truncate: bool = True,\n    output_dimensionality: Optional[int] = None,\n) -&gt; List[\"TextEmbedding\"]:\n    \"\"\"Calculates embeddings for the given texts.\n\n    Args:\n        texts: A list of texts or `TextEmbeddingInput` objects to embed.\n        auto_truncate: Whether to automatically truncate long texts. Default: True.\n        output_dimensionality: Optional dimensions of embeddings. Range: [1, 768]. Default: None.\n\n    Returns:\n        A list of `TextEmbedding` objects.\n    \"\"\"\n    prediction_request = self._prepare_text_embedding_request(\n        texts=texts,\n        auto_truncate=auto_truncate,\n        output_dimensionality=output_dimensionality,\n    )\n\n    prediction_response = self._endpoint.predict(\n        instances=prediction_request.instances,\n        parameters=prediction_request.parameters,\n    )\n\n    return [\n        TextEmbedding._parse_text_embedding_response(\n            prediction_response, i_prediction\n        )\n        for i_prediction, _ in enumerate(prediction_response.predictions)\n    ]\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextEmbeddingModel.get_embeddings_async","title":"get_embeddings_async  <code>async</code>","text":"<pre><code>get_embeddings_async(\n    texts: List[Union[str, TextEmbeddingInput]],\n    *,\n    auto_truncate: bool = True,\n    output_dimensionality: Optional[int] = None\n) -&gt; List[TextEmbedding]\n</code></pre> <p>Asynchronously calculates embeddings for the given texts.</p> PARAMETER DESCRIPTION <code>texts</code> <p>A list of texts or <code>TextEmbeddingInput</code> objects to embed.</p> <p> TYPE: <code>List[Union[str, TextEmbeddingInput]]</code> </p> <code>auto_truncate</code> <p>Whether to automatically truncate long texts. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>output_dimensionality</code> <p>Optional dimensions of embeddings. Range: [1, 768]. Default: None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextEmbedding]</code> <p>A list of <code>TextEmbedding</code> objects.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def get_embeddings_async(\n    self,\n    texts: List[Union[str, TextEmbeddingInput]],\n    *,\n    auto_truncate: bool = True,\n    output_dimensionality: Optional[int] = None,\n) -&gt; List[\"TextEmbedding\"]:\n    \"\"\"Asynchronously calculates embeddings for the given texts.\n\n    Args:\n        texts: A list of texts or `TextEmbeddingInput` objects to embed.\n        auto_truncate: Whether to automatically truncate long texts. Default: True.\n        output_dimensionality: Optional dimensions of embeddings. Range: [1, 768]. Default: None.\n\n    Returns:\n        A list of `TextEmbedding` objects.\n    \"\"\"\n    prediction_request = self._prepare_text_embedding_request(\n        texts=texts,\n        auto_truncate=auto_truncate,\n        output_dimensionality=output_dimensionality,\n    )\n\n    prediction_response = await self._endpoint.predict_async(\n        instances=prediction_request.instances,\n        parameters=prediction_request.parameters,\n    )\n\n    return [\n        TextEmbedding._parse_text_embedding_response(\n            prediction_response, i_prediction\n        )\n        for i_prediction, _ in enumerate(prediction_response.predictions)\n    ]\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel","title":"TextGenerationModel","text":"<pre><code>TextGenerationModel(\n    model_id: str, endpoint_name: Optional[str] = None\n)\n</code></pre> <p>               Bases: <code>_TextGenerationModel</code>, <code>_TunableTextModelMixin</code>, <code>_ModelWithBatchPredict</code>, <code>_RlhfTunableModelMixin</code></p> <p>This constructor should not be called directly. Use <code>LanguageModel.from_pretrained(model_name=...)</code> instead.</p> PARAMETER DESCRIPTION <code>model_id</code> <p>Identifier of a Vertex LLM. Example: \"text-bison@001\"</p> <p> TYPE: <code>str</code> </p> <code>endpoint_name</code> <p>Vertex Endpoint resource name for the model</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def __init__(self, model_id: str, endpoint_name: Optional[str] = None):\n    \"\"\"Creates a LanguageModel.\n\n    This constructor should not be called directly.\n    Use `LanguageModel.from_pretrained(model_name=...)` instead.\n\n    Args:\n        model_id: Identifier of a Vertex LLM. Example: \"text-bison@001\"\n        endpoint_name: Vertex Endpoint resource name for the model\n    \"\"\"\n\n    super().__init__(\n        model_id=model_id,\n        endpoint_name=endpoint_name,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.list_tuned_model_names","title":"list_tuned_model_names","text":"<pre><code>list_tuned_model_names() -&gt; Sequence[str]\n</code></pre> <p>Lists the names of tuned models.</p> RETURNS DESCRIPTION <code>Sequence[str]</code> <p>A list of tuned models that can be used with the <code>get_tuned_model</code> method.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def list_tuned_model_names(self) -&gt; Sequence[str]:\n    \"\"\"Lists the names of tuned models.\n\n    Returns:\n        A list of tuned models that can be used with the `get_tuned_model` method.\n    \"\"\"\n    model_info = _model_garden_models._get_model_info(\n        model_id=self._model_id,\n        schema_to_class_map={self._INSTANCE_SCHEMA_URI: type(self)},\n    )\n    return _list_tuned_model_names(model_id=model_info.tuning_model_id)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.get_tuned_model","title":"get_tuned_model  <code>classmethod</code>","text":"<pre><code>get_tuned_model(tuned_model_name: str) -&gt; _LanguageModel\n</code></pre> <p>Loads the specified tuned language model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>@classmethod\ndef get_tuned_model(cls, tuned_model_name: str) -&gt; \"_LanguageModel\":\n    \"\"\"Loads the specified tuned language model.\"\"\"\n\n    tuned_vertex_model = aiplatform.Model(tuned_model_name)\n    tuned_model_labels = tuned_vertex_model.labels\n\n    if _TUNING_BASE_MODEL_ID_LABEL_KEY not in tuned_model_labels:\n        raise ValueError(\n            f\"The provided model {tuned_model_name} does not have a base model ID.\"\n        )\n\n    tuning_model_id = tuned_vertex_model.labels[_TUNING_BASE_MODEL_ID_LABEL_KEY]\n\n    tuned_model_deployments = tuned_vertex_model.gca_resource.deployed_models\n    if len(tuned_model_deployments) == 0:\n        # Deploying the model\n        endpoint_name = tuned_vertex_model.deploy().resource_name\n    else:\n        endpoint_name = tuned_model_deployments[0].endpoint\n\n    base_model_id = _get_model_id_from_tuning_model_id(tuning_model_id)\n    model_info = _model_garden_models._get_model_info(\n        model_id=base_model_id,\n        schema_to_class_map={cls._INSTANCE_SCHEMA_URI: cls},\n    )\n    model = model_info.interface_class(\n        model_id=base_model_id,\n        endpoint_name=endpoint_name,\n    )\n    return model\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.tune_model_rlhf","title":"tune_model_rlhf","text":"<pre><code>tune_model_rlhf(\n    *,\n    prompt_data: Union[str, DataFrame],\n    preference_data: Union[str, DataFrame],\n    model_display_name: Optional[str] = None,\n    prompt_sequence_length: Optional[int] = None,\n    target_sequence_length: Optional[int] = None,\n    reward_model_learning_rate_multiplier: Optional[\n        float\n    ] = None,\n    reinforcement_learning_rate_multiplier: Optional[\n        float\n    ] = None,\n    reward_model_train_steps: Optional[int] = None,\n    reinforcement_learning_train_steps: Optional[\n        int\n    ] = None,\n    kl_coeff: Optional[float] = None,\n    default_context: Optional[str] = None,\n    tuning_job_location: Optional[str] = None,\n    accelerator_type: Optional[\n        _ACCELERATOR_TYPE_TYPE\n    ] = None,\n    tuning_evaluation_spec: Optional[\n        TuningEvaluationSpec\n    ] = None\n) -&gt; _LanguageModelTuningJob\n</code></pre> <p>Tunes a model using reinforcement learning from human feedback.</p> <p>This method launches and returns an asynchronous model tuning job. Usage: <pre><code>tuning_job = model.tune_model_rlhf(...)\n... do some other work\ntuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n</code></pre></p> PARAMETER DESCRIPTION <code>prompt_data</code> <p>A Pandas DataFrame or a URI pointing to data in JSON lines format. The dataset schema is model-specific. See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset</p> <p> TYPE: <code>Union[str, DataFrame]</code> </p> <code>preference_data</code> <p>A Pandas DataFrame or a URI pointing to data in JSON lines format. The dataset schema is model-specific. See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset</p> <p> TYPE: <code>Union[str, DataFrame]</code> </p> <code>model_display_name</code> <p>Custom display name for the tuned model. If not provided, a default name will be created.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>prompt_sequence_length</code> <p>Maximum tokenized sequence length for input text. Higher values increase memory overhead. This value should be at most 8192. Default value is 512.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>target_sequence_length</code> <p>Maximum tokenized sequence length for target text. Higher values increase memory overhead. This value should be at most 1024. Default value is 64.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>reward_model_learning_rate_multiplier</code> <p>Constant used to adjust the base learning rate used when training a reward model. Multiply by a number &gt; 1 to increase the magnitude of updates applied at each training step or multiply by a number &lt; 1 to decrease the magnitude of updates. Default value is 1.0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>reinforcement_learning_rate_multiplier</code> <p>Constant used to adjust the base learning rate used during reinforcement learning. Multiply by a number &gt; 1 to increase the magnitude of updates applied at each training step or multiply by a number &lt; 1 to decrease the magnitude of updates. Default value is 1.0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>reward_model_train_steps</code> <p>Number of steps to use when training a reward model. Default value is 1000.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>reinforcement_learning_train_steps</code> <p>Number of reinforcement learning steps to perform when tuning a base model. Default value is 1000.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>kl_coeff</code> <p>Coefficient for KL penalty. This regularizes the policy model and penalizes if it diverges from its initial distribution. If set to 0, the reference language model is not loaded into memory. Default value is 0.1.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>default_context</code> <p>This field lets the model know what task to perform. Base models have been trained over a large set of varied instructions. You can give a simple and intuitive description of the task and the model will follow it, e.g. \"Classify this movie review as positive or negative\" or \"Translate this sentence to Danish\". Do not specify this if your dataset already prepends the instruction to the inputs field.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tuning_job_location</code> <p>GCP location where the tuning job should be run.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>accelerator_type</code> <p>Type of accelerator to use. Can be \"TPU\" or \"GPU\".</p> <p> TYPE: <code>Optional[_ACCELERATOR_TYPE_TYPE]</code> DEFAULT: <code>None</code> </p> <code>tuning_evaluation_spec</code> <p>Evaluation settings to use during tuning.</p> <p> TYPE: <code>Optional[TuningEvaluationSpec]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>_LanguageModelTuningJob</code> <p>A <code>LanguageModelTuningJob</code> object that represents the tuning job.</p> <code>_LanguageModelTuningJob</code> <p>Calling <code>job.result()</code> blocks until the tuning is complete and returns a <code>LanguageModel</code> object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the \"tuning_job_location\" value is not supported</p> <code>RuntimeError</code> <p>If the model does not support tuning</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def tune_model_rlhf(\n    self,\n    *,\n    prompt_data: Union[str, \"pandas.core.frame.DataFrame\"],\n    preference_data: Union[str, \"pandas.core.frame.DataFrame\"],\n    model_display_name: Optional[str] = None,\n    prompt_sequence_length: Optional[int] = None,\n    target_sequence_length: Optional[int] = None,\n    reward_model_learning_rate_multiplier: Optional[float] = None,\n    reinforcement_learning_rate_multiplier: Optional[float] = None,\n    reward_model_train_steps: Optional[int] = None,\n    reinforcement_learning_train_steps: Optional[int] = None,\n    kl_coeff: Optional[float] = None,\n    default_context: Optional[str] = None,\n    tuning_job_location: Optional[str] = None,\n    accelerator_type: Optional[_ACCELERATOR_TYPE_TYPE] = None,\n    tuning_evaluation_spec: Optional[\"TuningEvaluationSpec\"] = None,\n) -&gt; \"_LanguageModelTuningJob\":\n    \"\"\"Tunes a model using reinforcement learning from human feedback.\n\n    This method launches and returns an asynchronous model tuning job.\n    Usage:\n    ```\n    tuning_job = model.tune_model_rlhf(...)\n    ... do some other work\n    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n    ```\n\n    Args:\n        prompt_data: A Pandas DataFrame or a URI pointing to data in JSON lines\n            format. The dataset schema is model-specific.\n            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset\n        preference_data: A Pandas DataFrame or a URI pointing to data in JSON lines\n            format. The dataset schema is model-specific.\n            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset\n        model_display_name: Custom display name for the tuned model.\n            If not provided, a default name will be created.\n        prompt_sequence_length: Maximum tokenized sequence length for input text.\n            Higher values increase memory overhead.\n            This value should be at most 8192. Default value is 512.\n        target_sequence_length: Maximum tokenized sequence length for target text.\n            Higher values increase memory overhead.\n            This value should be at most 1024. Default value is 64.\n        reward_model_learning_rate_multiplier: Constant used to adjust the base\n            learning rate used when training a reward model. Multiply by a\n            number &gt; 1 to increase the magnitude of updates applied at each\n            training step or multiply by a number &lt; 1 to decrease the magnitude\n            of updates. Default value is 1.0.\n        reinforcement_learning_rate_multiplier: Constant used to adjust the base\n            learning rate used during reinforcement learning. Multiply by a\n            number &gt; 1 to increase the magnitude of updates applied at each\n            training step or multiply by a number &lt; 1 to decrease the magnitude\n            of updates. Default value is 1.0.\n        reward_model_train_steps: Number of steps to use when training a reward\n            model. Default value is 1000.\n        reinforcement_learning_train_steps: Number of reinforcement learning steps\n            to perform when tuning a base model. Default value is 1000.\n        kl_coeff: Coefficient for KL penalty. This regularizes the policy model and\n            penalizes if it diverges from its initial distribution. If set to 0,\n            the reference language model is not loaded into memory. Default value\n            is 0.1.\n        default_context: This field lets the model know what task to perform.\n            Base models have been trained over a large set of varied instructions.\n            You can give a simple and intuitive description of the task and the\n            model will follow it, e.g. \"Classify this movie review as positive or\n            negative\" or \"Translate this sentence to Danish\". Do not specify this\n            if your dataset already prepends the instruction to the inputs field.\n        tuning_job_location: GCP location where the tuning job should be run.\n        accelerator_type: Type of accelerator to use. Can be \"TPU\" or \"GPU\".\n        tuning_evaluation_spec: Evaluation settings to use during tuning.\n\n    Returns:\n        A `LanguageModelTuningJob` object that represents the tuning job.\n        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.\n\n    Raises:\n        ValueError: If the \"tuning_job_location\" value is not supported\n        RuntimeError: If the model does not support tuning\n    \"\"\"\n    tuning_job_location = (\n        tuning_job_location or aiplatform_initializer.global_config.location\n    )\n    eval_dataset = None\n    tensorboard_resource_id = None\n    if tuning_evaluation_spec is not None:\n        _check_unused_rlhf_eval_specs(tuning_evaluation_spec)\n\n        eval_dataset = tuning_evaluation_spec.evaluation_data\n        if eval_dataset is not None and not eval_dataset.startswith(\"gs://\"):\n            raise ValueError(\n                \"evaluation_data must be a GCS URI that starts with gs://\"\n            )\n\n        tensorboard_resource_id = _get_tensorboard_resource_id_from_evaluation_spec(\n            tuning_evaluation_spec, tuning_job_location\n        )\n    prompt_dataset_uri = _maybe_upload_training_data(\n        training_data=prompt_data,\n        model_id=self._model_id,\n    )\n    preference_dataset_uri = _maybe_upload_training_data(\n        training_data=preference_data,\n        model_id=self._model_id,\n    )\n\n    if accelerator_type:\n        if accelerator_type not in _ACCELERATOR_TYPES:\n            raise ValueError(\n                f\"Unsupported accelerator type: {accelerator_type}.\"\n                f\" Supported types: {_ACCELERATOR_TYPES}\"\n            )\n\n    tuning_parameters = _RlhfTuningParameters(\n        prompt_dataset=prompt_dataset_uri,\n        preference_dataset=preference_dataset_uri,\n        large_model_reference=self._model_id.rsplit(\"/\", 1)[-1],\n        location=tuning_job_location,\n        model_display_name=model_display_name,\n        prompt_sequence_length=prompt_sequence_length,\n        target_sequence_length=target_sequence_length,\n        reward_model_learning_rate_multiplier=reward_model_learning_rate_multiplier,\n        reinforcement_learning_rate_multiplier=reinforcement_learning_rate_multiplier,\n        reward_model_train_steps=reward_model_train_steps,\n        reinforcement_learning_train_steps=reinforcement_learning_train_steps,\n        kl_coeff=kl_coeff,\n        instruction=default_context,\n        eval_dataset=eval_dataset,\n        accelerator_type=accelerator_type,\n        tensorboard_resource_id=tensorboard_resource_id,\n    )\n\n    return self._tune_model_rlhf(\n        tuning_parameters=tuning_parameters,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.batch_predict","title":"batch_predict","text":"<pre><code>batch_predict(\n    *,\n    dataset: Union[str, List[str]],\n    destination_uri_prefix: str,\n    model_parameters: Optional[Dict] = None\n) -&gt; BatchPredictionJob\n</code></pre> <p>Starts a batch prediction job with the model.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>The location of the dataset. <code>gs://</code> and <code>bq://</code> URIs are supported.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>destination_uri_prefix</code> <p>The URI prefix for the prediction. <code>gs://</code> and <code>bq://</code> URIs are supported.</p> <p> TYPE: <code>str</code> </p> <code>model_parameters</code> <p>Model-specific parameters to send to the model.</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BatchPredictionJob</code> <p>A <code>BatchPredictionJob</code> object</p> <p>Raises:     ValueError: When source or destination URI is not supported.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def batch_predict(\n    self,\n    *,\n    dataset: Union[str, List[str]],\n    destination_uri_prefix: str,\n    model_parameters: Optional[Dict] = None,\n) -&gt; aiplatform.BatchPredictionJob:\n    \"\"\"Starts a batch prediction job with the model.\n\n    Args:\n        dataset: The location of the dataset.\n            `gs://` and `bq://` URIs are supported.\n        destination_uri_prefix: The URI prefix for the prediction.\n            `gs://` and `bq://` URIs are supported.\n        model_parameters: Model-specific parameters to send to the model.\n\n    Returns:\n        A `BatchPredictionJob` object\n    Raises:\n        ValueError: When source or destination URI is not supported.\n    \"\"\"\n    arguments = {}\n    first_source_uri = dataset if isinstance(dataset, str) else dataset[0]\n    if first_source_uri.startswith(\"gs://\"):\n        if not isinstance(dataset, str):\n            if not all(uri.startswith(\"gs://\") for uri in dataset):\n                raise ValueError(\n                    f\"All URIs in the list must start with 'gs://': {dataset}\"\n                )\n        arguments[\"gcs_source\"] = dataset\n    elif first_source_uri.startswith(\"bq://\"):\n        if not isinstance(dataset, str):\n            raise ValueError(\n                f\"Only single BigQuery source can be specified: {dataset}\"\n            )\n        arguments[\"bigquery_source\"] = dataset\n    else:\n        raise ValueError(f\"Unsupported source_uri: {dataset}\")\n\n    if destination_uri_prefix.startswith(\"gs://\"):\n        arguments[\"gcs_destination_prefix\"] = destination_uri_prefix\n    elif destination_uri_prefix.startswith(\"bq://\"):\n        arguments[\"bigquery_destination_prefix\"] = destination_uri_prefix\n    else:\n        raise ValueError(f\"Unsupported destination_uri: {destination_uri_prefix}\")\n\n    model_name = self._model_resource_name\n\n    job = aiplatform.BatchPredictionJob.create(\n        model_name=model_name,\n        job_display_name=None,\n        **arguments,\n        model_parameters=model_parameters,\n    )\n    return job\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.tune_model","title":"tune_model","text":"<pre><code>tune_model(\n    training_data: Union[str, DataFrame],\n    *,\n    train_steps: Optional[int] = None,\n    learning_rate_multiplier: Optional[float] = None,\n    tuning_job_location: Optional[str] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    tuning_evaluation_spec: Optional[\n        TuningEvaluationSpec\n    ] = None,\n    accelerator_type: Optional[\n        _ACCELERATOR_TYPE_TYPE\n    ] = None,\n    max_context_length: Optional[str] = None\n) -&gt; _LanguageModelTuningJob\n</code></pre> <p>Tunes a model based on training data.</p> <p>This method launches and returns an asynchronous model tuning job. Usage: ``` tuning_job = model.tune_model(...) ... do some other work tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</p> <p>Args:     training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.         The dataset schema is model-specific.         See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format     train_steps: Number of training batches to tune on (batch size is 8 samples).     learning_rate_multiplier: Learning rate multiplier to use in tuning.     tuning_job_location: GCP location where the tuning job should be run.     tuned_model_location: GCP location where the tuned model should be deployed.     model_display_name: Custom display name for the tuned model.     tuning_evaluation_spec: Specification for the model evaluation during tuning.     accelerator_type: Type of accelerator to use. Can be \"TPU\" or \"GPU\".     max_context_length: The max context length used for tuning.         Can be either '8k' or '32k'</p> <p>Returns:     A <code>LanguageModelTuningJob</code> object that represents the tuning job.     Calling <code>job.result()</code> blocks until the tuning is complete and returns a <code>LanguageModel</code> object.</p> <p>Raises:     ValueError: If the \"tuning_job_location\" value is not supported     ValueError: If the \"tuned_model_location\" value is not supported     RuntimeError: If the model does not support tuning</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def tune_model(\n    self,\n    training_data: Union[str, \"pandas.core.frame.DataFrame\"],\n    *,\n    train_steps: Optional[int] = None,\n    learning_rate_multiplier: Optional[float] = None,\n    tuning_job_location: Optional[str] = None,\n    tuned_model_location: Optional[str] = None,\n    model_display_name: Optional[str] = None,\n    tuning_evaluation_spec: Optional[\"TuningEvaluationSpec\"] = None,\n    accelerator_type: Optional[_ACCELERATOR_TYPE_TYPE] = None,\n    max_context_length: Optional[str] = None,\n) -&gt; \"_LanguageModelTuningJob\":\n    \"\"\"Tunes a model based on training data.\n\n    This method launches and returns an asynchronous model tuning job.\n    Usage:\n    ```\n    tuning_job = model.tune_model(...)\n    ... do some other work\n    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete\n\n    Args:\n        training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.\n            The dataset schema is model-specific.\n            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format\n        train_steps: Number of training batches to tune on (batch size is 8 samples).\n        learning_rate_multiplier: Learning rate multiplier to use in tuning.\n        tuning_job_location: GCP location where the tuning job should be run.\n        tuned_model_location: GCP location where the tuned model should be deployed.\n        model_display_name: Custom display name for the tuned model.\n        tuning_evaluation_spec: Specification for the model evaluation during tuning.\n        accelerator_type: Type of accelerator to use. Can be \"TPU\" or \"GPU\".\n        max_context_length: The max context length used for tuning.\n            Can be either '8k' or '32k'\n\n    Returns:\n        A `LanguageModelTuningJob` object that represents the tuning job.\n        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.\n\n    Raises:\n        ValueError: If the \"tuning_job_location\" value is not supported\n        ValueError: If the \"tuned_model_location\" value is not supported\n        RuntimeError: If the model does not support tuning\n    \"\"\"\n    # Note: Chat models do not support default_context\n    return super().tune_model(\n        training_data=training_data,\n        train_steps=train_steps,\n        learning_rate_multiplier=learning_rate_multiplier,\n        tuning_job_location=tuning_job_location,\n        tuned_model_location=tuned_model_location,\n        model_display_name=model_display_name,\n        tuning_evaluation_spec=tuning_evaluation_spec,\n        accelerator_type=accelerator_type,\n        max_context_length=max_context_length,\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.predict","title":"predict","text":"<pre><code>predict(\n    prompt: str,\n    *,\n    max_output_tokens: Optional[\n        int\n    ] = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[WebSearch, VertexAISearch, InlineContext]\n    ] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Gets model response for a single prompt.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Question to ask the model.</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024].</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>_DEFAULT_MAX_OUTPUT_TOKENS</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of response candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>grounding_source</code> <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p> <p> TYPE: <code>Optional[Union[WebSearch, VertexAISearch, InlineContext]]</code> DEFAULT: <code>None</code> </p> <code>logprobs</code> <p>Returns the top <code>logprobs</code> most likely candidate tokens with their log probabilities at each generation step. The chosen tokens and their log probabilities at each step are always returned. The chosen token may or may not be in the top <code>logprobs</code> most likely candidates. The minimum value for <code>logprobs</code> is 0, which means only the chosen tokens and their log probabilities are returned. The maximum value for <code>logprobs</code> is 5.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>logit_bias</code> <p>Mapping from token IDs (integers) to their bias values (floats). The bias values are added to the logits before sampling. Larger positive bias increases the probability of choosing the token. Smaller negative bias decreases the probability of choosing the token. Range: [-100.0, 100.0]</p> <p> TYPE: <code>Optional[Dict[int, float]]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates the same output with the same seed. If seed is not set, the seed used in decoder will not be deterministic, thus the generated random noise will not be deterministic. If seed is set, the generated random noise will be deterministic.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def predict(\n    self,\n    prompt: str,\n    *,\n    max_output_tokens: Optional[int] = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[\n            GroundingSource.WebSearch,\n            GroundingSource.VertexAISearch,\n            GroundingSource.InlineContext,\n        ]\n    ] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Gets model response for a single prompt.\n\n    Args:\n        prompt: Question to ask the model.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        candidate_count: Number of response candidates to return.\n        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.\n        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities\n            at each generation step. The chosen tokens and their log probabilities at each step are always\n            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.\n            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log\n            probabilities are returned.\n            The maximum value for `logprobs` is 5.\n        presence_penalty:\n            Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics.\n            Range: [-2.0, 2.0]\n        frequency_penalty:\n            Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content.\n            Range: [-2.0, 2.0]\n        logit_bias:\n            Mapping from token IDs (integers) to their bias values (floats).\n            The bias values are added to the logits before sampling.\n            Larger positive bias increases the probability of choosing the token.\n            Smaller negative bias decreases the probability of choosing the token.\n            Range: [-100.0, 100.0]\n        seed:\n            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to\n            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates\n            the same output with the same seed. If seed is not set, the seed used in decoder will not be\n            deterministic, thus the generated random noise will not be deterministic. If seed is set, the\n            generated random noise will be deterministic.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.\n    \"\"\"\n    prediction_request = _create_text_generation_prediction_request(\n        prompt=prompt,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n        candidate_count=candidate_count,\n        grounding_source=grounding_source,\n        logprobs=logprobs,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        logit_bias=logit_bias,\n        seed=seed,\n    )\n\n    prediction_response = self._endpoint.predict(\n        instances=[prediction_request.instance],\n        parameters=prediction_request.parameters,\n    )\n\n    return _parse_text_generation_model_multi_candidate_response(\n        prediction_response\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.predict_async","title":"predict_async  <code>async</code>","text":"<pre><code>predict_async(\n    prompt: str,\n    *,\n    max_output_tokens: Optional[\n        int\n    ] = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[WebSearch, VertexAISearch, InlineContext]\n    ] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None\n) -&gt; MultiCandidateTextGenerationResponse\n</code></pre> <p>Asynchronously gets model response for a single prompt.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Question to ask the model.</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024].</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>_DEFAULT_MAX_OUTPUT_TOKENS</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of response candidates to return.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>grounding_source</code> <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p> <p> TYPE: <code>Optional[Union[WebSearch, VertexAISearch, InlineContext]]</code> DEFAULT: <code>None</code> </p> <code>logprobs</code> <p>Returns the top <code>logprobs</code> most likely candidate tokens with their log probabilities at each generation step. The chosen tokens and their log probabilities at each step are always returned. The chosen token may or may not be in the top <code>logprobs</code> most likely candidates. The minimum value for <code>logprobs</code> is 0, which means only the chosen tokens and their log probabilities are returned. The maximum value for <code>logprobs</code> is 5.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>logit_bias</code> <p>Mapping from token IDs (integers) to their bias values (floats). The bias values are added to the logits before sampling. Larger positive bias increases the probability of choosing the token. Smaller negative bias decreases the probability of choosing the token. Range: [-100.0, 100.0]</p> <p> TYPE: <code>Optional[Dict[int, float]]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates the same output with the same seed. If seed is not set, the seed used in decoder will not be deterministic, thus the generated random noise will not be deterministic. If seed is set, the generated random noise will be deterministic.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiCandidateTextGenerationResponse</code> <p>A <code>MultiCandidateTextGenerationResponse</code> object that contains the text produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def predict_async(\n    self,\n    prompt: str,\n    *,\n    max_output_tokens: Optional[int] = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    candidate_count: Optional[int] = None,\n    grounding_source: Optional[\n        Union[\n            GroundingSource.WebSearch,\n            GroundingSource.VertexAISearch,\n            GroundingSource.InlineContext,\n        ]\n    ] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None,\n) -&gt; \"MultiCandidateTextGenerationResponse\":\n    \"\"\"Asynchronously gets model response for a single prompt.\n\n    Args:\n        prompt: Question to ask the model.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        candidate_count: Number of response candidates to return.\n        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.\n        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities\n            at each generation step. The chosen tokens and their log probabilities at each step are always\n            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.\n            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log\n            probabilities are returned.\n            The maximum value for `logprobs` is 5.\n        presence_penalty:\n            Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics.\n            Range: [-2.0, 2.0]\n        frequency_penalty:\n            Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content.\n            Range: [-2.0, 2.0]\n        logit_bias:\n            Mapping from token IDs (integers) to their bias values (floats).\n            The bias values are added to the logits before sampling.\n            Larger positive bias increases the probability of choosing the token.\n            Smaller negative bias decreases the probability of choosing the token.\n            Range: [-100.0, 100.0]\n        seed:\n            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to\n            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates\n            the same output with the same seed. If seed is not set, the seed used in decoder will not be\n            deterministic, thus the generated random noise will not be deterministic. If seed is set, the\n            generated random noise will be deterministic.\n\n    Returns:\n        A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.\n    \"\"\"\n    prediction_request = _create_text_generation_prediction_request(\n        prompt=prompt,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n        candidate_count=candidate_count,\n        grounding_source=grounding_source,\n        logprobs=logprobs,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        logit_bias=logit_bias,\n        seed=seed,\n    )\n\n    prediction_response = await self._endpoint.predict_async(\n        instances=[prediction_request.instance],\n        parameters=prediction_request.parameters,\n    )\n\n    return _parse_text_generation_model_multi_candidate_response(\n        prediction_response\n    )\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.predict_streaming","title":"predict_streaming","text":"<pre><code>predict_streaming(\n    prompt: str,\n    *,\n    max_output_tokens: int = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None\n) -&gt; Iterator[TextGenerationResponse]\n</code></pre> <p>Gets a streaming model response for a single prompt.</p> <p>The result is a stream (generator) of partial responses.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Question to ask the model.</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024].</p> <p> TYPE: <code>int</code> DEFAULT: <code>_DEFAULT_MAX_OUTPUT_TOKENS</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>logprobs</code> <p>Returns the top <code>logprobs</code> most likely candidate tokens with their log probabilities at each generation step. The chosen tokens and their log probabilities at each step are always returned. The chosen token may or may not be in the top <code>logprobs</code> most likely candidates. The minimum value for <code>logprobs</code> is 0, which means only the chosen tokens and their log probabilities are returned. The maximum value for <code>logprobs</code> is 5.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>logit_bias</code> <p>Mapping from token IDs (integers) to their bias values (floats). The bias values are added to the logits before sampling. Larger positive bias increases the probability of choosing the token. Smaller negative bias decreases the probability of choosing the token. Range: [-100.0, 100.0]</p> <p> TYPE: <code>Optional[Dict[int, float]]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates the same output with the same seed. If seed is not set, the seed used in decoder will not be deterministic, thus the generated random noise will not be deterministic. If seed is set, the generated random noise will be deterministic.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>TextGenerationResponse</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>TextGenerationResponse</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>def predict_streaming(\n    self,\n    prompt: str,\n    *,\n    max_output_tokens: int = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None,\n) -&gt; Iterator[TextGenerationResponse]:\n    \"\"\"Gets a streaming model response for a single prompt.\n\n    The result is a stream (generator) of partial responses.\n\n    Args:\n        prompt: Question to ask the model.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities\n            at each generation step. The chosen tokens and their log probabilities at each step are always\n            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.\n            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log\n            probabilities are returned.\n            The maximum value for `logprobs` is 5.\n        presence_penalty:\n            Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics.\n            Range: [-2.0, 2.0]\n        frequency_penalty:\n            Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content.\n            Range: [-2.0, 2.0]\n        logit_bias:\n            Mapping from token IDs (integers) to their bias values (floats).\n            The bias values are added to the logits before sampling.\n            Larger positive bias increases the probability of choosing the token.\n            Smaller negative bias decreases the probability of choosing the token.\n            Range: [-100.0, 100.0]\n        seed:\n            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to\n            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates\n            the same output with the same seed. If seed is not set, the seed used in decoder will not be\n            deterministic, thus the generated random noise will not be deterministic. If seed is set, the\n            generated random noise will be deterministic.\n\n    Yields:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    prediction_request = _create_text_generation_prediction_request(\n        prompt=prompt,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n        logprobs=logprobs,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        logit_bias=logit_bias,\n        seed=seed,\n    )\n\n    prediction_service_client = self._endpoint._prediction_client\n    for (\n        prediction_dict\n    ) in _streaming_prediction.predict_stream_of_dicts_from_single_dict(\n        prediction_service_client=prediction_service_client,\n        endpoint_name=self._endpoint_name,\n        instance=prediction_request.instance,\n        parameters=prediction_request.parameters,\n    ):\n        prediction_obj = aiplatform.models.Prediction(\n            predictions=[prediction_dict],\n            deployed_model_id=\"\",\n        )\n        yield _parse_text_generation_model_response(prediction_obj)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationModel.predict_streaming_async","title":"predict_streaming_async  <code>async</code>","text":"<pre><code>predict_streaming_async(\n    prompt: str,\n    *,\n    max_output_tokens: int = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None\n) -&gt; AsyncIterator[TextGenerationResponse]\n</code></pre> <p>Asynchronously gets a streaming model response for a single prompt.</p> <p>The result is a stream (generator) of partial responses.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Question to ask the model.</p> <p> TYPE: <code>str</code> </p> <code>max_output_tokens</code> <p>Max length of the output text in tokens. Range: [1, 1024].</p> <p> TYPE: <code>int</code> DEFAULT: <code>_DEFAULT_MAX_OUTPUT_TOKENS</code> </p> <code>temperature</code> <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>Customized stop sequences to stop the decoding process.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>logprobs</code> <p>Returns the top <code>logprobs</code> most likely candidate tokens with their log probabilities at each generation step. The chosen tokens and their log probabilities at each step are always returned. The chosen token may or may not be in the top <code>logprobs</code> most likely candidates. The minimum value for <code>logprobs</code> is 0, which means only the chosen tokens and their log probabilities are returned. The maximum value for <code>logprobs</code> is 5.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>logit_bias</code> <p>Mapping from token IDs (integers) to their bias values (floats). The bias values are added to the logits before sampling. Larger positive bias increases the probability of choosing the token. Smaller negative bias decreases the probability of choosing the token. Range: [-100.0, 100.0]</p> <p> TYPE: <code>Optional[Dict[int, float]]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates the same output with the same seed. If seed is not set, the seed used in decoder will not be deterministic, thus the generated random noise will not be deterministic. If seed is set, the generated random noise will be deterministic.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>AsyncIterator[TextGenerationResponse]</code> <p>A stream of <code>TextGenerationResponse</code> objects that contain partial</p> <code>AsyncIterator[TextGenerationResponse]</code> <p>responses produced by the model.</p> Source code in <code>vertexai\\language_models\\_language_models.py</code> <pre><code>async def predict_streaming_async(\n    self,\n    prompt: str,\n    *,\n    max_output_tokens: int = _DEFAULT_MAX_OUTPUT_TOKENS,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    stop_sequences: Optional[List[str]] = None,\n    logprobs: Optional[int] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    logit_bias: Optional[Dict[int, float]] = None,\n    seed: Optional[int] = None,\n) -&gt; AsyncIterator[TextGenerationResponse]:\n    \"\"\"Asynchronously gets a streaming model response for a single prompt.\n\n    The result is a stream (generator) of partial responses.\n\n    Args:\n        prompt: Question to ask the model.\n        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].\n        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.\n        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.\n        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.\n        stop_sequences: Customized stop sequences to stop the decoding process.\n        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities\n            at each generation step. The chosen tokens and their log probabilities at each step are always\n            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.\n            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log\n            probabilities are returned.\n            The maximum value for `logprobs` is 5.\n        presence_penalty:\n            Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics.\n            Range: [-2.0, 2.0]\n        frequency_penalty:\n            Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content.\n            Range: [-2.0, 2.0]\n        logit_bias:\n            Mapping from token IDs (integers) to their bias values (floats).\n            The bias values are added to the logits before sampling.\n            Larger positive bias increases the probability of choosing the token.\n            Smaller negative bias decreases the probability of choosing the token.\n            Range: [-100.0, 100.0]\n        seed:\n            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to\n            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates\n            the same output with the same seed. If seed is not set, the seed used in decoder will not be\n            deterministic, thus the generated random noise will not be deterministic. If seed is set, the\n            generated random noise will be deterministic.\n\n    Yields:\n        A stream of `TextGenerationResponse` objects that contain partial\n        responses produced by the model.\n    \"\"\"\n    prediction_request = _create_text_generation_prediction_request(\n        prompt=prompt,\n        max_output_tokens=max_output_tokens,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        stop_sequences=stop_sequences,\n        logprobs=logprobs,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        logit_bias=logit_bias,\n        seed=seed,\n    )\n\n    prediction_service_async_client = self._endpoint._prediction_async_client\n    async for prediction_dict in _streaming_prediction.predict_stream_of_dicts_from_single_dict_async(\n        prediction_service_async_client=prediction_service_async_client,\n        endpoint_name=self._endpoint_name,\n        instance=prediction_request.instance,\n        parameters=prediction_request.parameters,\n    ):\n        prediction_obj = aiplatform.models.Prediction(\n            predictions=[prediction_dict],\n            deployed_model_id=\"\",\n        )\n        yield _parse_text_generation_model_response(prediction_obj)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse","title":"TextGenerationResponse  <code>dataclass</code>","text":"<pre><code>TextGenerationResponse(\n    __module__=\"vertexai.language_models\",\n    text: str,\n    _prediction_response: Any,\n    is_blocked: bool = False,\n    errors: Tuple[int] = tuple(),\n    safety_attributes: Dict[str, float] = dict(),\n    grounding_metadata: Optional[GroundingMetadata] = None,\n)\n</code></pre> <p>TextGenerationResponse represents a response of a language model. Attributes:     text: The generated text     is_blocked: Whether the the request was blocked.     errors: The error codes indicate why the response was blocked.         Learn more information about safety errors here:         this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors     safety_attributes: Scores for safety attributes.         Learn more about the safety attributes here:         https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions     grounding_metadata: Metadata for grounding.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse.is_blocked","title":"is_blocked  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_blocked: bool = False\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse.errors","title":"errors  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>errors: Tuple[int] = tuple()\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse.safety_attributes","title":"safety_attributes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>safety_attributes: Dict[str, float] = field(\n    default_factory=dict\n)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse.grounding_metadata","title":"grounding_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>grounding_metadata: Optional[GroundingMetadata] = None\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.TextGenerationResponse.raw_prediction_response","title":"raw_prediction_response  <code>property</code>","text":"<pre><code>raw_prediction_response: Prediction\n</code></pre> <p>Raw prediction response.</p>"},{"location":"vertexai/language_models/#vertexai.language_models.GroundingSource","title":"GroundingSource  <code>dataclass</code>","text":"<pre><code>GroundingSource(\n    WebSearch=WebSearch,\n    VertexAISearch=VertexAISearch,\n    InlineContext=InlineContext,\n)\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.GroundingSource.WebSearch","title":"WebSearch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WebSearch = WebSearch\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.GroundingSource.VertexAISearch","title":"VertexAISearch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VertexAISearch = VertexAISearch\n</code></pre>"},{"location":"vertexai/language_models/#vertexai.language_models.GroundingSource.InlineContext","title":"InlineContext  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>InlineContext = InlineContext\n</code></pre>"},{"location":"vertexai/vision_models/","title":"Vision models","text":""},{"location":"vertexai/vision_models/#vertexai.vision_models","title":"vertexai.vision_models","text":"<p>Classes for working with vision models.</p>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image","title":"Image","text":"<pre><code>Image(\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> <p>Image.</p> PARAMETER DESCRIPTION <code>image_bytes</code> <p>Image file bytes. Image can be in PNG or JPEG format.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(image_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either image_bytes or gcs_uri must be provided.\")\n\n    self._image_bytes = image_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(location)\n    if (\n        parsed_url.scheme == \"https\"\n        and parsed_url.netloc == \"storage.googleapis.com\"\n    ):\n        parsed_url = parsed_url._replace(\n            scheme=\"gs\", netloc=\"\", path=f\"/{urllib.parse.unquote(parsed_url.path)}\"\n        )\n        location = urllib.parse.urlunparse(parsed_url)\n\n    if parsed_url.scheme == \"gs\":\n        return Image(gcs_uri=location)\n\n    # Load image from local path\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image(image_bytes=image_bytes)\n    return image\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image.show","title":"show","text":"<pre><code>show()\n</code></pre> <p>Shows the image.</p> <p>This method only works when in a notebook environment.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def show(self):\n    \"\"\"Shows the image.\n\n    This method only works when in a notebook environment.\n    \"\"\"\n    if PIL_Image and IPython_display:\n        IPython_display.display(self._pil_image)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Image.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves image to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the image.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves image to a file.\n\n    Args:\n        location: Local path where to save the image.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._image_bytes)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageCaptioningModel","title":"ImageCaptioningModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates captions from image.</p> <p>Examples::</p> <pre><code>model = ImageCaptioningModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageCaptioningModel.get_captions","title":"get_captions","text":"<pre><code>get_captions(\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None\n) -&gt; List[str]\n</code></pre> <p>Generates captions for a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>language</code> <p>Language to use for captions. Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the captioned images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of image caption strings.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_captions(\n    self,\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None,\n) -&gt; List[str]:\n    \"\"\"Generates captions for a given image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        number_of_results: Number of captions to produce. Range: 1-3.\n        language: Language to use for captions.\n            Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n        output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n    Returns:\n        A list of image caption strings.\n    \"\"\"\n    instance = {}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n        \"language\": language,\n    }\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageQnAModel","title":"ImageQnAModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Answers questions about an image.</p> <p>Examples::</p> <pre><code>model = ImageQnAModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageQnAModel.ask_question","title":"ask_question","text":"<pre><code>ask_question(\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1\n) -&gt; List[str]\n</code></pre> <p>Answers questions about an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>question</code> <p>Question to ask about the image.</p> <p> TYPE: <code>str</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of answers.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def ask_question(\n    self,\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1,\n) -&gt; List[str]:\n    \"\"\"Answers questions about an image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        question: Question to ask about the image.\n        number_of_results: Number of captions to produce. Range: 1-3.\n\n    Returns:\n        A list of answers.\n    \"\"\"\n    instance = {\"prompt\": question}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n    }\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageTextModel","title":"ImageTextModel","text":"<p>               Bases: <code>ImageCaptioningModel</code>, <code>ImageQnAModel</code></p> <p>Generates text from images.</p> <p>Examples::</p> <pre><code>model = ImageTextModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\n\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageTextModel.ask_question","title":"ask_question","text":"<pre><code>ask_question(\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1\n) -&gt; List[str]\n</code></pre> <p>Answers questions about an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>question</code> <p>Question to ask about the image.</p> <p> TYPE: <code>str</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of answers.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def ask_question(\n    self,\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1,\n) -&gt; List[str]:\n    \"\"\"Answers questions about an image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        question: Question to ask about the image.\n        number_of_results: Number of captions to produce. Range: 1-3.\n\n    Returns:\n        A list of answers.\n    \"\"\"\n    instance = {\"prompt\": question}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n    }\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.ImageTextModel.get_captions","title":"get_captions","text":"<pre><code>get_captions(\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None\n) -&gt; List[str]\n</code></pre> <p>Generates captions for a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>language</code> <p>Language to use for captions. Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the captioned images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of image caption strings.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_captions(\n    self,\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None,\n) -&gt; List[str]:\n    \"\"\"Generates captions for a given image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        number_of_results: Number of captions to produce. Range: 1-3.\n        language: Language to use for captions.\n            Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n        output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n    Returns:\n        A list of image caption strings.\n    \"\"\"\n    instance = {}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n        \"language\": language,\n    }\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingModel","title":"MultiModalEmbeddingModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates embedding vectors from images and videos.</p> <p>Examples::</p> <pre><code>model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\nimage = Image.load_from_file(\"image.png\")\nvideo = Video.load_from_file(\"video.mp4\")\n\nembeddings = model.get_embeddings(\n    image=image,\n    video=video,\n    contextual_text=\"Hello world\",\n)\nimage_embedding = embeddings.image_embedding\nvideo_embeddings = embeddings.video_embeddings\ntext_embedding = embeddings.text_embedding\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[\n        VideoSegmentConfig\n    ] = None,\n) -&gt; MultiModalEmbeddingResponse\n</code></pre> <p>Gets embedding vectors from the provided image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Optional. The image to generate embeddings for. One of <code>image</code>, <code>video</code>, or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Image</code> DEFAULT: <code>None</code> </p> <code>video</code> <p>Optional. The video to generate embeddings for. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Video</code> DEFAULT: <code>None</code> </p> <code>contextual_text</code> <p>Optional. Contextual text for your input image or video. If provided, the model will also generate an embedding vector for the provided contextual text. The returned image and text embedding vectors are in the same semantic space with the same dimensionality, and the vectors can be used interchangeably for use cases like searching image by text or searching text by image. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>dimension</code> <p>Optional. The number of embedding dimensions. Lower values offer decreased latency when using these embeddings for subsequent tasks, while higher values offer better accuracy. Available values: <code>128</code>, <code>256</code>, <code>512</code>, and <code>1408</code> (default).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>video_segment_config</code> <p>Optional. The specific video segments (in seconds) the embeddings are generated for.</p> <p> TYPE: <code>VideoSegmentConfig</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiModalEmbeddingResponse</code> <p>The image and text embedding vectors.</p> <p> TYPE: <code>MultiModalEmbeddingResponse</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_embeddings(\n    self,\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[VideoSegmentConfig] = None,\n) -&gt; \"MultiModalEmbeddingResponse\":\n    \"\"\"Gets embedding vectors from the provided image.\n\n    Args:\n        image (Image): Optional. The image to generate embeddings for. One of\n          `image`, `video`, or `contextual_text` is required.\n        video (Video): Optional. The video to generate embeddings for. One of\n          `image`, `video` or `contextual_text` is required.\n        contextual_text (str): Optional. Contextual text for your input image or video.\n          If provided, the model will also generate an embedding vector for the\n          provided contextual text. The returned image and text embedding\n          vectors are in the same semantic space with the same dimensionality,\n          and the vectors can be used interchangeably for use cases like\n          searching image by text or searching text by image. One of `image`, `video` or\n          `contextual_text` is required.\n        dimension (int): Optional. The number of embedding dimensions. Lower\n          values offer decreased latency when using these embeddings for\n          subsequent tasks, while higher values offer better accuracy.\n          Available values: `128`, `256`, `512`, and `1408` (default).\n        video_segment_config (VideoSegmentConfig): Optional. The specific\n          video segments (in seconds) the embeddings are generated for.\n\n    Returns:\n        MultiModalEmbeddingResponse:\n            The image and text embedding vectors.\n    \"\"\"\n\n    if not image and not video and not contextual_text:\n        raise ValueError(\n            \"One of `image`, `video`, or `contextual_text` is required.\"\n        )\n\n    instance = {}\n\n    if image:\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n\n    if video:\n        if video._gcs_uri:  # pylint: disable=protected-access\n            instance[\"video\"] = {\n                \"gcsUri\": video._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"video\"] = {\n                \"bytesBase64Encoded\": video._as_base64_string()  # pylint: disable=protected-access\n            }  # pylint: disable=protected-access\n\n        if video_segment_config:\n            instance[\"video\"][\"videoSegmentConfig\"] = {\n                \"startOffsetSec\": video_segment_config.start_offset_sec,\n                \"endOffsetSec\": video_segment_config.end_offset_sec,\n                \"intervalSec\": video_segment_config.interval_sec,\n            }\n\n    if contextual_text:\n        instance[\"text\"] = contextual_text\n\n    parameters = {}\n    if dimension:\n        parameters[\"dimension\"] = dimension\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    image_embedding = response.predictions[0].get(\"imageEmbedding\")\n    video_embeddings = []\n    for video_embedding in response.predictions[0].get(\"videoEmbeddings\", []):\n        video_embeddings.append(\n            VideoEmbedding(\n                embedding=video_embedding[\"embedding\"],\n                start_offset_sec=video_embedding[\"startOffsetSec\"],\n                end_offset_sec=video_embedding[\"endOffsetSec\"],\n            )\n        )\n    text_embedding = (\n        response.predictions[0].get(\"textEmbedding\")\n        if \"textEmbedding\" in response.predictions[0]\n        else None\n    )\n    return MultiModalEmbeddingResponse(\n        image_embedding=image_embedding,\n        video_embeddings=video_embeddings,\n        _prediction_response=response,\n        text_embedding=text_embedding,\n    )\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingResponse","title":"MultiModalEmbeddingResponse  <code>dataclass</code>","text":"<pre><code>MultiModalEmbeddingResponse(\n    __module__=\"vertexai.vision_models\",\n    _prediction_response: Any,\n    image_embedding: Optional[List[float]] = None,\n    video_embeddings: Optional[List[VideoEmbedding]] = None,\n    text_embedding: Optional[List[float]] = None,\n)\n</code></pre> <p>The multimodal embedding response.</p> ATTRIBUTE DESCRIPTION <code>image_embedding</code> <p>Optional. The embedding vector generated from your image.</p> <p> TYPE: <code>List[float]</code> </p> <code>video_embeddings</code> <p>Optional. The embedding vectors generated from your video.</p> <p> TYPE: <code>List[VideoEmbedding]</code> </p> <code>text_embedding</code> <p>Optional. The embedding vector generated from the contextual text provided for your image or video.</p> <p> TYPE: <code>List[float]</code> </p>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingResponse.image_embedding","title":"image_embedding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_embedding: Optional[List[float]] = None\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingResponse.video_embeddings","title":"video_embeddings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>video_embeddings: Optional[List[VideoEmbedding]] = None\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.MultiModalEmbeddingResponse.text_embedding","title":"text_embedding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_embedding: Optional[List[float]] = None\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Video","title":"Video","text":"<pre><code>Video(\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> <p>Video.</p> PARAMETER DESCRIPTION <code>video_bytes</code> <p>Video file bytes. Video can be in AVI, FLV, MKV, MOV, MP4, MPEG, MPG, WEBM, and WMV formats.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        video_bytes: Video file bytes. Video can be in AVI, FLV, MKV, MOV,\n            MP4, MPEG, MPG, WEBM, and WMV formats.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(video_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either video_bytes or gcs_uri must be provided.\")\n\n    self._video_bytes = video_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Video.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Video\n</code></pre> <p>Loads video from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the video.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Video</code> <p>Loaded video as an <code>Video</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Video\":\n    \"\"\"Loads video from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the video.\n\n    Returns:\n        Loaded video as an `Video` object.\n    \"\"\"\n    if location.startswith(\"gs://\"):\n        return Video(gcs_uri=location)\n\n    video_bytes = pathlib.Path(location).read_bytes()\n    video = Video(video_bytes=video_bytes)\n    return video\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.Video.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves video to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the video.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves video to a file.\n\n    Args:\n        location: Local path where to save the video.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._video_bytes)\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoEmbedding","title":"VideoEmbedding","text":"<pre><code>VideoEmbedding(\n    start_offset_sec: int,\n    end_offset_sec: int,\n    embedding: List[float],\n)\n</code></pre> <p>Embeddings generated from video with offset times.</p> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>embedding</code> <p>Generated embedding for interval.</p> <p> TYPE: <code>List[float]</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self, start_offset_sec: int, end_offset_sec: int, embedding: List[float]\n):\n    \"\"\"Creates a `VideoEmbedding` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) of generated embeddings.\n        end_offset_sec: End time offset (in seconds) of generated embeddings.\n        embedding: Generated embedding for interval.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.embedding = embedding\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoEmbedding.start_offset_sec","title":"start_offset_sec  <code>instance-attribute</code>","text":"<pre><code>start_offset_sec: int = start_offset_sec\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoEmbedding.end_offset_sec","title":"end_offset_sec  <code>instance-attribute</code>","text":"<pre><code>end_offset_sec: int = end_offset_sec\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoEmbedding.embedding","title":"embedding  <code>instance-attribute</code>","text":"<pre><code>embedding: List[float] = embedding\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoSegmentConfig","title":"VideoSegmentConfig","text":"<pre><code>VideoSegmentConfig(\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n)\n</code></pre> <p>The specific video segments (in seconds) the embeddings are generated for.</p> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>120</code> </p> <code>interval_sec</code> <p>Interval to divide video for generated embeddings.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n):\n    \"\"\"Creates a `VideoSegmentConfig` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) to generate embeddings for.\n        end_offset_sec: End time offset (in seconds) to generate embeddings for.\n        interval_sec: Interval to divide video for generated embeddings.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.interval_sec = interval_sec\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoSegmentConfig.start_offset_sec","title":"start_offset_sec  <code>instance-attribute</code>","text":"<pre><code>start_offset_sec: int = start_offset_sec\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoSegmentConfig.end_offset_sec","title":"end_offset_sec  <code>instance-attribute</code>","text":"<pre><code>end_offset_sec: int = end_offset_sec\n</code></pre>"},{"location":"vertexai/vision_models/#vertexai.vision_models.VideoSegmentConfig.interval_sec","title":"interval_sec  <code>instance-attribute</code>","text":"<pre><code>interval_sec: int = interval_sec\n</code></pre>"},{"location":"vertexai/preview/","title":"vertexai.preview","text":""},{"location":"vertexai/preview/#vertexai.preview","title":"vertexai.preview","text":""},{"location":"vertexai/preview/#vertexai.preview.global_config","title":"global_config  <code>module-attribute</code>","text":"<pre><code>global_config = global_config\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.init","title":"init  <code>module-attribute</code>","text":"<pre><code>init = init\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.remote","title":"remote  <code>module-attribute</code>","text":"<pre><code>remote = remote\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.VertexModel","title":"VertexModel  <code>module-attribute</code>","text":"<pre><code>VertexModel = VertexModel\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.register","title":"register  <code>module-attribute</code>","text":"<pre><code>register = register\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.from_pretrained","title":"from_pretrained  <code>module-attribute</code>","text":"<pre><code>from_pretrained = from_pretrained\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.start_run","title":"start_run  <code>module-attribute</code>","text":"<pre><code>start_run = start_run\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.end_run","title":"end_run  <code>module-attribute</code>","text":"<pre><code>end_run = end_run\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.get_experiment_df","title":"get_experiment_df  <code>module-attribute</code>","text":"<pre><code>get_experiment_df = get_experiment_df\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.log_params","title":"log_params  <code>module-attribute</code>","text":"<pre><code>log_params = log_params\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.log_metrics","title":"log_metrics  <code>module-attribute</code>","text":"<pre><code>log_metrics = log_metrics\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.log_time_series_metrics","title":"log_time_series_metrics  <code>module-attribute</code>","text":"<pre><code>log_time_series_metrics = log_time_series_metrics\n</code></pre>"},{"location":"vertexai/preview/#vertexai.preview.log_classification_metrics","title":"log_classification_metrics  <code>module-attribute</code>","text":"<pre><code>log_classification_metrics = log_classification_metrics\n</code></pre>"},{"location":"vertexai/preview/batch_prediction/","title":"Batch prediction","text":""},{"location":"vertexai/preview/batch_prediction/#vertexai.preview.batch_prediction","title":"vertexai.preview.batch_prediction","text":"<p>Classes for batch prediction.</p>"},{"location":"vertexai/preview/evaluation/","title":"Evaluation","text":""},{"location":"vertexai/preview/evaluation/#vertexai.preview.evaluation","title":"vertexai.preview.evaluation","text":"<p>GenAI Rapid Evaluation Module.</p>"},{"location":"vertexai/preview/evaluation/#vertexai.preview.evaluation.EvalResult","title":"EvalResult  <code>module-attribute</code>","text":"<pre><code>EvalResult = EvalResult\n</code></pre>"},{"location":"vertexai/preview/evaluation/#vertexai.preview.evaluation.EvalTask","title":"EvalTask  <code>module-attribute</code>","text":"<pre><code>EvalTask = EvalTask\n</code></pre>"},{"location":"vertexai/preview/evaluation/#vertexai.preview.evaluation.CustomMetric","title":"CustomMetric  <code>module-attribute</code>","text":"<pre><code>CustomMetric = CustomMetric\n</code></pre>"},{"location":"vertexai/preview/evaluation/#vertexai.preview.evaluation.PairwiseMetric","title":"PairwiseMetric  <code>module-attribute</code>","text":"<pre><code>PairwiseMetric = PairwiseMetric\n</code></pre>"},{"location":"vertexai/preview/evaluation/#vertexai.preview.evaluation.make_metric","title":"make_metric  <code>module-attribute</code>","text":"<pre><code>make_metric = make_metric\n</code></pre>"},{"location":"vertexai/preview/evaluation/#vertexai.preview.evaluation.PromptTemplate","title":"PromptTemplate  <code>module-attribute</code>","text":"<pre><code>PromptTemplate = PromptTemplate\n</code></pre>"},{"location":"vertexai/preview/generative_models/","title":"Generative models","text":""},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models","title":"vertexai.preview.generative_models","text":"<p>Classes for working with the Gemini models.</p>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.FinishReason","title":"FinishReason  <code>module-attribute</code>","text":"<pre><code>FinishReason = FinishReason\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.HarmCategory","title":"HarmCategory  <code>module-attribute</code>","text":"<pre><code>HarmCategory = HarmCategory\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.HarmBlockThreshold","title":"HarmBlockThreshold  <code>module-attribute</code>","text":"<pre><code>HarmBlockThreshold = HarmBlockThreshold\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding","title":"grounding","text":"<pre><code>grounding()\n</code></pre> <p>Grounding namespace (preview).</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raise RuntimeError(\"This class must not be instantiated.\")\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.Retrieval","title":"Retrieval","text":"<pre><code>Retrieval(\n    source: Union[VertexAISearch],\n    disable_attribution: Optional[bool] = None,\n)\n</code></pre> <p>Defines a retrieval tool that model can call to access external knowledge.</p> PARAMETER DESCRIPTION <code>source</code> <p>Set to use data source powered by Vertex AI Search.</p> <p> TYPE: <code>VertexAISearch</code> </p> <code>disable_attribution</code> <p>Optional. Disable using the result from this tool in detecting grounding attribution. This does not affect how the result is given to the model for generation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    source: Union[\"grounding.VertexAISearch\"],\n    disable_attribution: Optional[bool] = None,\n):\n    \"\"\"Initializes a Retrieval tool.\n\n    Args:\n        source (VertexAISearch):\n            Set to use data source powered by Vertex AI Search.\n        disable_attribution (bool):\n            Optional. Disable using the result from this\n            tool in detecting grounding attribution. This\n            does not affect how the result is given to the\n            model for generation.\n    \"\"\"\n    self._raw_retrieval = gapic_tool_types.Retrieval(\n        vertex_ai_search=source._raw_vertex_ai_search,\n        disable_attribution=disable_attribution,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.VertexAISearch","title":"VertexAISearch","text":"<pre><code>VertexAISearch(datastore: str)\n</code></pre> <p>Retrieve from Vertex AI Search datastore for grounding. See https://cloud.google.com/vertex-ai-search-and-conversation</p> PARAMETER DESCRIPTION <code>datastore</code> <p>Required. Fully-qualified Vertex AI Search's datastore resource ID. projects/&lt;&gt;/locations/&lt;&gt;/collections/&lt;&gt;/dataStores/&lt;&gt;</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    datastore: str,\n):\n    \"\"\"Initializes a Vertex AI Search tool.\n\n    Args:\n        datastore (str):\n            Required. Fully-qualified Vertex AI Search's\n            datastore resource ID.\n            projects/&lt;&gt;/locations/&lt;&gt;/collections/&lt;&gt;/dataStores/&lt;&gt;\n    \"\"\"\n    self._raw_vertex_ai_search = gapic_tool_types.VertexAISearch(\n        datastore=datastore,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.grounding.GoogleSearchRetrieval","title":"GoogleSearchRetrieval","text":"<pre><code>GoogleSearchRetrieval(\n    disable_attribution: Optional[bool] = None,\n)\n</code></pre> <p>Tool to retrieve public web data for grounding, powered by Google Search.</p> ATTRIBUTE DESCRIPTION <code>disable_attribution</code> <p>Optional. Disable using the result from this tool in detecting grounding attribution. This does not affect how the result is given to the model for generation.</p> <p> TYPE: <code>bool</code> </p> PARAMETER DESCRIPTION <code>disable_attribution</code> <p>Optional. Disable using the result from this tool in detecting grounding attribution. This does not affect how the result is given to the model for generation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    disable_attribution: Optional[bool] = None,\n):\n    \"\"\"Initializes a Google Search Retrieval tool.\n\n    Args:\n        disable_attribution (bool):\n            Optional. Disable using the result from this\n            tool in detecting grounding attribution. This\n            does not affect how the result is given to the\n            model for generation.\n    \"\"\"\n    self._raw_google_search_retrieval = gapic_tool_types.GoogleSearchRetrieval(\n        disable_attribution=disable_attribution,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationConfig","title":"GenerationConfig","text":"<pre><code>GenerationConfig(\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None\n)\n</code></pre> <p>Parameters for the generation.</p> PARAMETER DESCRIPTION <code>temperature</code> <p>Controls the randomness of predictions. Range: [0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_p</code> <p>If specified, nucleus sampling will be used. Range: (0.0, 1.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>top_k</code> <p>If specified, top-k sampling will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>candidate_count</code> <p>Number of candidates to generate.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>max_output_tokens</code> <p>The maximum number of output tokens to generate per message.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stop_sequences</code> <p>A list of stop sequences.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>presence_penalty</code> <p>Positive values penalize tokens that have appeared in the generated text, thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>frequency_penalty</code> <p>Positive values penalize tokens that repeatedly appear in the generated text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>response_mime_type</code> <p>Output response mimetype of the generated candidate text. Supported mimetypes:</p> <ul> <li><code>text/plain</code>: (default) Text output.</li> <li><code>application/json</code>: JSON response in the candidates.</li> </ul> <p>The model needs to be prompted to output the appropriate response type, otherwise the behavior is undefined.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>response_schema</code> <p>Output response schema of the genreated candidate text. Only valid when response_mime_type is application/json.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Usage <pre><code>response = model.generate_content(\n    \"Why is sky blue?\",\n    generation_config=GenerationConfig(\n        temperature=0.1,\n        top_p=0.95,\n        top_k=20,\n        candidate_count=1,\n        max_output_tokens=100,\n        stop_sequences=[\"\\n\\n\\n\"],\n    )\n)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    top_k: Optional[int] = None,\n    candidate_count: Optional[int] = None,\n    max_output_tokens: Optional[int] = None,\n    stop_sequences: Optional[List[str]] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    response_mime_type: Optional[str] = None,\n    response_schema: Optional[Dict[str, Any]] = None,\n):\n    r\"\"\"Constructs a GenerationConfig object.\n\n    Args:\n        temperature: Controls the randomness of predictions. Range: [0.0, 1.0]\n        top_p: If specified, nucleus sampling will be used. Range: (0.0, 1.0]\n        top_k: If specified, top-k sampling will be used.\n        candidate_count: Number of candidates to generate.\n        max_output_tokens: The maximum number of output tokens to generate per message.\n        stop_sequences: A list of stop sequences.\n        presence_penalty: Positive values penalize tokens that have appeared in the generated text,\n            thus increasing the possibility of generating more diversed topics. Range: [-2.0, 2.0]\n        frequency_penalty: Positive values penalize tokens that repeatedly appear in the generated\n            text, thus decreasing the possibility of repeating the same content. Range: [-2.0, 2.0]\n        response_mime_type: Output response mimetype of the generated\n            candidate text. Supported mimetypes:\n\n            -  ``text/plain``: (default) Text output.\n            -  ``application/json``: JSON response in the candidates.\n\n            The model needs to be prompted to output the appropriate\n            response type, otherwise the behavior is undefined.\n        response_schema: Output response schema of the genreated candidate text. Only valid when\n            response_mime_type is application/json.\n\n    Usage:\n        ```\n        response = model.generate_content(\n            \"Why is sky blue?\",\n            generation_config=GenerationConfig(\n                temperature=0.1,\n                top_p=0.95,\n                top_k=20,\n                candidate_count=1,\n                max_output_tokens=100,\n                stop_sequences=[\"\\n\\n\\n\"],\n            )\n        )\n        ```\n    \"\"\"\n    if response_schema is None:\n        raw_schema = None\n    else:\n        gapic_schema_dict = _convert_schema_dict_to_gapic(response_schema)\n        raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_generation_config = gapic_content_types.GenerationConfig(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        candidate_count=candidate_count,\n        max_output_tokens=max_output_tokens,\n        stop_sequences=stop_sequences,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        response_mime_type=response_mime_type,\n        response_schema=raw_schema,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    generation_config_dict: Dict[str, Any]\n) -&gt; GenerationConfig\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, generation_config_dict: Dict[str, Any]) -&gt; \"GenerationConfig\":\n    raw_generation_config = gapic_content_types.GenerationConfig(\n        generation_config_dict\n    )\n    return cls._from_gapic(raw_generation_config=raw_generation_config)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_generation_config)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse","title":"GenerationResponse","text":"<pre><code>GenerationResponse()\n</code></pre> <p>The response from the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raw_response = gapic_prediction_service_types.GenerateContentResponse()\n    self._raw_response = raw_response\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse.candidates","title":"candidates  <code>property</code>","text":"<pre><code>candidates: List[Candidate]\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse.prompt_feedback","title":"prompt_feedback  <code>property</code>","text":"<pre><code>prompt_feedback: PromptFeedback\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse.usage_metadata","title":"usage_metadata  <code>property</code>","text":"<pre><code>usage_metadata: UsageMetadata\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    response_dict: Dict[str, Any]\n) -&gt; GenerationResponse\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, response_dict: Dict[str, Any]) -&gt; \"GenerationResponse\":\n    raw_response = gapic_prediction_service_types.GenerateContentResponse()\n    json_format.ParseDict(response_dict, raw_response._pb)\n    return cls._from_gapic(raw_response=raw_response)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerationResponse.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_response)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.AutomaticFunctionCallingResponder","title":"AutomaticFunctionCallingResponder","text":"<pre><code>AutomaticFunctionCallingResponder(\n    max_automatic_function_calls: int = 1,\n)\n</code></pre> <p>Responder that automatically responds to model's function calls.</p> PARAMETER DESCRIPTION <code>max_automatic_function_calls</code> <p>Maximum number of automatic function calls.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self, max_automatic_function_calls: int = 1):\n    \"\"\"Initializes the responder.\n\n    Args:\n        max_automatic_function_calls: Maximum number of automatic function calls.\n    \"\"\"\n    if not (max_automatic_function_calls &gt; 0):\n        raise ValueError(\"max_automatic_function_calls must be positive.\")\n    self._max_automatic_function_calls = max_automatic_function_calls\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.CallableFunctionDeclaration","title":"CallableFunctionDeclaration","text":"<pre><code>CallableFunctionDeclaration(\n    name: str,\n    function: Callable[..., Any],\n    parameters: Dict[str, Any],\n    description: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>FunctionDeclaration</code></p> <p>A function declaration plus a function.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    function: Callable[..., Any],\n    parameters: Dict[str, Any],\n    description: Optional[str] = None,\n):\n    super().__init__(\n        name=name,\n        description=description,\n        parameters=parameters,\n    )\n    self._function = function\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.CallableFunctionDeclaration.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_function_declaration)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.CallableFunctionDeclaration.from_func","title":"from_func  <code>classmethod</code>","text":"<pre><code>from_func(\n    func: Callable[..., Any]\n) -&gt; CallableFunctionDeclaration\n</code></pre> <p>Automatically creates a CallableFunctionDeclaration from a Python function.</p> <p>The function parameter schema is automatically extracted. Args:   func: The function from which to extract schema.</p> RETURNS DESCRIPTION <code>CallableFunctionDeclaration</code> <p>CallableFunctionDeclaration.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n    \"\"\"Automatically creates a CallableFunctionDeclaration from a Python function.\n\n    The function parameter schema is automatically extracted.\n    Args:\n      func: The function from which to extract schema.\n\n    Returns:\n        CallableFunctionDeclaration.\n    \"\"\"\n    from vertexai.generative_models import (\n        _function_calling_utils,\n    )\n\n    function_schema = _function_calling_utils.generate_json_schema_from_function(\n        func\n    )\n    # Getting out the description first since it will be removed from the schema.\n    function_description = function_schema[\"description\"]\n    function_schema = (\n        _function_calling_utils.adapt_json_schema_to_google_tool_schema(\n            function_schema\n        )\n    )\n\n    return CallableFunctionDeclaration(\n        name=func.__name__,\n        function=func,\n        description=function_description,\n        parameters=function_schema,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate","title":"Candidate","text":"<pre><code>Candidate()\n</code></pre> <p>A response candidate generated by the model.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raw_candidate = gapic_content_types.Candidate()\n    self._raw_candidate = raw_candidate\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.content","title":"content  <code>property</code>","text":"<pre><code>content: Content\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.finish_reason","title":"finish_reason  <code>property</code>","text":"<pre><code>finish_reason: FinishReason\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.finish_message","title":"finish_message  <code>property</code>","text":"<pre><code>finish_message: str\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.index","title":"index  <code>property</code>","text":"<pre><code>index: int\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.safety_ratings","title":"safety_ratings  <code>property</code>","text":"<pre><code>safety_ratings: Sequence[SafetyRating]\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.citation_metadata","title":"citation_metadata  <code>property</code>","text":"<pre><code>citation_metadata: CitationMetadata\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.grounding_metadata","title":"grounding_metadata  <code>property</code>","text":"<pre><code>grounding_metadata: GroundingMetadata\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.function_calls","title":"function_calls  <code>property</code>","text":"<pre><code>function_calls: Sequence[FunctionCall]\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(candidate_dict: Dict[str, Any]) -&gt; Candidate\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, candidate_dict: Dict[str, Any]) -&gt; \"Candidate\":\n    raw_candidate = gapic_content_types.Candidate()\n    json_format.ParseDict(candidate_dict, raw_candidate._pb)\n    return cls._from_gapic(raw_candidate=raw_candidate)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Candidate.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_candidate)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Content","title":"Content","text":"<pre><code>Content(\n    *, parts: List[Part] = None, role: Optional[str] = None\n)\n</code></pre> <p>The multi-part content of a message.</p> Usage <pre><code>response = model.generate_content(contents=[\n    Content(role=\"user\", parts=[Part.from_text(\"Why is sky blue?\")])\n])\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    parts: List[\"Part\"] = None,\n    role: Optional[str] = None,\n):\n    raw_parts = [part._raw_part for part in parts or []]\n    self._raw_content = gapic_content_types.Content(parts=raw_parts, role=role)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Content.parts","title":"parts  <code>property</code>","text":"<pre><code>parts: List[Part]\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Content.role","title":"role  <code>property</code> <code>writable</code>","text":"<pre><code>role: str\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Content.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Content.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(content_dict: Dict[str, Any]) -&gt; Content\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, content_dict: Dict[str, Any]) -&gt; \"Content\":\n    raw_content = gapic_content_types.Content()\n    json_format.ParseDict(content_dict, raw_content._pb)\n    return cls._from_gapic(raw_content=raw_content)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Content.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_content)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.FunctionDeclaration","title":"FunctionDeclaration","text":"<pre><code>FunctionDeclaration(\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None\n)\n</code></pre> <p>A representation of a function declaration.</p> Usage <p>Create function declaration and tool: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(\n    name=\"get_current_weather\",\n    description=\"Get the current weather in a given location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\n                    \"celsius\",\n                    \"fahrenheit\",\n                ]\n            }\n        },\n        \"required\": [\n            \"location\"\n        ]\n    },\n)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the function that the model can call.</p> <p> TYPE: <code>str</code> </p> <code>parameters</code> <p>Describes the parameters to this function in JSON Schema Object format.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>description</code> <p>Description and purpose of the function. Model uses it to decide how and whether to call the function.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    parameters: Dict[str, Any],\n    description: Optional[str] = None,\n):\n    \"\"\"Constructs a FunctionDeclaration.\n\n    Args:\n        name: The name of the function that the model can call.\n        parameters: Describes the parameters to this function in JSON Schema Object format.\n        description: Description and purpose of the function.\n            Model uses it to decide how and whether to call the function.\n    \"\"\"\n    gapic_schema_dict = _convert_schema_dict_to_gapic(parameters)\n    raw_schema = aiplatform_types.Schema(gapic_schema_dict)\n    self._raw_function_declaration = gapic_tool_types.FunctionDeclaration(\n        name=name, description=description, parameters=raw_schema\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.FunctionDeclaration.from_func","title":"from_func  <code>classmethod</code>","text":"<pre><code>from_func(\n    func: Callable[..., Any]\n) -&gt; CallableFunctionDeclaration\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_func(cls, func: Callable[..., Any]) -&gt; \"CallableFunctionDeclaration\":\n    return CallableFunctionDeclaration.from_func(func)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.FunctionDeclaration.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_function_declaration)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image","title":"Image","text":"<p>The image that can be sent to a generative model.</p>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image.data","title":"data  <code>property</code>","text":"<pre><code>data: bytes\n</code></pre> <p>Returns the image data.</p>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from file.\n\n    Args:\n        location: Local path from where to load the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image()\n    image._image_bytes = image_bytes\n    return image\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Image.from_bytes","title":"from_bytes  <code>staticmethod</code>","text":"<pre><code>from_bytes(data: bytes) -&gt; Image\n</code></pre> <p>Loads image from image bytes.</p> PARAMETER DESCRIPTION <code>data</code> <p>Image bytes.</p> <p> TYPE: <code>bytes</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_bytes(data: bytes) -&gt; \"Image\":\n    \"\"\"Loads image from image bytes.\n\n    Args:\n        data: Image bytes.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    image = Image()\n    image._image_bytes = data\n    return image\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part","title":"Part","text":"<pre><code>Part()\n</code></pre> <p>A part of a multi-part Content message.</p> Usage <pre><code>text_part = Part.from_text(\"Why is sky blue?\")\nimage_part = Part.from_image(Image.load_from_file(\"image.jpg\"))\nvideo_part = Part.from_uri(uri=\"gs://.../video.mp4\", mime_type=\"video/mp4\")\nfunction_response_part = Part.from_function_response(\n    name=\"get_current_weather\",\n    response={\n        \"content\": {\"weather_there\": \"super nice\"},\n    }\n)\n\nresponse1 = model.generate_content([text_part, image_part])\nresponse2 = model.generate_content(video_part)\nresponse3 = chat.send_message(function_response_part)\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self):\n    raw_part = gapic_content_types.Part()\n    self._raw_part = raw_part\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.mime_type","title":"mime_type  <code>property</code>","text":"<pre><code>mime_type: Optional[str]\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.inline_data","title":"inline_data  <code>property</code>","text":"<pre><code>inline_data: Blob\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.file_data","title":"file_data  <code>property</code>","text":"<pre><code>file_data: FileData\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.function_call","title":"function_call  <code>property</code>","text":"<pre><code>function_call: FunctionCall\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.function_response","title":"function_response  <code>property</code>","text":"<pre><code>function_response: FunctionResponse\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(part_dict: Dict[str, Any]) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, part_dict: Dict[str, Any]) -&gt; \"Part\":\n    raw_part = gapic_content_types.Part()\n    json_format.ParseDict(part_dict, raw_part._pb)\n    return cls._from_gapic(raw_part=raw_part)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.from_data","title":"from_data  <code>staticmethod</code>","text":"<pre><code>from_data(data: bytes, mime_type: str) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_data(data: bytes, mime_type: str) -&gt; \"Part\":\n    return Part._from_gapic(\n        raw_part=gapic_content_types.Part(\n            inline_data=gapic_content_types.Blob(data=data, mime_type=mime_type)\n        )\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.from_uri","title":"from_uri  <code>staticmethod</code>","text":"<pre><code>from_uri(uri: str, mime_type: str) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_uri(uri: str, mime_type: str) -&gt; \"Part\":\n    return Part._from_gapic(\n        raw_part=gapic_content_types.Part(\n            file_data=gapic_content_types.FileData(\n                file_uri=uri, mime_type=mime_type\n            )\n        )\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.from_text","title":"from_text  <code>staticmethod</code>","text":"<pre><code>from_text(text: str) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_text(text: str) -&gt; \"Part\":\n    return Part._from_gapic(raw_part=gapic_content_types.Part(text=text))\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.from_image","title":"from_image  <code>staticmethod</code>","text":"<pre><code>from_image(image: Image) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_image(image: \"Image\") -&gt; \"Part\":\n    return Part.from_data(data=image.data, mime_type=image._mime_type)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.from_function_response","title":"from_function_response  <code>staticmethod</code>","text":"<pre><code>from_function_response(\n    name: str, response: Dict[str, Any]\n) -&gt; Part\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@staticmethod\ndef from_function_response(name: str, response: Dict[str, Any]) -&gt; \"Part\":\n    return Part._from_gapic(\n        raw_part=gapic_content_types.Part(\n            function_response=gapic_tool_types.FunctionResponse(\n                name=name,\n                response=response,\n            )\n        )\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Part.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_part)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ResponseBlockedError","title":"ResponseBlockedError","text":"<pre><code>ResponseBlockedError(\n    message: str,\n    request_contents: List[Content],\n    responses: List[GenerationResponse],\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    request_contents: List[\"Content\"],\n    responses: List[\"GenerationResponse\"],\n):\n    super().__init__(message)\n    self.request = request_contents\n    self.responses = responses\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ResponseBlockedError.request","title":"request  <code>instance-attribute</code>","text":"<pre><code>request = request_contents\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ResponseBlockedError.responses","title":"responses  <code>instance-attribute</code>","text":"<pre><code>responses = responses\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ResponseValidationError","title":"ResponseValidationError","text":"<pre><code>ResponseValidationError(\n    message: str,\n    request_contents: List[Content],\n    responses: List[GenerationResponse],\n)\n</code></pre> <p>               Bases: <code>ResponseBlockedError</code></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    request_contents: List[\"Content\"],\n    responses: List[\"GenerationResponse\"],\n):\n    super().__init__(message)\n    self.request = request_contents\n    self.responses = responses\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting","title":"SafetySetting","text":"<pre><code>SafetySetting(\n    *,\n    category: HarmCategory,\n    threshold: HarmBlockThreshold,\n    method: Optional[HarmBlockMethod] = None\n)\n</code></pre> <p>Parameters for the generation.</p> PARAMETER DESCRIPTION <code>category</code> <p>Harm category.</p> <p> TYPE: <code>HarmCategory</code> </p> <code>threshold</code> <p>The harm block threshold.</p> <p> TYPE: <code>HarmBlockThreshold</code> </p> <code>method</code> <p>Specify if the threshold is used for probability or severity score. If not specified, the threshold is used for probability score.</p> <p> TYPE: <code>Optional[HarmBlockMethod]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    *,\n    category: \"SafetySetting.HarmCategory\",\n    threshold: \"SafetySetting.HarmBlockThreshold\",\n    method: Optional[\"SafetySetting.HarmBlockMethod\"] = None,\n):\n    r\"\"\"Safety settings.\n\n    Args:\n        category: Harm category.\n        threshold: The harm block threshold.\n        method: Specify if the threshold is used for probability or severity\n            score. If not specified, the threshold is used for probability\n            score.\n    \"\"\"\n    self._raw_safety_setting = gapic_content_types.SafetySetting(\n        category=category,\n        threshold=threshold,\n        method=method,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting.HarmCategory","title":"HarmCategory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HarmCategory = HarmCategory\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting.HarmBlockMethod","title":"HarmBlockMethod  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HarmBlockMethod = HarmBlockMethod\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting.HarmBlockThreshold","title":"HarmBlockThreshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HarmBlockThreshold = HarmBlockThreshold\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    safety_setting_dict: Dict[str, Any]\n) -&gt; SafetySetting\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, safety_setting_dict: Dict[str, Any]) -&gt; \"SafetySetting\":\n    raw_safety_setting = gapic_content_types.SafetySetting(safety_setting_dict)\n    return cls._from_gapic(raw_safety_setting=raw_safety_setting)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.SafetySetting.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_safety_setting)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Tool","title":"Tool","text":"<pre><code>Tool(function_declarations: List[FunctionDeclaration])\n</code></pre> <p>A collection of functions that the model may use to generate response.</p> Usage <p>Create tool from function declarations: <pre><code>get_current_weather_func = generative_models.FunctionDeclaration(...)\nweather_tool = generative_models.Tool(\n    function_declarations=[get_current_weather_func],\n)\n</code></pre> Use tool in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n))\n</code></pre> Use tool in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    function_declarations: List[\"FunctionDeclaration\"],\n):\n    gapic_function_declarations = [\n        function_declaration._raw_function_declaration\n        for function_declaration in function_declarations\n    ]\n    self._raw_tool = gapic_tool_types.Tool(\n        function_declarations=gapic_function_declarations\n    )\n    callable_functions = {\n        function_declaration._raw_function_declaration.name: function_declaration\n        for function_declaration in function_declarations\n        if isinstance(function_declaration, CallableFunctionDeclaration)\n    }\n    self._callable_functions = callable_functions\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Tool.from_function_declarations","title":"from_function_declarations  <code>classmethod</code>","text":"<pre><code>from_function_declarations(\n    function_declarations: List[FunctionDeclaration],\n) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_function_declarations(\n    cls,\n    function_declarations: List[\"FunctionDeclaration\"],\n) -&gt; \"Tool\":\n    return Tool(function_declarations=function_declarations)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Tool.from_retrieval","title":"from_retrieval  <code>classmethod</code>","text":"<pre><code>from_retrieval(\n    retrieval: Union[Retrieval, Retrieval]\n) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_retrieval(\n    cls,\n    retrieval: Union[\"grounding.Retrieval\", \"rag.Retrieval\"],\n) -&gt; \"Tool\":\n    raw_tool = gapic_tool_types.Tool(retrieval=retrieval._raw_retrieval)\n    return cls._from_gapic(raw_tool=raw_tool)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Tool.from_google_search_retrieval","title":"from_google_search_retrieval  <code>classmethod</code>","text":"<pre><code>from_google_search_retrieval(\n    google_search_retrieval: GoogleSearchRetrieval,\n) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_google_search_retrieval(\n    cls,\n    google_search_retrieval: \"grounding.GoogleSearchRetrieval\",\n) -&gt; \"Tool\":\n    raw_tool = gapic_tool_types.Tool(\n        google_search_retrieval=google_search_retrieval._raw_google_search_retrieval\n    )\n    return cls._from_gapic(raw_tool=raw_tool)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Tool.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(tool_dict: Dict[str, Any]) -&gt; Tool\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>@classmethod\ndef from_dict(cls, tool_dict: Dict[str, Any]) -&gt; \"Tool\":\n    tool_dict = copy.deepcopy(tool_dict)\n    function_declarations = tool_dict[\"function_declarations\"]\n    for function_declaration in function_declarations:\n        function_declaration[\"parameters\"] = _convert_schema_dict_to_gapic(\n            function_declaration[\"parameters\"]\n        )\n    raw_tool = gapic_tool_types.Tool(tool_dict)\n    return cls._from_gapic(raw_tool=raw_tool)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.Tool.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    return _proto_to_dict(self._raw_tool)\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ToolConfig","title":"ToolConfig","text":"<pre><code>ToolConfig(function_calling_config: FunctionCallingConfig)\n</code></pre> <p>Config shared for all tools provided in the request.</p> Usage <p>Create ToolConfig <pre><code>tool_config = ToolConfig(\n    function_calling_config=ToolConfig.FunctionCallingConfig(\n        mode=ToolConfig.FunctionCallingConfig.Mode.ANY,\n        allowed_function_names=[\"get_current_weather_func\"],\n))\n</code></pre> Use ToolConfig in <code>GenerativeModel.generate_content</code>: <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\n    \"What is the weather like in Boston?\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n    tool_config=tool_config,\n))\n</code></pre> Use ToolConfig in chat: <pre><code>model = GenerativeModel(\n    \"gemini-pro\",\n    # You can specify tools when creating a model to avoid having to send them with every request.\n    tools=[weather_tool],\n    tool_config=tool_config,\n)\nchat = model.start_chat()\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nprint(chat.send_message(\n    Part.from_function_response(\n        name=\"get_current_weather\",\n        response={\n            \"content\": {\"weather_there\": \"super nice\"},\n        }\n    ),\n))\n</code></pre></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(self, function_calling_config: \"ToolConfig.FunctionCallingConfig\"):\n    self._gapic_tool_config = gapic_tool_types.ToolConfig(\n        function_calling_config=function_calling_config._gapic_function_calling_config\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ToolConfig.FunctionCallingConfig","title":"FunctionCallingConfig","text":"<pre><code>FunctionCallingConfig(\n    mode: Mode,\n    allowed_function_names: Optional[List[str]] = None,\n)\n</code></pre> PARAMETER DESCRIPTION <code>mode</code> <p>Enum describing the function calling mode</p> <p> TYPE: <code>Mode</code> </p> <code>allowed_function_names</code> <p>A list of allowed function names (must match from Tool). Only set when the Mode is ANY.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    mode: \"ToolConfig.FunctionCallingConfig.Mode\",\n    allowed_function_names: Optional[List[str]] = None,\n):\n    \"\"\"Constructs FunctionCallingConfig.\n\n    Args:\n        mode: Enum describing the function calling mode\n        allowed_function_names: A list of allowed function names\n            (must match from Tool). Only set when the Mode is ANY.\n    \"\"\"\n    self._gapic_function_calling_config = (\n        gapic_tool_types.FunctionCallingConfig(\n            mode=mode,\n            allowed_function_names=allowed_function_names,\n        )\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ToolConfig.FunctionCallingConfig.Mode","title":"Mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Mode = Mode\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerativeModel","title":"GenerativeModel","text":"<pre><code>GenerativeModel(\n    model_name: str,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    tool_config: Optional[ToolConfig] = None,\n    system_instruction: Optional[PartsType] = None\n)\n</code></pre> <p>               Bases: <code>_PreviewGenerativeModel</code></p> Usage <pre><code>model = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\"Hello\"))\n</code></pre> PARAMETER DESCRIPTION <code>model_name</code> <p>Model Garden model resource name. Alternatively, a tuned model endpoint resource name can be provided.</p> <p> TYPE: <code>str</code> </p> <code>generation_config</code> <p>Default generation config to use in generate_content.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Default safety settings to use in generate_content.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>Default tools to use in generate_content.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>tool_config</code> <p>Default tool config to use in generate_content.</p> <p> TYPE: <code>Optional[ToolConfig]</code> DEFAULT: <code>None</code> </p> <code>system_instruction</code> <p>Default system instruction to use in generate_content. Note: Only text should be used in parts. Content of each part will become a separate paragraph.</p> <p> TYPE: <code>Optional[PartsType]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    tool_config: Optional[\"ToolConfig\"] = None,\n    system_instruction: Optional[PartsType] = None,\n):\n    r\"\"\"Initializes GenerativeModel.\n\n    Usage:\n        ```\n        model = GenerativeModel(\"gemini-pro\")\n        print(model.generate_content(\"Hello\"))\n        ```\n\n    Args:\n        model_name: Model Garden model resource name.\n            Alternatively, a tuned model endpoint resource name can be provided.\n        generation_config: Default generation config to use in generate_content.\n        safety_settings: Default safety settings to use in generate_content.\n        tools: Default tools to use in generate_content.\n        tool_config: Default tool config to use in generate_content.\n        system_instruction: Default system instruction to use in generate_content.\n            Note: Only text should be used in parts.\n            Content of each part will become a separate paragraph.\n    \"\"\"\n    if not model_name:\n        raise ValueError(\"model_name must not be empty\")\n    if \"/\" not in model_name:\n        model_name = \"publishers/google/models/\" + model_name\n    if model_name.startswith(\"models/\"):\n        model_name = \"publishers/google/\" + model_name\n\n    project = aiplatform_initializer.global_config.project\n    location = aiplatform_initializer.global_config.location\n\n    if model_name.startswith(\"publishers/\"):\n        prediction_resource_name = (\n            f\"projects/{project}/locations/{location}/{model_name}\"\n        )\n    elif model_name.startswith(\"projects/\"):\n        prediction_resource_name = model_name\n    else:\n        raise ValueError(\n            \"model_name must be either a Model Garden model ID or a full resource name.\"\n        )\n\n    location = aiplatform_utils.extract_project_and_location_from_parent(\n        prediction_resource_name\n    )[\"location\"]\n\n    self._model_name = model_name\n    self._prediction_resource_name = prediction_resource_name\n    self._location = location\n    self._generation_config = generation_config\n    self._safety_settings = safety_settings\n    self._tools = tools\n    self._tool_config = tool_config\n    self._system_instruction = system_instruction\n\n    # Validating the parameters\n    self._prepare_request(\n        contents=\"test\",\n        generation_config=generation_config,\n        safety_settings=safety_settings,\n        tools=tools,\n        tool_config=tool_config,\n        system_instruction=system_instruction,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerativeModel.generate_content","title":"generate_content","text":"<pre><code>generate_content(\n    contents: ContentsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    tool_config: Optional[ToolConfig] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, Iterable[GenerationResponse]\n]\n</code></pre> <p>Generates content.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>tool_config</code> <p>Config shared for all tools provided in the request.</p> <p> TYPE: <code>Optional[ToolConfig]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A stream of GenerationResponse objects if stream == True</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def generate_content(\n    self,\n    contents: ContentsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    tool_config: Optional[\"ToolConfig\"] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"],]:\n    \"\"\"Generates content.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        tool_config: Config shared for all tools provided in the request.\n        stream: Whether to stream the response.\n\n    Returns:\n        A single GenerationResponse object if stream == False\n        A stream of GenerationResponse objects if stream == True\n    \"\"\"\n    if stream:\n        # TODO(b/315810992): Surface prompt_feedback on the returned stream object\n        return self._generate_content_streaming(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n    else:\n        return self._generate_content(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerativeModel.generate_content_async","title":"generate_content_async  <code>async</code>","text":"<pre><code>generate_content_async(\n    contents: ContentsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    tool_config: Optional[ToolConfig] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, AsyncIterable[GenerationResponse]\n]\n</code></pre> <p>Generates content asynchronously.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>tool_config</code> <p>Config shared for all tools provided in the request.</p> <p> TYPE: <code>Optional[ToolConfig]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, AsyncIterable[GenerationResponse]]</code> <p>An awaitable for a single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, AsyncIterable[GenerationResponse]]</code> <p>An awaitable for a stream of GenerationResponse objects if stream == True</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>async def generate_content_async(\n    self,\n    contents: ContentsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    tool_config: Optional[\"ToolConfig\"] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", AsyncIterable[\"GenerationResponse\"],]:\n    \"\"\"Generates content asynchronously.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        tool_config: Config shared for all tools provided in the request.\n        stream: Whether to stream the response.\n\n    Returns:\n        An awaitable for a single GenerationResponse object if stream == False\n        An awaitable for a stream of GenerationResponse objects if stream == True\n    \"\"\"\n    if stream:\n        return await self._generate_content_streaming_async(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n    else:\n        return await self._generate_content_async(\n            contents=contents,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n            tool_config=tool_config,\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerativeModel.count_tokens","title":"count_tokens","text":"<pre><code>count_tokens(contents: ContentsType) -&gt; CountTokensResponse\n</code></pre> <p>Counts tokens.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> RETURNS DESCRIPTION <code>CountTokensResponse</code> <p>A CountTokensResponse object that has the following attributes: total_tokens: The total number of tokens counted across all instances from the request. total_billable_characters: The total number of billable characters counted across all instances from the request.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def count_tokens(\n    self, contents: ContentsType\n) -&gt; gapic_prediction_service_types.CountTokensResponse:\n    \"\"\"Counts tokens.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n\n    Returns:\n        A CountTokensResponse object that has the following attributes:\n            total_tokens: The total number of tokens counted across all instances from the request.\n            total_billable_characters: The total number of billable characters counted across all instances from the request.\n    \"\"\"\n    return self._prediction_client.count_tokens(\n        request=gapic_prediction_service_types.CountTokensRequest(\n            endpoint=self._prediction_resource_name,\n            model=self._prediction_resource_name,\n            contents=self._prepare_request(contents=contents).contents,\n        )\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerativeModel.count_tokens_async","title":"count_tokens_async  <code>async</code>","text":"<pre><code>count_tokens_async(\n    contents: ContentsType,\n) -&gt; CountTokensResponse\n</code></pre> <p>Counts tokens asynchronously.</p> PARAMETER DESCRIPTION <code>contents</code> <p>Contents to send to the model. Supports either a list of Content objects (passing a multi-turn conversation) or a value that can be converted to a single Content object (passing a single message). Supports * str, Image, Part, * List[Union[str, Image, Part]], * List[Content]</p> <p> TYPE: <code>ContentsType</code> </p> RETURNS DESCRIPTION <code>CountTokensResponse</code> <p>And awaitable for a CountTokensResponse object that has the following attributes: total_tokens: The total number of tokens counted across all instances from the request. total_billable_characters: The total number of billable characters counted across all instances from the request.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>async def count_tokens_async(\n    self, contents: ContentsType\n) -&gt; gapic_prediction_service_types.CountTokensResponse:\n    \"\"\"Counts tokens asynchronously.\n\n    Args:\n        contents: Contents to send to the model.\n            Supports either a list of Content objects (passing a multi-turn conversation)\n            or a value that can be converted to a single Content object (passing a single message).\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n            * List[Content]\n\n    Returns:\n        And awaitable for a CountTokensResponse object that has the following attributes:\n            total_tokens: The total number of tokens counted across all instances from the request.\n            total_billable_characters: The total number of billable characters counted across all instances from the request.\n    \"\"\"\n    return await self._prediction_async_client.count_tokens(\n        request=gapic_prediction_service_types.CountTokensRequest(\n            endpoint=self._prediction_resource_name,\n            model=self._prediction_resource_name,\n            contents=self._prepare_request(contents=contents).contents,\n        )\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.GenerativeModel.start_chat","title":"start_chat","text":"<pre><code>start_chat(\n    *,\n    history: Optional[List[Content]] = None,\n    response_validation: bool = True,\n    responder: Optional[\n        AutomaticFunctionCallingResponder\n    ] = None\n) -&gt; ChatSession\n</code></pre> <p>Creates a stateful chat session.</p> PARAMETER DESCRIPTION <code>history</code> <p>Previous history to initialize the chat session.</p> <p> TYPE: <code>Optional[List[Content]]</code> DEFAULT: <code>None</code> </p> <code>response_validation</code> <p>Whether to validate responses before adding them to chat history. By default, <code>send_message</code> will raise error if the request or response is blocked or if the response is incomplete due to going over the max token limit. If set to <code>False</code>, the chat session history will always accumulate the request and response messages even if the response if blocked or incomplete. This can result in an unusable chat session state.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>responder</code> <p>An responder object that can automatically respond to some model messages. Supported responder classes: <code>AutomaticFunctionCallingResponder</code>.</p> <p> TYPE: <code>Optional[AutomaticFunctionCallingResponder]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ChatSession</code> <p>A ChatSession object.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def start_chat(\n    self,\n    *,\n    history: Optional[List[\"Content\"]] = None,\n    response_validation: bool = True,\n    # Preview features:\n    responder: Optional[\"AutomaticFunctionCallingResponder\"] = None,\n) -&gt; \"ChatSession\":\n    \"\"\"Creates a stateful chat session.\n\n    Args:\n        history: Previous history to initialize the chat session.\n        response_validation: Whether to validate responses before adding\n            them to chat history. By default, `send_message` will raise\n            error if the request or response is blocked or if the response\n            is incomplete due to going over the max token limit.\n            If set to `False`, the chat session history will always\n            accumulate the request and response messages even if the\n            response if blocked or incomplete. This can result in an unusable\n            chat session state.\n        responder: An responder object that can automatically respond to\n            some model messages. Supported responder classes:\n            `AutomaticFunctionCallingResponder`.\n\n    Returns:\n        A ChatSession object.\n    \"\"\"\n    return _PreviewChatSession(\n        model=self,\n        history=history,\n        response_validation=response_validation,\n        responder=responder,\n    )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ChatSession","title":"ChatSession","text":"<pre><code>ChatSession(\n    model: _GenerativeModel,\n    *,\n    history: Optional[List[Content]] = None,\n    response_validation: bool = True,\n    responder: Optional[\n        AutomaticFunctionCallingResponder\n    ] = None,\n    raise_on_blocked: Optional[bool] = None\n)\n</code></pre> <p>               Bases: <code>_PreviewChatSession</code></p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def __init__(\n    self,\n    model: _GenerativeModel,\n    *,\n    history: Optional[List[\"Content\"]] = None,\n    response_validation: bool = True,\n    # Preview features:\n    responder: Optional[\"AutomaticFunctionCallingResponder\"] = None,\n    # Deprecated\n    raise_on_blocked: Optional[bool] = None,\n):\n    if raise_on_blocked is not None:\n        warnings.warn(\n            message=\"Use `response_validation` instead of `raise_on_blocked`.\"\n        )\n        if response_validation is not None:\n            raise ValueError(\n                \"Cannot use `response_validation` when `raise_on_blocked` is set.\"\n            )\n        response_validation = raise_on_blocked\n    super().__init__(\n        model=model,\n        history=history,\n        response_validation=response_validation,\n    )\n    self._responder = responder\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ChatSession.history","title":"history  <code>property</code>","text":"<pre><code>history: List[Content]\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ChatSession.send_message","title":"send_message","text":"<pre><code>send_message(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    GenerationResponse, Iterable[GenerationResponse]\n]\n</code></pre> <p>Generates content.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A single GenerationResponse object if stream == False</p> <code>Union[GenerationResponse, Iterable[GenerationResponse]]</code> <p>A stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\"GenerationResponse\", Iterable[\"GenerationResponse\"]]:\n    \"\"\"Generates content.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        A single GenerationResponse object if stream == False\n        A stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"vertexai/preview/generative_models/#vertexai.preview.generative_models.ChatSession.send_message_async","title":"send_message_async","text":"<pre><code>send_message_async(\n    content: PartsType,\n    *,\n    generation_config: Optional[\n        GenerationConfigType\n    ] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[Tool]] = None,\n    stream: bool = False\n) -&gt; Union[\n    Awaitable[GenerationResponse],\n    Awaitable[AsyncIterable[GenerationResponse]],\n]\n</code></pre> <p>Generates content asynchronously.</p> PARAMETER DESCRIPTION <code>content</code> <p>Content to send to the model. Supports a value that can be converted to a Part or a list of such values. Supports * str, Image, Part, * List[Union[str, Image, Part]],</p> <p> TYPE: <code>PartsType</code> </p> <code>generation_config</code> <p>Parameters for the generation.</p> <p> TYPE: <code>Optional[GenerationConfigType]</code> DEFAULT: <code>None</code> </p> <code>safety_settings</code> <p>Safety settings as a mapping from HarmCategory to HarmBlockThreshold.</p> <p> TYPE: <code>Optional[SafetySettingsType]</code> DEFAULT: <code>None</code> </p> <code>tools</code> <p>A list of tools (functions) that the model can try calling.</p> <p> TYPE: <code>Optional[List[Tool]]</code> DEFAULT: <code>None</code> </p> <code>stream</code> <p>Whether to stream the response.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a single GenerationResponse object if stream == False</p> <code>Union[Awaitable[GenerationResponse], Awaitable[AsyncIterable[GenerationResponse]]]</code> <p>An awaitable for a stream of GenerationResponse objects if stream == True</p> RAISES DESCRIPTION <code>ResponseValidationError</code> <p>If the response was blocked or is incomplete.</p> Source code in <code>vertexai\\generative_models\\_generative_models.py</code> <pre><code>def send_message_async(\n    self,\n    content: PartsType,\n    *,\n    generation_config: Optional[GenerationConfigType] = None,\n    safety_settings: Optional[SafetySettingsType] = None,\n    tools: Optional[List[\"Tool\"]] = None,\n    stream: bool = False,\n) -&gt; Union[\n    Awaitable[\"GenerationResponse\"],\n    Awaitable[AsyncIterable[\"GenerationResponse\"]],\n]:\n    \"\"\"Generates content asynchronously.\n\n    Args:\n        content: Content to send to the model.\n            Supports a value that can be converted to a Part or a list of such values.\n            Supports\n            * str, Image, Part,\n            * List[Union[str, Image, Part]],\n        generation_config: Parameters for the generation.\n        safety_settings: Safety settings as a mapping from HarmCategory to HarmBlockThreshold.\n        tools: A list of tools (functions) that the model can try calling.\n        stream: Whether to stream the response.\n\n    Returns:\n        An awaitable for a single GenerationResponse object if stream == False\n        An awaitable for a stream of GenerationResponse objects if stream == True\n\n    Raises:\n        ResponseValidationError: If the response was blocked or is incomplete.\n    \"\"\"\n    if stream:\n        return self._send_message_streaming_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n    else:\n        return self._send_message_async(\n            content=content,\n            generation_config=generation_config,\n            safety_settings=safety_settings,\n            tools=tools,\n        )\n</code></pre>"},{"location":"vertexai/preview/language_models/","title":"Language models","text":""},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models","title":"vertexai.preview.language_models","text":"<p>Classes for working with language models.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.ChatModel","title":"ChatModel  <code>module-attribute</code>","text":"<pre><code>ChatModel = _PreviewChatModel\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.ChatSession","title":"ChatSession  <code>module-attribute</code>","text":"<pre><code>ChatSession = _PreviewChatSession\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.CodeChatModel","title":"CodeChatModel  <code>module-attribute</code>","text":"<pre><code>CodeChatModel = _PreviewCodeChatModel\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.CodeChatSession","title":"CodeChatSession  <code>module-attribute</code>","text":"<pre><code>CodeChatSession = _PreviewCodeChatSession\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.CodeGenerationModel","title":"CodeGenerationModel  <code>module-attribute</code>","text":"<pre><code>CodeGenerationModel = _PreviewCodeGenerationModel\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationModel","title":"TextGenerationModel  <code>module-attribute</code>","text":"<pre><code>TextGenerationModel = _PreviewTextGenerationModel\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbeddingModel","title":"TextEmbeddingModel  <code>module-attribute</code>","text":"<pre><code>TextEmbeddingModel = _PreviewTextEmbeddingModel\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.ChatMessage","title":"ChatMessage  <code>dataclass</code>","text":"<pre><code>ChatMessage(\n    __module__=\"vertexai.language_models\",\n    content: str,\n    author: str,\n)\n</code></pre> <p>A chat message.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>Content of the message.</p> <p> TYPE: <code>str</code> </p> <code>author</code> <p>Author of the message.</p> <p> TYPE: <code>str</code> </p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.ChatMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.ChatMessage.author","title":"author  <code>instance-attribute</code>","text":"<pre><code>author: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.CountTokensResponse","title":"CountTokensResponse  <code>dataclass</code>","text":"<pre><code>CountTokensResponse(\n    total_tokens: int,\n    total_billable_characters: int,\n    _count_tokens_response: Any,\n)\n</code></pre> <p>The response from a count_tokens request. Attributes:     total_tokens (int):         The total number of tokens counted across all         instances passed to the request.     total_billable_characters (int):         The total number of billable characters         counted across all instances from the request.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.CountTokensResponse.total_tokens","title":"total_tokens  <code>instance-attribute</code>","text":"<pre><code>total_tokens: int\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.CountTokensResponse.total_billable_characters","title":"total_billable_characters  <code>instance-attribute</code>","text":"<pre><code>total_billable_characters: int\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.InputOutputTextPair","title":"InputOutputTextPair  <code>dataclass</code>","text":"<pre><code>InputOutputTextPair(\n    __module__=\"vertexai.language_models\",\n    input_text: str,\n    output_text: str,\n)\n</code></pre> <p>InputOutputTextPair represents a pair of input and output texts.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.InputOutputTextPair.input_text","title":"input_text  <code>instance-attribute</code>","text":"<pre><code>input_text: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.InputOutputTextPair.output_text","title":"output_text  <code>instance-attribute</code>","text":"<pre><code>output_text: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbedding","title":"TextEmbedding  <code>dataclass</code>","text":"<pre><code>TextEmbedding(\n    __module__=\"vertexai.language_models\",\n    values: List[float],\n    statistics: Optional[TextEmbeddingStatistics] = None,\n    _prediction_response: Optional[Prediction] = None,\n)\n</code></pre> <p>Text embedding vector and statistics.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbedding.values","title":"values  <code>instance-attribute</code>","text":"<pre><code>values: List[float]\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbedding.statistics","title":"statistics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>statistics: Optional[TextEmbeddingStatistics] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbeddingInput","title":"TextEmbeddingInput  <code>dataclass</code>","text":"<pre><code>TextEmbeddingInput(\n    __module__=\"vertexai.language_models\",\n    text: str,\n    task_type: Optional[str] = None,\n    title: Optional[str] = None,\n)\n</code></pre> <p>Structural text embedding input.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The main text content to embed.</p> <p> TYPE: <code>str</code> </p> <code>task_type</code> <p>The name of the downstream task the embeddings will be used for. Valid values: RETRIEVAL_QUERY     Specifies the given text is a query in a search/retrieval setting. RETRIEVAL_DOCUMENT     Specifies the given text is a document from the corpus being searched. SEMANTIC_SIMILARITY     Specifies the given text will be used for STS. CLASSIFICATION     Specifies that the given text will be classified. CLUSTERING     Specifies that the embeddings will be used for clustering. QUESTION_ANSWERING     Specifies that the embeddings will be used for question answering. FACT_VERIFICATION     Specifies that the embeddings will be used for fact verification.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>title</code> <p>Optional identifier of the text content.</p> <p> TYPE: <code>Optional[str]</code> </p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbeddingInput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbeddingInput.task_type","title":"task_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>task_type: Optional[str] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextEmbeddingInput.title","title":"title  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>title: Optional[str] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse","title":"TextGenerationResponse  <code>dataclass</code>","text":"<pre><code>TextGenerationResponse(\n    __module__=\"vertexai.language_models\",\n    text: str,\n    _prediction_response: Any,\n    is_blocked: bool = False,\n    errors: Tuple[int] = tuple(),\n    safety_attributes: Dict[str, float] = dict(),\n    grounding_metadata: Optional[GroundingMetadata] = None,\n)\n</code></pre> <p>TextGenerationResponse represents a response of a language model. Attributes:     text: The generated text     is_blocked: Whether the the request was blocked.     errors: The error codes indicate why the response was blocked.         Learn more information about safety errors here:         this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors     safety_attributes: Scores for safety attributes.         Learn more about the safety attributes here:         https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions     grounding_metadata: Metadata for grounding.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse.is_blocked","title":"is_blocked  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_blocked: bool = False\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse.errors","title":"errors  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>errors: Tuple[int] = tuple()\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse.safety_attributes","title":"safety_attributes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>safety_attributes: Dict[str, float] = field(\n    default_factory=dict\n)\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse.grounding_metadata","title":"grounding_metadata  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>grounding_metadata: Optional[GroundingMetadata] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TextGenerationResponse.raw_prediction_response","title":"raw_prediction_response  <code>property</code>","text":"<pre><code>raw_prediction_response: Prediction\n</code></pre> <p>Raw prediction response.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec","title":"TuningEvaluationSpec  <code>dataclass</code>","text":"<pre><code>TuningEvaluationSpec(\n    __module__=\"vertexai.language_models\",\n    evaluation_data: Optional[str] = None,\n    evaluation_interval: Optional[int] = None,\n    enable_early_stopping: Optional[bool] = None,\n    enable_checkpoint_selection: Optional[bool] = None,\n    tensorboard: Optional[Union[Tensorboard, str]] = None,\n)\n</code></pre> <p>Specification for model evaluation to perform during tuning.</p> ATTRIBUTE DESCRIPTION <code>evaluation_data</code> <p>GCS URI of the evaluation dataset. This will run model evaluation as part of the tuning job.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>evaluation_interval</code> <p>The evaluation will run at every evaluation_interval tuning steps. Default: 20.</p> <p> TYPE: <code>Optional[int]</code> </p> <code>enable_early_stopping</code> <p>If True, the tuning may stop early before completing all the tuning steps. Requires evaluation_data.</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>enable_checkpoint_selection</code> <p>If set to True, the tuning process returns the best model checkpoint (based on model evaluation). If set to False, the latest model checkpoint is returned. If unset, the selection is only enabled for <code>*-bison@001</code> models.</p> <p> TYPE: <code>Optional[bool]</code> </p> <code>tensorboard</code> <p>Vertex Tensorboard where to write the evaluation metrics. The Tensorboard must be in the same location as the tuning job.</p> <p> TYPE: <code>Optional[Union[Tensorboard, str]]</code> </p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec.evaluation_data","title":"evaluation_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_data: Optional[str] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec.evaluation_interval","title":"evaluation_interval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>evaluation_interval: Optional[int] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec.enable_early_stopping","title":"enable_early_stopping  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>enable_early_stopping: Optional[bool] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec.enable_checkpoint_selection","title":"enable_checkpoint_selection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>enable_checkpoint_selection: Optional[bool] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec.tensorboard","title":"tensorboard  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tensorboard: Optional[Union[Tensorboard, str]] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextGenerationSpec","title":"EvaluationTextGenerationSpec  <code>dataclass</code>","text":"<pre><code>EvaluationTextGenerationSpec(\n    ground_truth_data: Union[List[str], str, DataFrame]\n)\n</code></pre> <p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for text generation model evaluation tasks.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextGenerationSpec.ground_truth_data","title":"ground_truth_data  <code>instance-attribute</code>","text":"<pre><code>ground_truth_data: Union[List[str], str, DataFrame]\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextGenerationSpec.task_name","title":"task_name  <code>property</code>","text":"<pre><code>task_name: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextSummarizationSpec","title":"EvaluationTextSummarizationSpec  <code>dataclass</code>","text":"<pre><code>EvaluationTextSummarizationSpec(\n    ground_truth_data: Union[List[str], str, DataFrame],\n    task_name: str = \"summarization\",\n)\n</code></pre> <p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for text summarization model evaluation tasks.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextSummarizationSpec.ground_truth_data","title":"ground_truth_data  <code>instance-attribute</code>","text":"<pre><code>ground_truth_data: Union[List[str], str, DataFrame]\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextSummarizationSpec.task_name","title":"task_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>task_name: str = 'summarization'\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationQuestionAnsweringSpec","title":"EvaluationQuestionAnsweringSpec  <code>dataclass</code>","text":"<pre><code>EvaluationQuestionAnsweringSpec(\n    ground_truth_data: Union[List[str], str, DataFrame],\n    task_name: str = \"question-answering\",\n)\n</code></pre> <p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for question answering model evaluation tasks.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationQuestionAnsweringSpec.ground_truth_data","title":"ground_truth_data  <code>instance-attribute</code>","text":"<pre><code>ground_truth_data: Union[List[str], str, DataFrame]\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationQuestionAnsweringSpec.task_name","title":"task_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>task_name: str = 'question-answering'\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextClassificationSpec","title":"EvaluationTextClassificationSpec  <code>dataclass</code>","text":"<pre><code>EvaluationTextClassificationSpec(\n    ground_truth_data: Union[List[str], str, DataFrame],\n    target_column_name: str,\n    class_names: List[str],\n)\n</code></pre> <p>               Bases: <code>_EvaluationTaskSpec</code></p> <p>Spec for text classification model evaluation tasks.</p> PARAMETER DESCRIPTION <code>target_column_name</code> <p>Required. The label column in the dataset provided in <code>ground_truth_data</code>. Required when task_name='text-classification'.</p> <p> TYPE: <code>str</code> </p> <code>class_names</code> <p>Required. A list of all possible label names in your dataset. Required when task_name='text-classification'.</p> <p> TYPE: <code>List[str]</code> </p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextClassificationSpec.ground_truth_data","title":"ground_truth_data  <code>instance-attribute</code>","text":"<pre><code>ground_truth_data: Union[List[str], str, DataFrame]\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextClassificationSpec.target_column_name","title":"target_column_name  <code>instance-attribute</code>","text":"<pre><code>target_column_name: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextClassificationSpec.class_names","title":"class_names  <code>instance-attribute</code>","text":"<pre><code>class_names: List[str]\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationTextClassificationSpec.task_name","title":"task_name  <code>property</code>","text":"<pre><code>task_name: str\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric","title":"EvaluationClassificationMetric  <code>dataclass</code>","text":"<pre><code>EvaluationClassificationMetric(\n    label_name: Optional[str] = None,\n    auPrc: Optional[float] = None,\n    auRoc: Optional[float] = None,\n    logLoss: Optional[float] = None,\n    confidenceMetrics: Optional[\n        List[Dict[str, Any]]\n    ] = None,\n    confusionMatrix: Optional[Dict[str, Any]] = None,\n)\n</code></pre> <p>               Bases: <code>_EvaluationMetricBase</code></p> <p>The evaluation metric response for classification metrics.</p> PARAMETER DESCRIPTION <code>label_name</code> <p>Optional. The name of the label associated with the metrics. This is only returned when <code>only_summary_metrics=False</code> is passed to evaluate().</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auPrc</code> <p>Optional. The area under the precision recall curve.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>auRoc</code> <p>Optional. The area under the receiver operating characteristic curve.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>logLoss</code> <p>Optional. Logarithmic loss.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>confidenceMetrics</code> <p>Optional. This is only returned when <code>only_summary_metrics=False</code> is passed to evaluate().</p> <p> TYPE: <code>List[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>confusionMatrix</code> <p>Optional. This is only returned when <code>only_summary_metrics=False</code> is passed to evaluate().</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.input_dataset_paths","title":"input_dataset_paths  <code>property</code>","text":"<pre><code>input_dataset_paths: str\n</code></pre> <p>The Google Cloud Storage paths to the dataset used for this evaluation.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.task_name","title":"task_name  <code>property</code>","text":"<pre><code>task_name: str\n</code></pre> <p>The type of evaluation task for the evaluation..</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.label_name","title":"label_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label_name: Optional[str] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.auPrc","title":"auPrc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>auPrc: Optional[float] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.auRoc","title":"auRoc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>auRoc: Optional[float] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.logLoss","title":"logLoss  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logLoss: Optional[float] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.confidenceMetrics","title":"confidenceMetrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confidenceMetrics: Optional[List[Dict[str, Any]]] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationClassificationMetric.confusionMatrix","title":"confusionMatrix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confusionMatrix: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationMetric","title":"EvaluationMetric  <code>dataclass</code>","text":"<pre><code>EvaluationMetric(\n    bleu: Optional[float] = None,\n    rougeLSum: Optional[float] = None,\n)\n</code></pre> <p>               Bases: <code>_EvaluationMetricBase</code></p> <p>The evaluation metric response.</p> PARAMETER DESCRIPTION <code>bleu</code> <p>Optional. BLEU (Bilingual evauation understudy). Scores based on sacrebleu implementation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>rougeLSum</code> <p>Optional. ROUGE-L (Longest Common Subsequence) scoring at summary level.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationMetric.input_dataset_paths","title":"input_dataset_paths  <code>property</code>","text":"<pre><code>input_dataset_paths: str\n</code></pre> <p>The Google Cloud Storage paths to the dataset used for this evaluation.</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationMetric.task_name","title":"task_name  <code>property</code>","text":"<pre><code>task_name: str\n</code></pre> <p>The type of evaluation task for the evaluation..</p>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationMetric.bleu","title":"bleu  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bleu: Optional[float] = None\n</code></pre>"},{"location":"vertexai/preview/language_models/#vertexai.preview.language_models.EvaluationMetric.rougeLSum","title":"rougeLSum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rougeLSum: Optional[float] = None\n</code></pre>"},{"location":"vertexai/preview/tuning/","title":"Tuning","text":""},{"location":"vertexai/preview/tuning/#vertexai.preview.tuning","title":"vertexai.preview.tuning","text":"<p>Classes for tuning models.</p>"},{"location":"vertexai/preview/vision_models/","title":"Vision models","text":""},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models","title":"vertexai.preview.vision_models","text":"<p>Classes for working with vision models.</p>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage","title":"GeneratedImage","text":"<pre><code>GeneratedImage(\n    image_bytes: Optional[bytes],\n    generation_parameters: Dict[str, Any],\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>Image</code></p> <p>Generated image.</p> PARAMETER DESCRIPTION <code>image_bytes</code> <p>Image file bytes. Image can be in PNG or JPEG format.</p> <p> TYPE: <code>Optional[bytes]</code> </p> <code>generation_parameters</code> <p>Image generation parameter values.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>gcs_uri</code> <p>Image file Google Cloud Storage uri.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    image_bytes: Optional[bytes],\n    generation_parameters: Dict[str, Any],\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates a `GeneratedImage` object.\n\n    Args:\n        image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n        generation_parameters: Image generation parameter values.\n        gcs_uri: Image file Google Cloud Storage uri.\n    \"\"\"\n    super().__init__(image_bytes=image_bytes, gcs_uri=gcs_uri)\n    self._generation_parameters = generation_parameters\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.generation_parameters","title":"generation_parameters  <code>property</code>","text":"<pre><code>generation_parameters\n</code></pre> <p>Image generation parameters as a dictionary.</p>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.show","title":"show","text":"<pre><code>show()\n</code></pre> <p>Shows the image.</p> <p>This method only works when in a notebook environment.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def show(self):\n    \"\"\"Shows the image.\n\n    This method only works when in a notebook environment.\n    \"\"\"\n    if PIL_Image and IPython_display:\n        IPython_display.display(self._pil_image)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; GeneratedImage\n</code></pre> <p>Loads image from file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>GeneratedImage</code> <p>Loaded image as a <code>GeneratedImage</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"GeneratedImage\":\n    \"\"\"Loads image from file.\n\n    Args:\n        location: Local path from where to load the image.\n\n    Returns:\n        Loaded image as a `GeneratedImage` object.\n    \"\"\"\n    base_image = Image.load_from_file(location=location)\n    exif = base_image._pil_image.getexif()  # pylint: disable=protected-access\n    exif_comment_dict = json.loads(exif[_EXIF_USER_COMMENT_TAG_IDX])\n    generation_parameters = exif_comment_dict[_IMAGE_GENERATION_PARAMETERS_EXIF_KEY]\n    return GeneratedImage(\n        image_bytes=base_image._image_bytes,  # pylint: disable=protected-access\n        generation_parameters=generation_parameters,\n        gcs_uri=base_image._gcs_uri,  # pylint: disable=protected-access\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.GeneratedImage.save","title":"save","text":"<pre><code>save(\n    location: str,\n    include_generation_parameters: bool = True,\n)\n</code></pre> <p>Saves image to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the image.</p> <p> TYPE: <code>str</code> </p> <code>include_generation_parameters</code> <p>Whether to include the image generation parameters in the image's EXIF metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str, include_generation_parameters: bool = True):\n    \"\"\"Saves image to a file.\n\n    Args:\n        location: Local path where to save the image.\n        include_generation_parameters: Whether to include the image\n            generation parameters in the image's EXIF metadata.\n    \"\"\"\n    if include_generation_parameters:\n        if not self._generation_parameters:\n            raise ValueError(\"Image does not have generation parameters.\")\n        if not PIL_Image:\n            raise ValueError(\n                \"The PIL module is required for saving generation parameters.\"\n            )\n\n        exif = self._pil_image.getexif()\n        exif[_EXIF_USER_COMMENT_TAG_IDX] = json.dumps(\n            {_IMAGE_GENERATION_PARAMETERS_EXIF_KEY: self._generation_parameters}\n        )\n        self._pil_image.save(location, exif=exif)\n    else:\n        super().save(location=location)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image","title":"Image","text":"<pre><code>Image(\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> <p>Image.</p> PARAMETER DESCRIPTION <code>image_bytes</code> <p>Image file bytes. Image can be in PNG or JPEG format.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    image_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(image_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either image_bytes or gcs_uri must be provided.\")\n\n    self._image_bytes = image_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Image\n</code></pre> <p>Loads image from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the image.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>Loaded image as an <code>Image</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Image\":\n    \"\"\"Loads image from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the image.\n\n    Returns:\n        Loaded image as an `Image` object.\n    \"\"\"\n    parsed_url = urllib.parse.urlparse(location)\n    if (\n        parsed_url.scheme == \"https\"\n        and parsed_url.netloc == \"storage.googleapis.com\"\n    ):\n        parsed_url = parsed_url._replace(\n            scheme=\"gs\", netloc=\"\", path=f\"/{urllib.parse.unquote(parsed_url.path)}\"\n        )\n        location = urllib.parse.urlunparse(parsed_url)\n\n    if parsed_url.scheme == \"gs\":\n        return Image(gcs_uri=location)\n\n    # Load image from local path\n    image_bytes = pathlib.Path(location).read_bytes()\n    image = Image(image_bytes=image_bytes)\n    return image\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image.show","title":"show","text":"<pre><code>show()\n</code></pre> <p>Shows the image.</p> <p>This method only works when in a notebook environment.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def show(self):\n    \"\"\"Shows the image.\n\n    This method only works when in a notebook environment.\n    \"\"\"\n    if PIL_Image and IPython_display:\n        IPython_display.display(self._pil_image)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Image.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves image to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the image.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves image to a file.\n\n    Args:\n        location: Local path where to save the image.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._image_bytes)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageCaptioningModel","title":"ImageCaptioningModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates captions from image.</p> <p>Examples::</p> <pre><code>model = ImageCaptioningModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageCaptioningModel.get_captions","title":"get_captions","text":"<pre><code>get_captions(\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None\n) -&gt; List[str]\n</code></pre> <p>Generates captions for a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>language</code> <p>Language to use for captions. Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the captioned images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of image caption strings.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_captions(\n    self,\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None,\n) -&gt; List[str]:\n    \"\"\"Generates captions for a given image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        number_of_results: Number of captions to produce. Range: 1-3.\n        language: Language to use for captions.\n            Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n        output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n    Returns:\n        A list of image caption strings.\n    \"\"\"\n    instance = {}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n        \"language\": language,\n    }\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel","title":"ImageGenerationModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates images from text prompt.</p> <p>Examples::</p> <pre><code>model = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\nresponse = model.generate_images(\n    prompt=\"Astronaut riding a horse\",\n    # Optional:\n    number_of_images=1,\n    seed=0,\n)\nresponse[0].show()\nresponse[0].save(\"image1.png\")\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel.generate_images","title":"generate_images","text":"<pre><code>generate_images(\n    prompt: str,\n    *,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    aspect_ratio: Optional[\n        Literal[\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"]\n    ] = None,\n    guidance_scale: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    add_watermark: Optional[bool] = True,\n    safety_filter_level: Optional[\n        Literal[\n            \"block_most\",\n            \"block_some\",\n            \"block_few\",\n            \"block_fewest\",\n        ]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None\n) -&gt; ImageGenerationResponse\n</code></pre> <p>Generates images from text prompt.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Text prompt for the image.</p> <p> TYPE: <code>str</code> </p> <code>negative_prompt</code> <p>A description of what you want to omit in the generated images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>number_of_images</code> <p>Number of images to generate. Range: 1..8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>aspect_ratio</code> <p>Changes the aspect ratio of the generated image Supported values are: * \"1:1\" : 1:1 aspect ratio * \"9:16\" : 9:16 aspect ratio * \"16:9\" : 16:9 aspect ratio * \"4:3\" : 4:3 aspect ratio * \"3:4\" : 3:4 aspect_ratio</p> <p> TYPE: <code>Optional[Literal['1:1', '9:16', '16:9', '4:3', '3:4']]</code> DEFAULT: <code>None</code> </p> <code>guidance_scale</code> <p>Controls the strength of the prompt. Suggested values are: * 0-9 (low strength) * 10-20 (medium strength) * 21+ (high strength)</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language of the text prompt for the image. Default: None. Supported values are <code>\"en\"</code> for English, <code>\"hi\"</code> for Hindi, <code>\"ja\"</code> for Japanese, <code>\"ko\"</code> for Korean, and <code>\"auto\"</code> for automatic language detection.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Image generation random seed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the generated images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>add_watermark</code> <p>Add a watermark to the generated image</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>True</code> </p> <code>safety_filter_level</code> <p>Adds a filter level to Safety filtering. Supported values are: * \"block_most\" : Strongest filtering level, most strict blocking * \"block_some\" : Block some problematic prompts and responses * \"block_few\" : Block fewer problematic prompts and responses * \"block_fewest\" : Block very few problematic prompts and responses</p> <p> TYPE: <code>Optional[Literal['block_most', 'block_some', 'block_few', 'block_fewest']]</code> DEFAULT: <code>None</code> </p> <code>person_generation</code> <p>Allow generation of people by the model Supported values are: * \"dont_allow\" : Block generation of people * \"allow_adult\" : Generate adults, but not children * \"allow_all\" : Generate adults and children</p> <p> TYPE: <code>Optional[Literal['dont_allow', 'allow_adult', 'allow_all']]</code> DEFAULT: <code>None</code> </p> <p>Returns:     An <code>ImageGenerationResponse</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def generate_images(\n    self,\n    prompt: str,\n    *,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    aspect_ratio: Optional[Literal[\"1:1\", \"9:16\", \"16:9\", \"4:3\", \"3:4\"]] = None,\n    guidance_scale: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    add_watermark: Optional[bool] = True,\n    safety_filter_level: Optional[\n        Literal[\"block_most\", \"block_some\", \"block_few\", \"block_fewest\"]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None,\n) -&gt; \"ImageGenerationResponse\":\n    \"\"\"Generates images from text prompt.\n\n    Args:\n        prompt: Text prompt for the image.\n        negative_prompt: A description of what you want to omit in the generated\n            images.\n        number_of_images: Number of images to generate. Range: 1..8.\n        aspect_ratio: Changes the aspect ratio of the generated image Supported\n            values are:\n            * \"1:1\" : 1:1 aspect ratio\n            * \"9:16\" : 9:16 aspect ratio\n            * \"16:9\" : 16:9 aspect ratio\n            * \"4:3\" : 4:3 aspect ratio\n            * \"3:4\" : 3:4 aspect_ratio\n        guidance_scale: Controls the strength of the prompt. Suggested values are:\n            * 0-9 (low strength)\n            * 10-20 (medium strength)\n            * 21+ (high strength)\n        language: Language of the text prompt for the image. Default: None.\n            Supported values are `\"en\"` for English, `\"hi\"` for Hindi, `\"ja\"`\n            for Japanese, `\"ko\"` for Korean, and `\"auto\"` for automatic language\n            detection.\n        seed: Image generation random seed.\n        output_gcs_uri: Google Cloud Storage uri to store the generated images.\n        add_watermark: Add a watermark to the generated image\n        safety_filter_level: Adds a filter level to Safety filtering. Supported\n            values are:\n            * \"block_most\" : Strongest filtering level, most strict\n            blocking\n            * \"block_some\" : Block some problematic prompts and responses\n            * \"block_few\" : Block fewer problematic prompts and responses\n            * \"block_fewest\" : Block very few problematic prompts and responses\n        person_generation: Allow generation of people by the model Supported\n            values are:\n            * \"dont_allow\" : Block generation of people\n            * \"allow_adult\" : Generate adults, but not children\n            * \"allow_all\" : Generate adults and children\n    Returns:\n        An `ImageGenerationResponse` object.\n    \"\"\"\n    return self._generate_images(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        number_of_images=number_of_images,\n        aspect_ratio=aspect_ratio,\n        guidance_scale=guidance_scale,\n        language=language,\n        seed=seed,\n        output_gcs_uri=output_gcs_uri,\n        add_watermark=add_watermark,\n        safety_filter_level=safety_filter_level,\n        person_generation=person_generation,\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel.edit_image","title":"edit_image","text":"<pre><code>edit_image(\n    *,\n    prompt: str,\n    base_image: Image,\n    mask: Optional[Image] = None,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    guidance_scale: Optional[float] = None,\n    edit_mode: Optional[\n        Literal[\n            \"inpainting-insert\",\n            \"inpainting-remove\",\n            \"outpainting\",\n            \"product-image\",\n        ]\n    ] = None,\n    mask_mode: Optional[\n        Literal[\"background\", \"foreground\", \"semantic\"]\n    ] = None,\n    segmentation_classes: Optional[List[str]] = None,\n    mask_dilation: Optional[float] = None,\n    product_position: Optional[\n        Literal[\"fixed\", \"reposition\"]\n    ] = None,\n    output_mime_type: Optional[\n        Literal[\"image/png\", \"image/jpeg\"]\n    ] = None,\n    compression_quality: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    safety_filter_level: Optional[\n        Literal[\n            \"block_most\",\n            \"block_some\",\n            \"block_few\",\n            \"block_fewest\",\n        ]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None\n) -&gt; ImageGenerationResponse\n</code></pre> <p>Edits an existing image based on text prompt.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>Text prompt for the image.</p> <p> TYPE: <code>str</code> </p> <code>base_image</code> <p>Base image from which to generate the new image.</p> <p> TYPE: <code>Image</code> </p> <code>mask</code> <p>Mask for the base image.</p> <p> TYPE: <code>Optional[Image]</code> DEFAULT: <code>None</code> </p> <code>negative_prompt</code> <p>A description of what you want to omit in the generated images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>number_of_images</code> <p>Number of images to generate. Range: 1..8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>guidance_scale</code> <p>Controls the strength of the prompt. Suggested values are: * 0-9 (low strength) * 10-20 (medium strength) * 21+ (high strength)</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>edit_mode</code> <p>Describes the editing mode for the request. Supported values are: * inpainting-insert: fills the mask area based on the text prompt (requires mask and text) * inpainting-remove: removes the object(s) in the mask area. (requires mask) * outpainting: extend the image based on the mask area. (Requires mask) * product-image: Changes the background for the predominant product or subject in the image</p> <p> TYPE: <code>Optional[Literal['inpainting-insert', 'inpainting-remove', 'outpainting', 'product-image']]</code> DEFAULT: <code>None</code> </p> <code>mask_mode</code> <p>Solicits generation of the mask (v/s providing mask as an input). Supported values are: * background: Automatically generates a mask for all regions except the primary subject(s) of the image * foreground: Automatically generates a mask for the primary subjects(s) of the image. * semantic: Segment one or more of the segmentation classes using class ID</p> <p> TYPE: <code>Optional[Literal['background', 'foreground', 'semantic']]</code> DEFAULT: <code>None</code> </p> <code>segmentation_classes</code> <p>List of class IDs for segmentation. Max of 5 IDs</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>mask_dilation</code> <p>Defines the dilation percentage of the mask provided. Float between 0 and 1. Defaults to 0.03</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>product_position</code> <p>Defines whether the product should stay fixed or be repositioned. Supported Values: * fixed: Fixed position * reposition: Can be moved (default)</p> <p> TYPE: <code>Optional[Literal['fixed', 'reposition']]</code> DEFAULT: <code>None</code> </p> <code>output_mime_type</code> <p>Which image format should the output be saved as. Supported values: * image/png: Save as a PNG image * image/jpeg: Save as a JPEG image</p> <p> TYPE: <code>Optional[Literal['image/png', 'image/jpeg']]</code> DEFAULT: <code>None</code> </p> <code>compression_quality</code> <p>Level of compression if the output mime type is selected to be image/jpeg. Float between 0 to 100</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language of the text prompt for the image. Default: None. Supported values are <code>\"en\"</code> for English, <code>\"hi\"</code> for Hindi, <code>\"ja\"</code> for Japanese, <code>\"ko\"</code> for Korean, and <code>\"auto\"</code> for automatic language detection.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Image generation random seed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the edited images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>safety_filter_level</code> <p>Adds a filter level to Safety filtering. Supported values are: * \"block_most\" : Strongest filtering level, most strict blocking * \"block_some\" : Block some problematic prompts and responses * \"block_few\" : Block fewer problematic prompts and responses * \"block_fewest\" : Block very few problematic prompts and responses</p> <p> TYPE: <code>Optional[Literal['block_most', 'block_some', 'block_few', 'block_fewest']]</code> DEFAULT: <code>None</code> </p> <code>person_generation</code> <p>Allow generation of people by the model Supported values are: * \"dont_allow\" : Block generation of people * \"allow_adult\" : Generate adults, but not children * \"allow_all\" : Generate adults and children</p> <p> TYPE: <code>Optional[Literal['dont_allow', 'allow_adult', 'allow_all']]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ImageGenerationResponse</code> <p>An <code>ImageGenerationResponse</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def edit_image(\n    self,\n    *,\n    prompt: str,\n    base_image: \"Image\",\n    mask: Optional[\"Image\"] = None,\n    negative_prompt: Optional[str] = None,\n    number_of_images: int = 1,\n    guidance_scale: Optional[float] = None,\n    edit_mode: Optional[\n        Literal[\n            \"inpainting-insert\", \"inpainting-remove\", \"outpainting\", \"product-image\"\n        ]\n    ] = None,\n    mask_mode: Optional[Literal[\"background\", \"foreground\", \"semantic\"]] = None,\n    segmentation_classes: Optional[List[str]] = None,\n    mask_dilation: Optional[float] = None,\n    product_position: Optional[Literal[\"fixed\", \"reposition\"]] = None,\n    output_mime_type: Optional[Literal[\"image/png\", \"image/jpeg\"]] = None,\n    compression_quality: Optional[float] = None,\n    language: Optional[str] = None,\n    seed: Optional[int] = None,\n    output_gcs_uri: Optional[str] = None,\n    safety_filter_level: Optional[\n        Literal[\"block_most\", \"block_some\", \"block_few\", \"block_fewest\"]\n    ] = None,\n    person_generation: Optional[\n        Literal[\"dont_allow\", \"allow_adult\", \"allow_all\"]\n    ] = None,\n) -&gt; \"ImageGenerationResponse\":\n    \"\"\"Edits an existing image based on text prompt.\n\n    Args:\n        prompt: Text prompt for the image.\n        base_image: Base image from which to generate the new image.\n        mask: Mask for the base image.\n        negative_prompt: A description of what you want to omit in\n            the generated images.\n        number_of_images: Number of images to generate. Range: 1..8.\n        guidance_scale: Controls the strength of the prompt.\n            Suggested values are:\n            * 0-9 (low strength)\n            * 10-20 (medium strength)\n            * 21+ (high strength)\n        edit_mode: Describes the editing mode for the request. Supported values are:\n            * inpainting-insert: fills the mask area based on the text prompt\n            (requires mask and text)\n            * inpainting-remove: removes the object(s) in the mask area.\n            (requires mask)\n            * outpainting: extend the image based on the mask area.\n            (Requires mask)\n            * product-image: Changes the background for the predominant product\n            or subject in the image\n        mask_mode: Solicits generation of the mask (v/s providing mask as an\n            input). Supported values are:\n            * background: Automatically generates a mask for all regions except\n            the primary subject(s) of the image\n            * foreground: Automatically generates a mask for the primary\n            subjects(s) of the image.\n            * semantic: Segment one or more of the segmentation classes using\n            class ID\n        segmentation_classes: List of class IDs for segmentation. Max of 5 IDs\n        mask_dilation: Defines the dilation percentage of the mask provided.\n            Float between 0 and 1. Defaults to 0.03\n        product_position: Defines whether the product should stay fixed or be\n            repositioned. Supported Values:\n            * fixed: Fixed position\n            * reposition: Can be moved (default)\n        output_mime_type: Which image format should the output be saved as.\n            Supported values:\n            * image/png: Save as a PNG image\n            * image/jpeg: Save as a JPEG image\n        compression_quality: Level of compression if the output mime type is\n          selected to be image/jpeg. Float between 0 to 100\n        language: Language of the text prompt for the image. Default: None.\n            Supported values are `\"en\"` for English, `\"hi\"` for Hindi,\n            `\"ja\"` for Japanese, `\"ko\"` for Korean, and `\"auto\"` for\n            automatic language detection.\n        seed: Image generation random seed.\n        output_gcs_uri: Google Cloud Storage uri to store the edited images.\n        safety_filter_level: Adds a filter level to Safety filtering. Supported\n            values are:\n            * \"block_most\" : Strongest filtering level, most strict\n            blocking\n            * \"block_some\" : Block some problematic prompts and responses\n            * \"block_few\" : Block fewer problematic prompts and responses\n            * \"block_fewest\" : Block very few problematic prompts and responses\n        person_generation: Allow generation of people by the model Supported\n            values are:\n            * \"dont_allow\" : Block generation of people\n            * \"allow_adult\" : Generate adults, but not children\n            * \"allow_all\" : Generate adults and children\n\n    Returns:\n        An `ImageGenerationResponse` object.\n    \"\"\"\n    return self._generate_images(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        number_of_images=number_of_images,\n        guidance_scale=guidance_scale,\n        seed=seed,\n        base_image=base_image,\n        mask=mask,\n        edit_mode=edit_mode,\n        mask_mode=mask_mode,\n        segmentation_classes=segmentation_classes,\n        mask_dilation=mask_dilation,\n        product_position=product_position,\n        output_mime_type=output_mime_type,\n        compression_quality=compression_quality,\n        language=language,\n        output_gcs_uri=output_gcs_uri,\n        add_watermark=False,  # Not supported for editing yet\n        safety_filter_level=safety_filter_level,\n        person_generation=person_generation,\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationModel.upscale_image","title":"upscale_image","text":"<pre><code>upscale_image(\n    image: Union[Image, GeneratedImage],\n    new_size: Optional[int] = 2048,\n    output_gcs_uri: Optional[str] = None,\n) -&gt; Image\n</code></pre> <p>Upscales an image.</p> <p>This supports upscaling images generated through the <code>generate_images()</code> method, or upscaling a new image that is 1024x1024.</p> <p>Examples::</p> <pre><code># Upscale a generated image\nmodel = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\nresponse = model.generate_images(\n    prompt=\"Astronaut riding a horse\",\n)\nmodel.upscale_image(image=response[0])\n\n# Upscale a new 1024x1024 image\nmy_image = Image.load_from_file(\"my-image.png\")\nmodel.upscale_image(image=my_image)\n</code></pre> PARAMETER DESCRIPTION <code>image</code> <p>Required. The generated image to upscale.</p> <p> TYPE: <code>Union[GeneratedImage, Image]</code> </p> <code>new_size</code> <p>The size of the biggest dimension of the upscaled image. Only 2048 and 4096 are currently supported. Results in a 2048x2048 or 4096x4096 image. Defaults to 2048 if not provided.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2048</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the upscaled images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>An <code>Image</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def upscale_image(\n    self,\n    image: Union[\"Image\", \"GeneratedImage\"],\n    new_size: Optional[int] = 2048,\n    output_gcs_uri: Optional[str] = None,\n) -&gt; \"Image\":\n    \"\"\"Upscales an image.\n\n    This supports upscaling images generated through the `generate_images()` method,\n    or upscaling a new image that is 1024x1024.\n\n    Examples::\n\n        # Upscale a generated image\n        model = ImageGenerationModel.from_pretrained(\"imagegeneration@002\")\n        response = model.generate_images(\n            prompt=\"Astronaut riding a horse\",\n        )\n        model.upscale_image(image=response[0])\n\n        # Upscale a new 1024x1024 image\n        my_image = Image.load_from_file(\"my-image.png\")\n        model.upscale_image(image=my_image)\n\n    Args:\n        image (Union[GeneratedImage, Image]):\n            Required. The generated image to upscale.\n        new_size (int):\n            The size of the biggest dimension of the upscaled image. Only 2048 and 4096 are currently\n            supported. Results in a 2048x2048 or 4096x4096 image. Defaults to 2048 if not provided.\n        output_gcs_uri: Google Cloud Storage uri to store the upscaled images.\n\n    Returns:\n        An `Image` object.\n    \"\"\"\n\n    # Currently this method only supports 1024x1024 images\n    if image._size[0] != 1024 and image._size[1] != 1024:\n        raise ValueError(\n            \"Upscaling is currently only supported on images that are 1024x1024.\"\n        )\n\n    if new_size not in _SUPPORTED_UPSCALING_SIZES:\n        raise ValueError(\n            f\"Only the folowing square upscaling sizes are currently supported: {_SUPPORTED_UPSCALING_SIZES}.\"\n        )\n\n    instance = {\"prompt\": \"\"}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n\n    parameters = {\n        \"sampleImageSize\": str(new_size),\n        \"sampleCount\": 1,\n        \"mode\": \"upscale\",\n    }\n\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n\n    upscaled_image = response.predictions[0]\n\n    if isinstance(image, GeneratedImage):\n        generation_parameters = image.generation_parameters\n\n    else:\n        generation_parameters = {}\n\n    generation_parameters[\"upscaled_image_size\"] = new_size\n\n    encoded_bytes = upscaled_image.get(\"bytesBase64Encoded\")\n    return GeneratedImage(\n        image_bytes=base64.b64decode(encoded_bytes) if encoded_bytes else None,\n        generation_parameters=generation_parameters,\n        gcs_uri=upscaled_image.get(\"gcsUri\"),\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationResponse","title":"ImageGenerationResponse  <code>dataclass</code>","text":"<pre><code>ImageGenerationResponse(\n    __module__=\"vertexai.preview.vision_models\",\n    images: List[GeneratedImage],\n)\n</code></pre> <p>Image generation response.</p> ATTRIBUTE DESCRIPTION <code>images</code> <p>The list of generated images.</p> <p> TYPE: <code>List[GeneratedImage]</code> </p>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageGenerationResponse.images","title":"images  <code>instance-attribute</code>","text":"<pre><code>images: List[GeneratedImage]\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageQnAModel","title":"ImageQnAModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Answers questions about an image.</p> <p>Examples::</p> <pre><code>model = ImageQnAModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageQnAModel.ask_question","title":"ask_question","text":"<pre><code>ask_question(\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1\n) -&gt; List[str]\n</code></pre> <p>Answers questions about an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>question</code> <p>Question to ask about the image.</p> <p> TYPE: <code>str</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of answers.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def ask_question(\n    self,\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1,\n) -&gt; List[str]:\n    \"\"\"Answers questions about an image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        question: Question to ask about the image.\n        number_of_results: Number of captions to produce. Range: 1-3.\n\n    Returns:\n        A list of answers.\n    \"\"\"\n    instance = {\"prompt\": question}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n    }\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageTextModel","title":"ImageTextModel","text":"<p>               Bases: <code>ImageCaptioningModel</code>, <code>ImageQnAModel</code></p> <p>Generates text from images.</p> <p>Examples::</p> <pre><code>model = ImageTextModel.from_pretrained(\"imagetext@001\")\nimage = Image.load_from_file(\"image.png\")\n\ncaptions = model.get_captions(\n    image=image,\n    # Optional:\n    number_of_results=1,\n    language=\"en\",\n)\n\nanswers = model.ask_question(\n    image=image,\n    question=\"What color is the car in this image?\",\n    # Optional:\n    number_of_results=1,\n)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageTextModel.ask_question","title":"ask_question","text":"<pre><code>ask_question(\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1\n) -&gt; List[str]\n</code></pre> <p>Answers questions about an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>question</code> <p>Question to ask about the image.</p> <p> TYPE: <code>str</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of answers.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def ask_question(\n    self,\n    image: Image,\n    question: str,\n    *,\n    number_of_results: int = 1,\n) -&gt; List[str]:\n    \"\"\"Answers questions about an image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        question: Question to ask about the image.\n        number_of_results: Number of captions to produce. Range: 1-3.\n\n    Returns:\n        A list of answers.\n    \"\"\"\n    instance = {\"prompt\": question}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n    }\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.ImageTextModel.get_captions","title":"get_captions","text":"<pre><code>get_captions(\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None\n) -&gt; List[str]\n</code></pre> <p>Generates captions for a given image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to get captions for. Size limit: 10 MB.</p> <p> TYPE: <code>Image</code> </p> <code>number_of_results</code> <p>Number of captions to produce. Range: 1-3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>language</code> <p>Language to use for captions. Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en'</code> </p> <code>output_gcs_uri</code> <p>Google Cloud Storage uri to store the captioned images.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of image caption strings.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_captions(\n    self,\n    image: Image,\n    *,\n    number_of_results: int = 1,\n    language: str = \"en\",\n    output_gcs_uri: Optional[str] = None,\n) -&gt; List[str]:\n    \"\"\"Generates captions for a given image.\n\n    Args:\n        image: The image to get captions for. Size limit: 10 MB.\n        number_of_results: Number of captions to produce. Range: 1-3.\n        language: Language to use for captions.\n            Supported languages: \"en\", \"fr\", \"de\", \"it\", \"es\"\n        output_gcs_uri: Google Cloud Storage uri to store the captioned images.\n\n    Returns:\n        A list of image caption strings.\n    \"\"\"\n    instance = {}\n\n    if image._gcs_uri:  # pylint: disable=protected-access\n        instance[\"image\"] = {\n            \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n        }\n    else:\n        instance[\"image\"] = {\n            \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n        }\n    parameters = {\n        \"sampleCount\": number_of_results,\n        \"language\": language,\n    }\n    if output_gcs_uri is not None:\n        parameters[\"storageUri\"] = output_gcs_uri\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    return response.predictions\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingModel","title":"MultiModalEmbeddingModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Generates embedding vectors from images and videos.</p> <p>Examples::</p> <pre><code>model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")\nimage = Image.load_from_file(\"image.png\")\nvideo = Video.load_from_file(\"video.mp4\")\n\nembeddings = model.get_embeddings(\n    image=image,\n    video=video,\n    contextual_text=\"Hello world\",\n)\nimage_embedding = embeddings.image_embedding\nvideo_embeddings = embeddings.video_embeddings\ntext_embedding = embeddings.text_embedding\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingModel.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[\n        VideoSegmentConfig\n    ] = None,\n) -&gt; MultiModalEmbeddingResponse\n</code></pre> <p>Gets embedding vectors from the provided image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Optional. The image to generate embeddings for. One of <code>image</code>, <code>video</code>, or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Image</code> DEFAULT: <code>None</code> </p> <code>video</code> <p>Optional. The video to generate embeddings for. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>Video</code> DEFAULT: <code>None</code> </p> <code>contextual_text</code> <p>Optional. Contextual text for your input image or video. If provided, the model will also generate an embedding vector for the provided contextual text. The returned image and text embedding vectors are in the same semantic space with the same dimensionality, and the vectors can be used interchangeably for use cases like searching image by text or searching text by image. One of <code>image</code>, <code>video</code> or <code>contextual_text</code> is required.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>dimension</code> <p>Optional. The number of embedding dimensions. Lower values offer decreased latency when using these embeddings for subsequent tasks, while higher values offer better accuracy. Available values: <code>128</code>, <code>256</code>, <code>512</code>, and <code>1408</code> (default).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>video_segment_config</code> <p>Optional. The specific video segments (in seconds) the embeddings are generated for.</p> <p> TYPE: <code>VideoSegmentConfig</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>MultiModalEmbeddingResponse</code> <p>The image and text embedding vectors.</p> <p> TYPE: <code>MultiModalEmbeddingResponse</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def get_embeddings(\n    self,\n    image: Optional[Image] = None,\n    video: Optional[Video] = None,\n    contextual_text: Optional[str] = None,\n    dimension: Optional[int] = None,\n    video_segment_config: Optional[VideoSegmentConfig] = None,\n) -&gt; \"MultiModalEmbeddingResponse\":\n    \"\"\"Gets embedding vectors from the provided image.\n\n    Args:\n        image (Image): Optional. The image to generate embeddings for. One of\n          `image`, `video`, or `contextual_text` is required.\n        video (Video): Optional. The video to generate embeddings for. One of\n          `image`, `video` or `contextual_text` is required.\n        contextual_text (str): Optional. Contextual text for your input image or video.\n          If provided, the model will also generate an embedding vector for the\n          provided contextual text. The returned image and text embedding\n          vectors are in the same semantic space with the same dimensionality,\n          and the vectors can be used interchangeably for use cases like\n          searching image by text or searching text by image. One of `image`, `video` or\n          `contextual_text` is required.\n        dimension (int): Optional. The number of embedding dimensions. Lower\n          values offer decreased latency when using these embeddings for\n          subsequent tasks, while higher values offer better accuracy.\n          Available values: `128`, `256`, `512`, and `1408` (default).\n        video_segment_config (VideoSegmentConfig): Optional. The specific\n          video segments (in seconds) the embeddings are generated for.\n\n    Returns:\n        MultiModalEmbeddingResponse:\n            The image and text embedding vectors.\n    \"\"\"\n\n    if not image and not video and not contextual_text:\n        raise ValueError(\n            \"One of `image`, `video`, or `contextual_text` is required.\"\n        )\n\n    instance = {}\n\n    if image:\n        if image._gcs_uri:  # pylint: disable=protected-access\n            instance[\"image\"] = {\n                \"gcsUri\": image._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"image\"] = {\n                \"bytesBase64Encoded\": image._as_base64_string()  # pylint: disable=protected-access\n            }\n\n    if video:\n        if video._gcs_uri:  # pylint: disable=protected-access\n            instance[\"video\"] = {\n                \"gcsUri\": video._gcs_uri  # pylint: disable=protected-access\n            }\n        else:\n            instance[\"video\"] = {\n                \"bytesBase64Encoded\": video._as_base64_string()  # pylint: disable=protected-access\n            }  # pylint: disable=protected-access\n\n        if video_segment_config:\n            instance[\"video\"][\"videoSegmentConfig\"] = {\n                \"startOffsetSec\": video_segment_config.start_offset_sec,\n                \"endOffsetSec\": video_segment_config.end_offset_sec,\n                \"intervalSec\": video_segment_config.interval_sec,\n            }\n\n    if contextual_text:\n        instance[\"text\"] = contextual_text\n\n    parameters = {}\n    if dimension:\n        parameters[\"dimension\"] = dimension\n\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n    image_embedding = response.predictions[0].get(\"imageEmbedding\")\n    video_embeddings = []\n    for video_embedding in response.predictions[0].get(\"videoEmbeddings\", []):\n        video_embeddings.append(\n            VideoEmbedding(\n                embedding=video_embedding[\"embedding\"],\n                start_offset_sec=video_embedding[\"startOffsetSec\"],\n                end_offset_sec=video_embedding[\"endOffsetSec\"],\n            )\n        )\n    text_embedding = (\n        response.predictions[0].get(\"textEmbedding\")\n        if \"textEmbedding\" in response.predictions[0]\n        else None\n    )\n    return MultiModalEmbeddingResponse(\n        image_embedding=image_embedding,\n        video_embeddings=video_embeddings,\n        _prediction_response=response,\n        text_embedding=text_embedding,\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingResponse","title":"MultiModalEmbeddingResponse  <code>dataclass</code>","text":"<pre><code>MultiModalEmbeddingResponse(\n    __module__=\"vertexai.vision_models\",\n    _prediction_response: Any,\n    image_embedding: Optional[List[float]] = None,\n    video_embeddings: Optional[List[VideoEmbedding]] = None,\n    text_embedding: Optional[List[float]] = None,\n)\n</code></pre> <p>The multimodal embedding response.</p> ATTRIBUTE DESCRIPTION <code>image_embedding</code> <p>Optional. The embedding vector generated from your image.</p> <p> TYPE: <code>List[float]</code> </p> <code>video_embeddings</code> <p>Optional. The embedding vectors generated from your video.</p> <p> TYPE: <code>List[VideoEmbedding]</code> </p> <code>text_embedding</code> <p>Optional. The embedding vector generated from the contextual text provided for your image or video.</p> <p> TYPE: <code>List[float]</code> </p>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingResponse.image_embedding","title":"image_embedding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_embedding: Optional[List[float]] = None\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingResponse.video_embeddings","title":"video_embeddings  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>video_embeddings: Optional[List[VideoEmbedding]] = None\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.MultiModalEmbeddingResponse.text_embedding","title":"text_embedding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text_embedding: Optional[List[float]] = None\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Video","title":"Video","text":"<pre><code>Video(\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n)\n</code></pre> <p>Video.</p> PARAMETER DESCRIPTION <code>video_bytes</code> <p>Video file bytes. Video can be in AVI, FLV, MKV, MOV, MP4, MPEG, MPG, WEBM, and WMV formats.</p> <p> TYPE: <code>Optional[bytes]</code> DEFAULT: <code>None</code> </p> <code>gcs_uri</code> <p>Image URI in Google Cloud Storage.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    video_bytes: Optional[bytes] = None,\n    gcs_uri: Optional[str] = None,\n):\n    \"\"\"Creates an `Image` object.\n\n    Args:\n        video_bytes: Video file bytes. Video can be in AVI, FLV, MKV, MOV,\n            MP4, MPEG, MPG, WEBM, and WMV formats.\n        gcs_uri: Image URI in Google Cloud Storage.\n    \"\"\"\n    if bool(video_bytes) == bool(gcs_uri):\n        raise ValueError(\"Either video_bytes or gcs_uri must be provided.\")\n\n    self._video_bytes = video_bytes\n    self._gcs_uri = gcs_uri\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Video.load_from_file","title":"load_from_file  <code>staticmethod</code>","text":"<pre><code>load_from_file(location: str) -&gt; Video\n</code></pre> <p>Loads video from local file or Google Cloud Storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path or Google Cloud Storage uri from where to load the video.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Video</code> <p>Loaded video as an <code>Video</code> object.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>@staticmethod\ndef load_from_file(location: str) -&gt; \"Video\":\n    \"\"\"Loads video from local file or Google Cloud Storage.\n\n    Args:\n        location: Local path or Google Cloud Storage uri from where to load\n            the video.\n\n    Returns:\n        Loaded video as an `Video` object.\n    \"\"\"\n    if location.startswith(\"gs://\"):\n        return Video(gcs_uri=location)\n\n    video_bytes = pathlib.Path(location).read_bytes()\n    video = Video(video_bytes=video_bytes)\n    return video\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.Video.save","title":"save","text":"<pre><code>save(location: str)\n</code></pre> <p>Saves video to a file.</p> PARAMETER DESCRIPTION <code>location</code> <p>Local path where to save the video.</p> <p> TYPE: <code>str</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def save(self, location: str):\n    \"\"\"Saves video to a file.\n\n    Args:\n        location: Local path where to save the video.\n    \"\"\"\n    pathlib.Path(location).write_bytes(self._video_bytes)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoEmbedding","title":"VideoEmbedding","text":"<pre><code>VideoEmbedding(\n    start_offset_sec: int,\n    end_offset_sec: int,\n    embedding: List[float],\n)\n</code></pre> <p>Embeddings generated from video with offset times.</p> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) of generated embeddings.</p> <p> TYPE: <code>int</code> </p> <code>embedding</code> <p>Generated embedding for interval.</p> <p> TYPE: <code>List[float]</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self, start_offset_sec: int, end_offset_sec: int, embedding: List[float]\n):\n    \"\"\"Creates a `VideoEmbedding` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) of generated embeddings.\n        end_offset_sec: End time offset (in seconds) of generated embeddings.\n        embedding: Generated embedding for interval.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.embedding = embedding\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoEmbedding.start_offset_sec","title":"start_offset_sec  <code>instance-attribute</code>","text":"<pre><code>start_offset_sec: int = start_offset_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoEmbedding.end_offset_sec","title":"end_offset_sec  <code>instance-attribute</code>","text":"<pre><code>end_offset_sec: int = end_offset_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoEmbedding.embedding","title":"embedding  <code>instance-attribute</code>","text":"<pre><code>embedding: List[float] = embedding\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoSegmentConfig","title":"VideoSegmentConfig","text":"<pre><code>VideoSegmentConfig(\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n)\n</code></pre> <p>The specific video segments (in seconds) the embeddings are generated for.</p> PARAMETER DESCRIPTION <code>start_offset_sec</code> <p>Start time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end_offset_sec</code> <p>End time offset (in seconds) to generate embeddings for.</p> <p> TYPE: <code>int</code> DEFAULT: <code>120</code> </p> <code>interval_sec</code> <p>Interval to divide video for generated embeddings.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def __init__(\n    self,\n    start_offset_sec: int = 0,\n    end_offset_sec: int = 120,\n    interval_sec: int = 16,\n):\n    \"\"\"Creates a `VideoSegmentConfig` object.\n\n    Args:\n        start_offset_sec: Start time offset (in seconds) to generate embeddings for.\n        end_offset_sec: End time offset (in seconds) to generate embeddings for.\n        interval_sec: Interval to divide video for generated embeddings.\n    \"\"\"\n    self.start_offset_sec = start_offset_sec\n    self.end_offset_sec = end_offset_sec\n    self.interval_sec = interval_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoSegmentConfig.start_offset_sec","title":"start_offset_sec  <code>instance-attribute</code>","text":"<pre><code>start_offset_sec: int = start_offset_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoSegmentConfig.end_offset_sec","title":"end_offset_sec  <code>instance-attribute</code>","text":"<pre><code>end_offset_sec: int = end_offset_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.VideoSegmentConfig.interval_sec","title":"interval_sec  <code>instance-attribute</code>","text":"<pre><code>interval_sec: int = interval_sec\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.WatermarkVerificationModel","title":"WatermarkVerificationModel","text":"<p>               Bases: <code>_ModelGardenModel</code></p> <p>Verifies if an image has a watermark.</p>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.WatermarkVerificationModel.verify_image","title":"verify_image","text":"<pre><code>verify_image(image: Image) -&gt; WatermarkVerificationResponse\n</code></pre> <p>Verifies the watermark of an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>The image to verify.</p> <p> TYPE: <code>Image</code> </p> RETURNS DESCRIPTION <code>WatermarkVerificationResponse</code> <p>A WatermarkVerificationResponse, containing the confidence level of</p> <code>WatermarkVerificationResponse</code> <p>the image being watermarked.</p> Source code in <code>vertexai\\vision_models\\_vision_models.py</code> <pre><code>def verify_image(self, image: Image) -&gt; WatermarkVerificationResponse:\n    \"\"\"Verifies the watermark of an image.\n\n    Args:\n        image: The image to verify.\n\n    Returns:\n        A WatermarkVerificationResponse, containing the confidence level of\n        the image being watermarked.\n    \"\"\"\n    if not image:\n        raise ValueError(\"Image is required.\")\n\n    instance = {}\n\n    if image._gcs_uri:\n        instance[\"image\"] = {\"gcsUri\": image._gcs_uri}\n    else:\n        instance[\"image\"] = {\"bytesBase64Encoded\": image._as_base64_string()}\n\n    parameters = {}\n    response = self._endpoint.predict(\n        instances=[instance],\n        parameters=parameters,\n    )\n\n    verification_likelihood = response.predictions[0].get(\"decision\")\n    return WatermarkVerificationResponse(\n        _prediction_response=response,\n        watermark_verification_result=verification_likelihood,\n    )\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.WatermarkVerificationResponse","title":"WatermarkVerificationResponse  <code>dataclass</code>","text":"<pre><code>WatermarkVerificationResponse(\n    __module__=\"vertexai.preview.vision_models\",\n    _prediction_response: Any,\n    watermark_verification_result: Optional[str] = None,\n)\n</code></pre>"},{"location":"vertexai/preview/vision_models/#vertexai.preview.vision_models.WatermarkVerificationResponse.watermark_verification_result","title":"watermark_verification_result  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>watermark_verification_result: Optional[str] = None\n</code></pre>"}]}