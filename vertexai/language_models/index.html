
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Google Cloud Vertex AI SDK.">
      
      
      
        <link rel="canonical" href="https://github.com/googleapis/python-aiplatform/vertexai/language_models/">
      
      
        <link rel="prev" href="../generative_models/">
      
      
        <link rel="next" href="../vision_models/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.25">
    
    
      
        <title>Language models - Vertex SDK</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#vertexai.language_models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Vertex SDK" class="md-header__button md-logo" aria-label="Vertex SDK" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Vertex SDK
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Language models
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/googleapis/python-aiplatform" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    googleapis/python-aiplatform
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Overview

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../generative_models/" class="md-tabs__link">
        
  
    
  
  Generative models

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Language models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../vision_models/" class="md-tabs__link">
        
  
    
  
  Vision models

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Vertex SDK" class="md-nav__button md-logo" aria-label="Vertex SDK" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Vertex SDK
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/googleapis/python-aiplatform" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    googleapis/python-aiplatform
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generative models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Language models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Language models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models" class="md-nav__link">
    <span class="md-ellipsis">
      language_models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage" class="md-nav__link">
    <span class="md-ellipsis">
      ChatMessage
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      ChatModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      ChatSession
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.start_chat" class="md-nav__link">
    <span class="md-ellipsis">
      start_chat
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair" class="md-nav__link">
    <span class="md-ellipsis">
      InputOutputTextPair
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbedding
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbeddingInput
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse" class="md-nav__link">
    <span class="md-ellipsis">
      TextGenerationResponse
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextGenerationResponse">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="md-nav__link">
    <span class="md-ellipsis">
      raw_prediction_response
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision models
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models" class="md-nav__link">
    <span class="md-ellipsis">
      language_models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage" class="md-nav__link">
    <span class="md-ellipsis">
      ChatMessage
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      ChatModel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      ChatSession
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.start_chat" class="md-nav__link">
    <span class="md-ellipsis">
      start_chat
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair" class="md-nav__link">
    <span class="md-ellipsis">
      InputOutputTextPair
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbedding
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbeddingInput
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse" class="md-nav__link">
    <span class="md-ellipsis">
      TextGenerationResponse
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextGenerationResponse">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="md-nav__link">
    <span class="md-ellipsis">
      raw_prediction_response
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Language models</h1>

<div class="doc doc-object doc-module">



<a id="vertexai.language_models"></a>
    <div class="doc doc-contents first">

      <p>Classes for working with language models.</p>



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.ChatMessage" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatMessage</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>A chat message.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.ChatMessage.content">content</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Content of the message.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.ChatMessage.author">author</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Author of the message.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2571</span>
<span class="normal">2572</span>
<span class="normal">2573</span>
<span class="normal">2574</span>
<span class="normal">2575</span>
<span class="normal">2576</span>
<span class="normal">2577</span>
<span class="normal">2578</span>
<span class="normal">2579</span>
<span class="normal">2580</span>
<span class="normal">2581</span>
<span class="normal">2582</span>
<span class="normal">2583</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">ChatMessage</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A chat message.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        content: Content of the message.</span>
<span class="sd">        author: Author of the message.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">content</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">author</span><span class="p">:</span> <span class="nb">str</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.ChatModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatModel</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatModelBase">_ChatModelBase</span></code>, <code><span title="vertexai.language_models._language_models._TunableChatModelMixin">_TunableChatModelMixin</span></code>, <code><span title="vertexai.language_models._language_models._RlhfTunableModelMixin">_RlhfTunableModelMixin</span></code></p>


      <p>ChatModel represents a language model that is capable of chat.</p>
<p>Examples::</p>
<div class="language-text highlight"><pre><span></span><code>chat_model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)

chat = chat_model.start_chat(
    context=&quot;My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.&quot;,
    examples=[
        InputOutputTextPair(
            input_text=&quot;Who do you work for?&quot;,
            output_text=&quot;I work for Ned.&quot;,
        ),
        InputOutputTextPair(
            input_text=&quot;What do I like?&quot;,
            output_text=&quot;Ned likes watching movies.&quot;,
        ),
    ],
    temperature=0.3,
)

chat.send_message(&quot;Do you know any cool events this weekend?&quot;)
</code></pre></div>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2631</span>
<span class="normal">2632</span>
<span class="normal">2633</span>
<span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span>
<span class="normal">2641</span>
<span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ChatModel</span><span class="p">(</span><span class="n">_ChatModelBase</span><span class="p">,</span> <span class="n">_TunableChatModelMixin</span><span class="p">,</span> <span class="n">_RlhfTunableModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatModel represents a language model that is capable of chat.</span>

<span class="sd">    Examples::</span>

<span class="sd">        chat_model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)</span>

<span class="sd">        chat = chat_model.start_chat(</span>
<span class="sd">            context=&quot;My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.&quot;,</span>
<span class="sd">            examples=[</span>
<span class="sd">                InputOutputTextPair(</span>
<span class="sd">                    input_text=&quot;Who do you work for?&quot;,</span>
<span class="sd">                    output_text=&quot;I work for Ned.&quot;,</span>
<span class="sd">                ),</span>
<span class="sd">                InputOutputTextPair(</span>
<span class="sd">                    input_text=&quot;What do I like?&quot;,</span>
<span class="sd">                    output_text=&quot;Ned likes watching movies.&quot;,</span>
<span class="sd">                ),</span>
<span class="sd">            ],</span>
<span class="sd">            temperature=0.3,</span>
<span class="sd">        )</span>

<span class="sd">        chat.send_message(&quot;Do you know any cool events this weekend?&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/chat_generation_1.0.0.yaml&quot;</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.ChatSession" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatSession</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatSessionBase">_ChatSessionBase</span></code></p>


      <p>ChatSession represents a chat session with a language model.</p>
<p>Within a chat session, the model keeps context and remembers the previous conversation.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3324</span>
<span class="normal">3325</span>
<span class="normal">3326</span>
<span class="normal">3327</span>
<span class="normal">3328</span>
<span class="normal">3329</span>
<span class="normal">3330</span>
<span class="normal">3331</span>
<span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ChatSession</span><span class="p">(</span><span class="n">_ChatSessionBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ChatSession represents a chat session with a language model.</span>

<span class="sd">    Within a chat session, the model keeps context and remembers the previous conversation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ChatModel</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.CodeChatModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">CodeChatModel</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatModelBase">_ChatModelBase</span></code>, <code><span title="vertexai.language_models._language_models._TunableChatModelMixin">_TunableChatModelMixin</span></code></p>


      <p>CodeChatModel represents a model that is capable of completing code.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <p>code_chat_model = CodeChatModel.from_pretrained("codechat-bison@001")</p>
<p>code_chat = code_chat_model.start_chat(
    context="I'm writing a large-scale enterprise application.",
    max_output_tokens=128,
    temperature=0.2,
)</p>
<p>code_chat.send_message("Please help write a function to calculate the min of two numbers")</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2707</span>
<span class="normal">2708</span>
<span class="normal">2709</span>
<span class="normal">2710</span>
<span class="normal">2711</span>
<span class="normal">2712</span>
<span class="normal">2713</span>
<span class="normal">2714</span>
<span class="normal">2715</span>
<span class="normal">2716</span>
<span class="normal">2717</span>
<span class="normal">2718</span>
<span class="normal">2719</span>
<span class="normal">2720</span>
<span class="normal">2721</span>
<span class="normal">2722</span>
<span class="normal">2723</span>
<span class="normal">2724</span>
<span class="normal">2725</span>
<span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CodeChatModel</span><span class="p">(</span><span class="n">_ChatModelBase</span><span class="p">,</span> <span class="n">_TunableChatModelMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CodeChatModel represents a model that is capable of completing code.</span>

<span class="sd">    Examples:</span>
<span class="sd">        code_chat_model = CodeChatModel.from_pretrained(&quot;codechat-bison@001&quot;)</span>

<span class="sd">        code_chat = code_chat_model.start_chat(</span>
<span class="sd">            context=&quot;I&#39;m writing a large-scale enterprise application.&quot;,</span>
<span class="sd">            max_output_tokens=128,</span>
<span class="sd">            temperature=0.2,</span>
<span class="sd">        )</span>

<span class="sd">        code_chat.send_message(&quot;Please help write a function to calculate the min of two numbers&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">_INSTANCE_SCHEMA_URI</span> <span class="o">=</span> <span class="s2">&quot;gs://google-cloud-aiplatform/schema/predict/instance/codechat_generation_1.0.0.yaml&quot;</span>

    <span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;CodeChatSession&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the code chat model.</span>

<span class="sd">        Args:</span>
<span class="sd">            context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">                For example, you can use context to specify words the model can or</span>
<span class="sd">                cannot use, topics to focus on or avoid, or the response format or style.</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `ChatSession` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">CodeChatSession</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatModel.start_chat" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">start_chat</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">start_chat</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CodeChatSession</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Starts a chat session with the code chat model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Context shapes how the model responds throughout the conversation.
For example, you can use context to specify words the model can or
cannot use, topics to focus on or avoid, or the response format or style.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.CodeChatSession" href="#vertexai.language_models.CodeChatSession">CodeChatSession</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">ChatSession</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;CodeChatSession&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the code chat model.</span>

<span class="sd">    Args:</span>
<span class="sd">        context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">            For example, you can use context to specify words the model can or</span>
<span class="sd">            cannot use, topics to focus on or avoid, or the response format or style.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `ChatSession` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">CodeChatSession</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.CodeChatSession" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">CodeChatSession</span>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatSessionBase">_ChatSessionBase</span></code></p>


      <p>CodeChatSession represents a chat session with code chat language model.</p>
<p>Within a code chat session, the model keeps context and remembers the previous converstion.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3357</span>
<span class="normal">3358</span>
<span class="normal">3359</span>
<span class="normal">3360</span>
<span class="normal">3361</span>
<span class="normal">3362</span>
<span class="normal">3363</span>
<span class="normal">3364</span>
<span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span>
<span class="normal">3382</span>
<span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span>
<span class="normal">3414</span>
<span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span>
<span class="normal">3444</span>
<span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span>
<span class="normal">3476</span>
<span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CodeChatSession</span><span class="p">(</span><span class="n">_ChatSessionBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;CodeChatSession represents a chat session with code chat language model.</span>

<span class="sd">    Within a code chat session, the model keeps context and remembers the previous converstion.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">CodeChatModel</span><span class="p">,</span>
        <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the code chat model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">                Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">                 Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the code chat model and gets a response.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">                Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">                 Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">            candidate_count: Number of candidates to return.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">            text produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_async</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span>

    <span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">        The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">        Args:</span>
<span class="sd">            message: Message to send to the model</span>
<span class="sd">            max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">            stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">                Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">            responses produced by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming_async</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
            <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiCandidateTextGenerationResponse</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the code chat model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].
Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].
 Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the code chat model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">             Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">        text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiCandidateTextGenerationResponse</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the code chat model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].
Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].
 Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the code chat model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">             Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">        text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_async</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message_streaming" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="vertexai.language_models.CodeChatSession.send_message_streaming_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming_async</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming_async</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.InputOutputTextPair" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">InputOutputTextPair</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>InputOutputTextPair represents a pair of input and output texts.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2561</span>
<span class="normal">2562</span>
<span class="normal">2563</span>
<span class="normal">2564</span>
<span class="normal">2565</span>
<span class="normal">2566</span>
<span class="normal">2567</span>
<span class="normal">2568</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">InputOutputTextPair</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;InputOutputTextPair represents a pair of input and output texts.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">output_text</span><span class="p">:</span> <span class="nb">str</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.TextEmbedding" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextEmbedding</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>Text embedding vector and statistics.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2520</span>
<span class="normal">2521</span>
<span class="normal">2522</span>
<span class="normal">2523</span>
<span class="normal">2524</span>
<span class="normal">2525</span>
<span class="normal">2526</span>
<span class="normal">2527</span>
<span class="normal">2528</span>
<span class="normal">2529</span>
<span class="normal">2530</span>
<span class="normal">2531</span>
<span class="normal">2532</span>
<span class="normal">2533</span>
<span class="normal">2534</span>
<span class="normal">2535</span>
<span class="normal">2536</span>
<span class="normal">2537</span>
<span class="normal">2538</span>
<span class="normal">2539</span>
<span class="normal">2540</span>
<span class="normal">2541</span>
<span class="normal">2542</span>
<span class="normal">2543</span>
<span class="normal">2544</span>
<span class="normal">2545</span>
<span class="normal">2546</span>
<span class="normal">2547</span>
<span class="normal">2548</span>
<span class="normal">2549</span>
<span class="normal">2550</span>
<span class="normal">2551</span>
<span class="normal">2552</span>
<span class="normal">2553</span>
<span class="normal">2554</span>
<span class="normal">2555</span>
<span class="normal">2556</span>
<span class="normal">2557</span>
<span class="normal">2558</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextEmbedding</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Text embedding vector and statistics.&quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
    <span class="n">statistics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TextEmbeddingStatistics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_parse_text_embedding_response</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">prediction_response</span><span class="p">:</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">,</span> <span class="n">prediction_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;TextEmbedding&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a `TextEmbedding` object from a prediction.</span>

<span class="sd">        Args:</span>
<span class="sd">            prediction_response: `aiplatform.models.Prediction` object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `TextEmbedding` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">[</span><span class="n">prediction_index</span><span class="p">]</span>
        <span class="n">is_prediction_from_pretrained_models</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="n">prediction</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Mapping</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">is_prediction_from_pretrained_models</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="s2">&quot;embeddings&quot;</span><span class="p">]</span>
            <span class="n">embedding_stats</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;statistics&quot;</span><span class="p">]</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
                <span class="n">values</span><span class="o">=</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">],</span>
                <span class="n">statistics</span><span class="o">=</span><span class="n">TextEmbeddingStatistics</span><span class="p">(</span>
                    <span class="n">token_count</span><span class="o">=</span><span class="n">embedding_stats</span><span class="p">[</span><span class="s2">&quot;token_count&quot;</span><span class="p">],</span>
                    <span class="n">truncated</span><span class="o">=</span><span class="n">embedding_stats</span><span class="p">[</span><span class="s2">&quot;truncated&quot;</span><span class="p">],</span>
                <span class="p">),</span>
                <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">_prediction_response</span><span class="o">=</span><span class="n">prediction_response</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.TextEmbeddingInput" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextEmbeddingInput</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>Structural text embedding input.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.TextEmbeddingInput.text">text</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The main text content to embed.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.TextEmbeddingInput.task_type">task_type</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The name of the downstream task the embeddings will be used for.
Valid values:
RETRIEVAL_QUERY
    Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT
    Specifies the given text is a document from the corpus being searched.
SEMANTIC_SIMILARITY
    Specifies the given text will be used for STS.
CLASSIFICATION
    Specifies that the given text will be classified.
CLUSTERING
    Specifies that the embeddings will be used for clustering.
QUESTION_ANSWERING
    Specifies that the embeddings will be used for question answering.
FACT_VERIFICATION
    Specifies that the embeddings will be used for fact verification.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><span title="vertexai.language_models.TextEmbeddingInput.title">title</span></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Optional identifier of the text content.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2073</span>
<span class="normal">2074</span>
<span class="normal">2075</span>
<span class="normal">2076</span>
<span class="normal">2077</span>
<span class="normal">2078</span>
<span class="normal">2079</span>
<span class="normal">2080</span>
<span class="normal">2081</span>
<span class="normal">2082</span>
<span class="normal">2083</span>
<span class="normal">2084</span>
<span class="normal">2085</span>
<span class="normal">2086</span>
<span class="normal">2087</span>
<span class="normal">2088</span>
<span class="normal">2089</span>
<span class="normal">2090</span>
<span class="normal">2091</span>
<span class="normal">2092</span>
<span class="normal">2093</span>
<span class="normal">2094</span>
<span class="normal">2095</span>
<span class="normal">2096</span>
<span class="normal">2097</span>
<span class="normal">2098</span>
<span class="normal">2099</span>
<span class="normal">2100</span>
<span class="normal">2101</span>
<span class="normal">2102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextEmbeddingInput</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Structural text embedding input.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        text: The main text content to embed.</span>
<span class="sd">        task_type: The name of the downstream task the embeddings will be used for.</span>
<span class="sd">            Valid values:</span>
<span class="sd">            RETRIEVAL_QUERY</span>
<span class="sd">                Specifies the given text is a query in a search/retrieval setting.</span>
<span class="sd">            RETRIEVAL_DOCUMENT</span>
<span class="sd">                Specifies the given text is a document from the corpus being searched.</span>
<span class="sd">            SEMANTIC_SIMILARITY</span>
<span class="sd">                Specifies the given text will be used for STS.</span>
<span class="sd">            CLASSIFICATION</span>
<span class="sd">                Specifies that the given text will be classified.</span>
<span class="sd">            CLUSTERING</span>
<span class="sd">                Specifies that the embeddings will be used for clustering.</span>
<span class="sd">            QUESTION_ANSWERING</span>
<span class="sd">                Specifies that the embeddings will be used for question answering.</span>
<span class="sd">            FACT_VERIFICATION</span>
<span class="sd">                Specifies that the embeddings will be used for fact verification.</span>
<span class="sd">        title: Optional identifier of the text content.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">task_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">title</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="vertexai.language_models.TextGenerationResponse" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextGenerationResponse</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


    <div class="doc doc-contents ">


      <p>TextGenerationResponse represents a response of a language model.
Attributes:
    text: The generated text
    is_blocked: Whether the the request was blocked.
    errors: The error codes indicate why the response was blocked.
        Learn more information about safety errors here:
        this documentation <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors">https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors</a>
    safety_attributes: Scores for safety attributes.
        Learn more about the safety attributes here:
        <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions">https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</a>
    grounding_metadata: Metadata for grounding.</p>

              <details class="quote">
                <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">TextGenerationResponse</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TextGenerationResponse represents a response of a language model.</span>
<span class="sd">    Attributes:</span>
<span class="sd">        text: The generated text</span>
<span class="sd">        is_blocked: Whether the the request was blocked.</span>
<span class="sd">        errors: The error codes indicate why the response was blocked.</span>
<span class="sd">            Learn more information about safety errors here:</span>
<span class="sd">            this documentation https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors</span>
<span class="sd">        safety_attributes: Scores for safety attributes.</span>
<span class="sd">            Learn more about the safety attributes here:</span>
<span class="sd">            https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</span>
<span class="sd">        grounding_metadata: Metadata for grounding.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s2">&quot;vertexai.language_models&quot;</span>

    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">is_blocked</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">errors</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">()</span>
    <span class="n">safety_attributes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
    <span class="n">grounding_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GroundingMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">text</span>
        <span class="c1"># Falling back to the full representation</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">grounding_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="s2">&quot;TextGenerationResponse(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;text=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, is_blocked=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_blocked</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, errors=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, safety_attributes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">safety_attributes</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, grounding_metadata=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">grounding_metadata</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;)&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="s2">&quot;TextGenerationResponse(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;text=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">text</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, is_blocked=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_blocked</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, errors=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;, safety_attributes=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">safety_attributes</span><span class="si">!r}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;)&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">raw_prediction_response</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Raw prediction response.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prediction_response</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">raw_prediction_response</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">raw_prediction_response</span><span class="p">:</span> <span class="n">Prediction</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Raw prediction response.</p>
    </div>

</div>





  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.tabs"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.081f42fc.min.js"></script>
      
    
  </body>
</html>