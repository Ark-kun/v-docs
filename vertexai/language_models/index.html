
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Google Cloud Vertex AI SDK.">
      
      
      
      
        <link rel="prev" href="../generative_models/">
      
      
        <link rel="next" href="../vision_models/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.25">
    
    
      
        <title>Language models - Vertex SDK</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#vertexai.language_models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Vertex SDK" class="md-header__button md-logo" aria-label="Vertex SDK" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Vertex SDK
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Language models
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/googleapis/python-aiplatform" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    googleapis/python-aiplatform
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Overview

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../generative_models/" class="md-tabs__link">
        
  
    
  
  Generative models

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Language models

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../vision_models/" class="md-tabs__link">
        
  
    
  
  Vision models

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../preview/" class="md-tabs__link">
          
  
  Preview

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Vertex SDK" class="md-nav__button md-logo" aria-label="Vertex SDK" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Vertex SDK
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/googleapis/python-aiplatform" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    googleapis/python-aiplatform
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generative models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Language models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Language models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models" class="md-nav__link">
    <span class="md-ellipsis">
      language_models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="language_models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage" class="md-nav__link">
    <span class="md-ellipsis">
      ChatMessage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ChatMessage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage.content" class="md-nav__link">
    <span class="md-ellipsis">
      content
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage.author" class="md-nav__link">
    <span class="md-ellipsis">
      author
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      ChatModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ChatModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.tune_model_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model_rlhf
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.start_chat" class="md-nav__link">
    <span class="md-ellipsis">
      start_chat
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      ChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.USER_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      USER_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.MODEL_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      MODEL_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.message_history" class="md-nav__link">
    <span class="md-ellipsis">
      message_history
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.start_chat" class="md-nav__link">
    <span class="md-ellipsis">
      start_chat
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.USER_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      USER_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.MODEL_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      MODEL_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.message_history" class="md-nav__link">
    <span class="md-ellipsis">
      message_history
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeGenerationModel" class="md-nav__link">
    <span class="md-ellipsis">
      CodeGenerationModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair" class="md-nav__link">
    <span class="md-ellipsis">
      InputOutputTextPair
    </span>
  </a>
  
    <nav class="md-nav" aria-label="InputOutputTextPair">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair.input_text" class="md-nav__link">
    <span class="md-ellipsis">
      input_text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair.output_text" class="md-nav__link">
    <span class="md-ellipsis">
      output_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding.values" class="md-nav__link">
    <span class="md-ellipsis">
      values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding.statistics" class="md-nav__link">
    <span class="md-ellipsis">
      statistics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbeddingInput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextEmbeddingInput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput.text" class="md-nav__link">
    <span class="md-ellipsis">
      text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput.task_type" class="md-nav__link">
    <span class="md-ellipsis">
      task_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput.title" class="md-nav__link">
    <span class="md-ellipsis">
      title
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbeddingModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextEmbeddingModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.deploy_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      deploy_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.get_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.get_embeddings_async" class="md-nav__link">
    <span class="md-ellipsis">
      get_embeddings_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel" class="md-nav__link">
    <span class="md-ellipsis">
      TextGenerationModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextGenerationModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.tune_model_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model_rlhf
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.batch_predict" class="md-nav__link">
    <span class="md-ellipsis">
      batch_predict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict_async" class="md-nav__link">
    <span class="md-ellipsis">
      predict_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      predict_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      predict_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse" class="md-nav__link">
    <span class="md-ellipsis">
      TextGenerationResponse
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextGenerationResponse">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.text" class="md-nav__link">
    <span class="md-ellipsis">
      text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.is_blocked" class="md-nav__link">
    <span class="md-ellipsis">
      is_blocked
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.errors" class="md-nav__link">
    <span class="md-ellipsis">
      errors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.safety_attributes" class="md-nav__link">
    <span class="md-ellipsis">
      safety_attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.grounding_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      grounding_metadata
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="md-nav__link">
    <span class="md-ellipsis">
      raw_prediction_response
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource" class="md-nav__link">
    <span class="md-ellipsis">
      GroundingSource
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GroundingSource">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource.WebSearch" class="md-nav__link">
    <span class="md-ellipsis">
      WebSearch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource.VertexAISearch" class="md-nav__link">
    <span class="md-ellipsis">
      VertexAISearch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource.InlineContext" class="md-nav__link">
    <span class="md-ellipsis">
      InlineContext
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../vision_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Preview
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Preview
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    vertexai.preview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preview/generative_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generative models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preview/language_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../preview/vision_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision models
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vertexai.language_models" class="md-nav__link">
    <span class="md-ellipsis">
      language_models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="language_models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage" class="md-nav__link">
    <span class="md-ellipsis">
      ChatMessage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ChatMessage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage.content" class="md-nav__link">
    <span class="md-ellipsis">
      content
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatMessage.author" class="md-nav__link">
    <span class="md-ellipsis">
      author
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      ChatModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ChatModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.tune_model_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model_rlhf
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatModel.start_chat" class="md-nav__link">
    <span class="md-ellipsis">
      start_chat
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      ChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.USER_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      USER_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.MODEL_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      MODEL_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.message_history" class="md-nav__link">
    <span class="md-ellipsis">
      message_history
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.ChatSession.send_message_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatModel.start_chat" class="md-nav__link">
    <span class="md-ellipsis">
      start_chat
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession" class="md-nav__link">
    <span class="md-ellipsis">
      CodeChatSession
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CodeChatSession">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.USER_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      USER_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.MODEL_AUTHOR" class="md-nav__link">
    <span class="md-ellipsis">
      MODEL_AUTHOR
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.message_history" class="md-nav__link">
    <span class="md-ellipsis">
      message_history
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message" class="md-nav__link">
    <span class="md-ellipsis">
      send_message
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeChatSession.send_message_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      send_message_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.CodeGenerationModel" class="md-nav__link">
    <span class="md-ellipsis">
      CodeGenerationModel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair" class="md-nav__link">
    <span class="md-ellipsis">
      InputOutputTextPair
    </span>
  </a>
  
    <nav class="md-nav" aria-label="InputOutputTextPair">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair.input_text" class="md-nav__link">
    <span class="md-ellipsis">
      input_text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.InputOutputTextPair.output_text" class="md-nav__link">
    <span class="md-ellipsis">
      output_text
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbedding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextEmbedding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding.values" class="md-nav__link">
    <span class="md-ellipsis">
      values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbedding.statistics" class="md-nav__link">
    <span class="md-ellipsis">
      statistics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbeddingInput
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextEmbeddingInput">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput.text" class="md-nav__link">
    <span class="md-ellipsis">
      text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput.task_type" class="md-nav__link">
    <span class="md-ellipsis">
      task_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingInput.title" class="md-nav__link">
    <span class="md-ellipsis">
      title
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel" class="md-nav__link">
    <span class="md-ellipsis">
      TextEmbeddingModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextEmbeddingModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.deploy_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      deploy_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.get_embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      get_embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextEmbeddingModel.get_embeddings_async" class="md-nav__link">
    <span class="md-ellipsis">
      get_embeddings_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel" class="md-nav__link">
    <span class="md-ellipsis">
      TextGenerationModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextGenerationModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.list_tuned_model_names" class="md-nav__link">
    <span class="md-ellipsis">
      list_tuned_model_names
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.get_tuned_model" class="md-nav__link">
    <span class="md-ellipsis">
      get_tuned_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.tune_model_rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model_rlhf
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.batch_predict" class="md-nav__link">
    <span class="md-ellipsis">
      batch_predict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.tune_model" class="md-nav__link">
    <span class="md-ellipsis">
      tune_model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict_async" class="md-nav__link">
    <span class="md-ellipsis">
      predict_async
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict_streaming" class="md-nav__link">
    <span class="md-ellipsis">
      predict_streaming
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationModel.predict_streaming_async" class="md-nav__link">
    <span class="md-ellipsis">
      predict_streaming_async
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse" class="md-nav__link">
    <span class="md-ellipsis">
      TextGenerationResponse
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TextGenerationResponse">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.text" class="md-nav__link">
    <span class="md-ellipsis">
      text
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.is_blocked" class="md-nav__link">
    <span class="md-ellipsis">
      is_blocked
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.errors" class="md-nav__link">
    <span class="md-ellipsis">
      errors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.safety_attributes" class="md-nav__link">
    <span class="md-ellipsis">
      safety_attributes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.grounding_metadata" class="md-nav__link">
    <span class="md-ellipsis">
      grounding_metadata
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="md-nav__link">
    <span class="md-ellipsis">
      raw_prediction_response
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource" class="md-nav__link">
    <span class="md-ellipsis">
      GroundingSource
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GroundingSource">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource.WebSearch" class="md-nav__link">
    <span class="md-ellipsis">
      WebSearch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource.VertexAISearch" class="md-nav__link">
    <span class="md-ellipsis">
      VertexAISearch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vertexai.language_models.GroundingSource.InlineContext" class="md-nav__link">
    <span class="md-ellipsis">
      InlineContext
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Language models</h1>

<div class="doc doc-object doc-module">



<h2 id="vertexai.language_models" class="doc doc-heading">
            <span class="doc doc-object-name doc-module-name">vertexai.language_models</span>


</h2>

    <div class="doc doc-contents first">

      <p>Classes for working with language models.</p>



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.ChatMessage" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatMessage</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">ChatMessage</span><span class="p">(</span>
    <span class="vm">__module__</span><span class="o">=</span><span class="s2">&quot;vertexai.language_models&quot;</span><span class="p">,</span>
    <span class="n">content</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="n">author</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">


      <p>A chat message.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="vertexai.language_models.ChatMessage.content" href="#vertexai.language_models.ChatMessage.content">content</a></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Content of the message.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="vertexai.language_models.ChatMessage.author" href="#vertexai.language_models.ChatMessage.author">author</a></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Author of the message.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>




  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.ChatMessage.content" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">content</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">content</span><span class="p">:</span> <span class="n">str</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.ChatMessage.author" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">author</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">author</span><span class="p">:</span> <span class="n">str</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.ChatModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatModel</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">ChatModel</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatModelBase">_ChatModelBase</span></code>, <code><span title="vertexai.language_models._language_models._TunableChatModelMixin">_TunableChatModelMixin</span></code>, <code><span title="vertexai.language_models._language_models._RlhfTunableModelMixin">_RlhfTunableModelMixin</span></code></p>


      <p>ChatModel represents a language model that is capable of chat.</p>
<p>Examples::</p>
<div class="language-text highlight"><pre><span></span><code>chat_model = ChatModel.from_pretrained(&quot;chat-bison@001&quot;)

chat = chat_model.start_chat(
    context=&quot;My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.&quot;,
    examples=[
        InputOutputTextPair(
            input_text=&quot;Who do you work for?&quot;,
            output_text=&quot;I work for Ned.&quot;,
        ),
        InputOutputTextPair(
            input_text=&quot;What do I like?&quot;,
            output_text=&quot;Ned likes watching movies.&quot;,
        ),
    ],
    temperature=0.3,
)

chat.send_message(&quot;Do you know any cool events this weekend?&quot;)
</code></pre></div>

      <p>This constructor should not be called directly.
Use <code class="language-python highlight"><span class="n">LanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="o">=...</span><span class="p">)</span></code> instead.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Identifier of a Vertex LLM. Example: "text-bison@001"</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>endpoint_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vertex Endpoint resource name for the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

                  <details class="quote">
                    <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a LanguageModel.</span>

<span class="sd">    This constructor should not be called directly.</span>
<span class="sd">    Use `LanguageModel.from_pretrained(model_name=...)` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_id: Identifier of a Vertex LLM. Example: &quot;text-bison@001&quot;</span>
<span class="sd">        endpoint_name: Vertex Endpoint resource name for the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatModel.list_tuned_model_names" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">list_tuned_model_names</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">list_tuned_model_names</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.Sequence">Sequence</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Lists the names of tuned models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Sequence">Sequence</span>[str]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of tuned models that can be used with the <code class="language-python highlight"><span class="n">get_tuned_model</span></code> method.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">list_tuned_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lists the names of tuned models.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of tuned models that can be used with the `get_tuned_model` method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)},</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_list_tuned_model_names</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">tuning_model_id</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatModel.get_tuned_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_tuned_model</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">get_tuned_model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">:</span> <span class="n">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModel">_LanguageModel</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Loads the specified tuned language model.</p>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">tuned_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModel&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads the specified tuned language model.&quot;&quot;&quot;</span>

    <span class="n">tuned_vertex_model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">)</span>
    <span class="n">tuned_model_labels</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span>

    <span class="k">if</span> <span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuned_model_labels</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The provided model </span><span class="si">{</span><span class="n">tuned_model_name</span><span class="si">}</span><span class="s2"> does not have a base model ID.&quot;</span>
        <span class="p">)</span>

    <span class="n">tuning_model_id</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span><span class="p">]</span>

    <span class="n">tuned_model_deployments</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">deployed_models</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tuned_model_deployments</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Deploying the model</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">()</span><span class="o">.</span><span class="n">resource_name</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model_deployments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">endpoint</span>

    <span class="n">base_model_id</span> <span class="o">=</span> <span class="n">_get_model_id_from_tuning_model_id</span><span class="p">(</span><span class="n">tuning_model_id</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="bp">cls</span><span class="p">},</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">interface_class</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatModel.tune_model_rlhf" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">tune_model_rlhf</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">tune_model_rlhf</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">prompt_data</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="pandas.core.frame.DataFrame">DataFrame</span></span><span class="p">],</span>
    <span class="n">preference_data</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="pandas.core.frame.DataFrame">DataFrame</span></span><span class="p">],</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_sequence_length</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target_sequence_length</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_learning_rate_multiplier</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">float</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_rate_multiplier</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">float</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">int</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kl_coeff</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Tunes a model using reinforcement learning from human feedback.</p>
<p>This method launches and returns an asynchronous model tuning job.
Usage:
<div class="language-text highlight"><pre><span></span><code>tuning_job = model.tune_model_rlhf(...)
... do some other work
tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete
</code></pre></div></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt_data</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A Pandas DataFrame or a URI pointing to data in JSON lines
format. The dataset schema is model-specific.
See <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset">https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="pandas.core.frame.DataFrame">DataFrame</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>preference_data</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A Pandas DataFrame or a URI pointing to data in JSON lines
format. The dataset schema is model-specific.
See <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset">https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="pandas.core.frame.DataFrame">DataFrame</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>model_display_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom display name for the tuned model.
If not provided, a default name will be created.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>prompt_sequence_length</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Maximum tokenized sequence length for input text.
Higher values increase memory overhead.
This value should be at most 8192. Default value is 512.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>target_sequence_length</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Maximum tokenized sequence length for target text.
Higher values increase memory overhead.
This value should be at most 1024. Default value is 64.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reward_model_learning_rate_multiplier</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Constant used to adjust the base
learning rate used when training a reward model. Multiply by a
number &gt; 1 to increase the magnitude of updates applied at each
training step or multiply by a number &lt; 1 to decrease the magnitude
of updates. Default value is 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reinforcement_learning_rate_multiplier</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Constant used to adjust the base
learning rate used during reinforcement learning. Multiply by a
number &gt; 1 to increase the magnitude of updates applied at each
training step or multiply by a number &lt; 1 to decrease the magnitude
of updates. Default value is 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reward_model_train_steps</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of steps to use when training a reward
model. Default value is 1000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reinforcement_learning_train_steps</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of reinforcement learning steps
to perform when tuning a base model. Default value is 1000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>kl_coeff</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Coefficient for KL penalty. This regularizes the policy model and
penalizes if it diverges from its initial distribution. If set to 0,
the reference language model is not loaded into memory. Default value
is 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>default_context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>This field lets the model know what task to perform.
Base models have been trained over a large set of varied instructions.
You can give a simple and intuitive description of the task and the
model will follow it, e.g. "Classify this movie review as positive or
negative" or "Translate this sentence to Danish". Do not specify this
if your dataset already prepends the instruction to the inputs field.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_job_location</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>GCP location where the tuning job should be run.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>accelerator_type</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Type of accelerator to use. Can be "TPU" or "GPU".</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_evaluation_spec</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Evaluation settings to use during tuning.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">LanguageModelTuningJob</span></code> object that represents the tuning job.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Calling <code class="language-python highlight"><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span></code> blocks until the tuning is complete and returns a <code class="language-python highlight"><span class="n">LanguageModel</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the "tuning_job_location" value is not supported</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the model does not support tuning</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tune_model_rlhf</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">prompt_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="n">preference_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kl_coeff</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tunes a model using reinforcement learning from human feedback.</span>

<span class="sd">    This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">    Usage:</span>
<span class="sd">    ```</span>
<span class="sd">    tuning_job = model.tune_model_rlhf(...)</span>
<span class="sd">    ... do some other work</span>
<span class="sd">    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt_data: A Pandas DataFrame or a URI pointing to data in JSON lines</span>
<span class="sd">            format. The dataset schema is model-specific.</span>
<span class="sd">            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset</span>
<span class="sd">        preference_data: A Pandas DataFrame or a URI pointing to data in JSON lines</span>
<span class="sd">            format. The dataset schema is model-specific.</span>
<span class="sd">            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset</span>
<span class="sd">        model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            If not provided, a default name will be created.</span>
<span class="sd">        prompt_sequence_length: Maximum tokenized sequence length for input text.</span>
<span class="sd">            Higher values increase memory overhead.</span>
<span class="sd">            This value should be at most 8192. Default value is 512.</span>
<span class="sd">        target_sequence_length: Maximum tokenized sequence length for target text.</span>
<span class="sd">            Higher values increase memory overhead.</span>
<span class="sd">            This value should be at most 1024. Default value is 64.</span>
<span class="sd">        reward_model_learning_rate_multiplier: Constant used to adjust the base</span>
<span class="sd">            learning rate used when training a reward model. Multiply by a</span>
<span class="sd">            number &gt; 1 to increase the magnitude of updates applied at each</span>
<span class="sd">            training step or multiply by a number &lt; 1 to decrease the magnitude</span>
<span class="sd">            of updates. Default value is 1.0.</span>
<span class="sd">        reinforcement_learning_rate_multiplier: Constant used to adjust the base</span>
<span class="sd">            learning rate used during reinforcement learning. Multiply by a</span>
<span class="sd">            number &gt; 1 to increase the magnitude of updates applied at each</span>
<span class="sd">            training step or multiply by a number &lt; 1 to decrease the magnitude</span>
<span class="sd">            of updates. Default value is 1.0.</span>
<span class="sd">        reward_model_train_steps: Number of steps to use when training a reward</span>
<span class="sd">            model. Default value is 1000.</span>
<span class="sd">        reinforcement_learning_train_steps: Number of reinforcement learning steps</span>
<span class="sd">            to perform when tuning a base model. Default value is 1000.</span>
<span class="sd">        kl_coeff: Coefficient for KL penalty. This regularizes the policy model and</span>
<span class="sd">            penalizes if it diverges from its initial distribution. If set to 0,</span>
<span class="sd">            the reference language model is not loaded into memory. Default value</span>
<span class="sd">            is 0.1.</span>
<span class="sd">        default_context: This field lets the model know what task to perform.</span>
<span class="sd">            Base models have been trained over a large set of varied instructions.</span>
<span class="sd">            You can give a simple and intuitive description of the task and the</span>
<span class="sd">            model will follow it, e.g. &quot;Classify this movie review as positive or</span>
<span class="sd">            negative&quot; or &quot;Translate this sentence to Danish&quot;. Do not specify this</span>
<span class="sd">            if your dataset already prepends the instruction to the inputs field.</span>
<span class="sd">        tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">        accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">        tuning_evaluation_spec: Evaluation settings to use during tuning.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">        RuntimeError: If the model does not support tuning</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tuning_job_location</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tuning_job_location</span> <span class="ow">or</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">location</span>
    <span class="p">)</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">tensorboard_resource_id</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">tuning_evaluation_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_unused_rlhf_eval_specs</span><span class="p">(</span><span class="n">tuning_evaluation_spec</span><span class="p">)</span>

        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_data</span>
        <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">eval_dataset</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;evaluation_data must be a GCS URI that starts with gs://&quot;</span>
            <span class="p">)</span>

        <span class="n">tensorboard_resource_id</span> <span class="o">=</span> <span class="n">_get_tensorboard_resource_id_from_evaluation_spec</span><span class="p">(</span>
            <span class="n">tuning_evaluation_spec</span><span class="p">,</span> <span class="n">tuning_job_location</span>
        <span class="p">)</span>
    <span class="n">prompt_dataset_uri</span> <span class="o">=</span> <span class="n">_maybe_upload_training_data</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">prompt_data</span><span class="p">,</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">preference_dataset_uri</span> <span class="o">=</span> <span class="n">_maybe_upload_training_data</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">preference_data</span><span class="p">,</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">accelerator_type</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">accelerator_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_ACCELERATOR_TYPES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unsupported accelerator type: </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; Supported types: </span><span class="si">{</span><span class="n">_ACCELERATOR_TYPES</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="n">tuning_parameters</span> <span class="o">=</span> <span class="n">_RlhfTuningParameters</span><span class="p">(</span>
        <span class="n">prompt_dataset</span><span class="o">=</span><span class="n">prompt_dataset_uri</span><span class="p">,</span>
        <span class="n">preference_dataset</span><span class="o">=</span><span class="n">preference_dataset_uri</span><span class="p">,</span>
        <span class="n">large_model_reference</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
        <span class="n">prompt_sequence_length</span><span class="o">=</span><span class="n">prompt_sequence_length</span><span class="p">,</span>
        <span class="n">target_sequence_length</span><span class="o">=</span><span class="n">target_sequence_length</span><span class="p">,</span>
        <span class="n">reward_model_learning_rate_multiplier</span><span class="o">=</span><span class="n">reward_model_learning_rate_multiplier</span><span class="p">,</span>
        <span class="n">reinforcement_learning_rate_multiplier</span><span class="o">=</span><span class="n">reinforcement_learning_rate_multiplier</span><span class="p">,</span>
        <span class="n">reward_model_train_steps</span><span class="o">=</span><span class="n">reward_model_train_steps</span><span class="p">,</span>
        <span class="n">reinforcement_learning_train_steps</span><span class="o">=</span><span class="n">reinforcement_learning_train_steps</span><span class="p">,</span>
        <span class="n">kl_coeff</span><span class="o">=</span><span class="n">kl_coeff</span><span class="p">,</span>
        <span class="n">instruction</span><span class="o">=</span><span class="n">default_context</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
        <span class="n">tensorboard_resource_id</span><span class="o">=</span><span class="n">tensorboard_resource_id</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tune_model_rlhf</span><span class="p">(</span>
        <span class="n">tuning_parameters</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatModel.tune_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">tune_model</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">tune_model</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="pandas.core.frame.DataFrame">DataFrame</span></span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Tunes a model based on training data.</p>
<p>This method launches and returns an asynchronous model tuning job.
Usage:
<div class="language-text highlight"><pre><span></span><code>tuning_job = model.tune_model(...)
... do some other work
tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete
</code></pre></div></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>training_data</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A Pandas DataFrame or a URI pointing to data in JSON lines format.
The dataset schema is model-specific.
See <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format">https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="pandas.core.frame.DataFrame">DataFrame</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>train_steps</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of training batches to tune on (batch size is 8 samples).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>learning_rate</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Deprecated. Use learning_rate_multiplier instead.
Learning rate to use in tuning.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>learning_rate_multiplier</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Learning rate multiplier to use in tuning.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_job_location</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>GCP location where the tuning job should be run.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuned_model_location</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>GCP location where the tuned model should be deployed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>model_display_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom display name for the tuned model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>default_context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The context to use for all training samples by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>accelerator_type</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Type of accelerator to use. Can be "TPU" or "GPU".</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_evaluation_spec</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Specification for the model evaluation during tuning.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">LanguageModelTuningJob</span></code> object that represents the tuning job.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Calling <code class="language-python highlight"><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span></code> blocks until the tuning is complete and returns a <code class="language-python highlight"><span class="n">LanguageModel</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the "tuning_job_location" value is not supported</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the "tuned_model_location" value is not supported</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the model does not support tuning</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AttributeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If any attribute in the "tuning_evaluation_spec" is not supported</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">    This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">    Usage:</span>
<span class="sd">    ```</span>
<span class="sd">    tuning_job = model.tune_model(...)</span>
<span class="sd">    ... do some other work</span>
<span class="sd">    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">            The dataset schema is model-specific.</span>
<span class="sd">            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">        train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">        learning_rate: Deprecated. Use learning_rate_multiplier instead.</span>
<span class="sd">            Learning rate to use in tuning.</span>
<span class="sd">        learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">        tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">        tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">        model_display_name: Custom display name for the tuned model.</span>
<span class="sd">        default_context: The context to use for all training samples by default.</span>
<span class="sd">        accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">        tuning_evaluation_spec: Specification for the model evaluation during tuning.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">        ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">        RuntimeError: If the model does not support tuning</span>
<span class="sd">        AttributeError: If any attribute in the &quot;tuning_evaluation_spec&quot; is not supported</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">tuning_evaluation_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">unsupported_chat_model_tuning_eval_spec</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;evaluation_data&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_data</span><span class="p">,</span>
            <span class="s2">&quot;evaluation_interval&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_interval</span><span class="p">,</span>
            <span class="s2">&quot;enable_early_stopping&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">enable_early_stopping</span><span class="p">,</span>
            <span class="s2">&quot;enable_checkpoint_selection&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">enable_checkpoint_selection</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">att_name</span><span class="p">,</span> <span class="n">att_value</span> <span class="ow">in</span> <span class="n">unsupported_chat_model_tuning_eval_spec</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">att_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;ChatModel and CodeChatModel only support tensorboard as attribute for TuningEvaluationSpec&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;found attribute name </span><span class="si">{</span><span class="n">att_name</span><span class="si">}</span><span class="s2"> with value </span><span class="si">{</span><span class="n">att_value</span><span class="si">}</span><span class="s2">, please leave </span><span class="si">{</span><span class="n">att_name</span><span class="si">}</span><span class="s2"> to None&quot;</span>
                    <span class="p">)</span>
                <span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="o">=</span><span class="n">learning_rate_multiplier</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
        <span class="n">default_context</span><span class="o">=</span><span class="n">default_context</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="o">=</span><span class="n">tuning_evaluation_spec</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatModel.start_chat" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">start_chat</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">start_chat</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.InputOutputTextPair" href="#vertexai.language_models.InputOutputTextPair">InputOutputTextPair</a></span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatMessage" href="#vertexai.language_models.ChatMessage">ChatMessage</a></span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatSession" href="#vertexai.language_models.ChatSession">ChatSession</a></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Starts a chat session with the model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Context shapes how the model responds throughout the conversation.
For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>examples</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>List of structured messages to the model to learn how to respond to the conversation.
A list of <code class="language-python highlight"><span class="n">InputOutputTextPair</span></code> objects.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.InputOutputTextPair" href="#vertexai.language_models.InputOutputTextPair">InputOutputTextPair</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>message_history</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of previously sent and received messages.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatMessage" href="#vertexai.language_models.ChatMessage">ChatMessage</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatSession" href="#vertexai.language_models.ChatSession">ChatSession</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">ChatSession</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2589</span>
<span class="normal">2590</span>
<span class="normal">2591</span>
<span class="normal">2592</span>
<span class="normal">2593</span>
<span class="normal">2594</span>
<span class="normal">2595</span>
<span class="normal">2596</span>
<span class="normal">2597</span>
<span class="normal">2598</span>
<span class="normal">2599</span>
<span class="normal">2600</span>
<span class="normal">2601</span>
<span class="normal">2602</span>
<span class="normal">2603</span>
<span class="normal">2604</span>
<span class="normal">2605</span>
<span class="normal">2606</span>
<span class="normal">2607</span>
<span class="normal">2608</span>
<span class="normal">2609</span>
<span class="normal">2610</span>
<span class="normal">2611</span>
<span class="normal">2612</span>
<span class="normal">2613</span>
<span class="normal">2614</span>
<span class="normal">2615</span>
<span class="normal">2616</span>
<span class="normal">2617</span>
<span class="normal">2618</span>
<span class="normal">2619</span>
<span class="normal">2620</span>
<span class="normal">2621</span>
<span class="normal">2622</span>
<span class="normal">2623</span>
<span class="normal">2624</span>
<span class="normal">2625</span>
<span class="normal">2626</span>
<span class="normal">2627</span>
<span class="normal">2628</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ChatSession&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">            For example, you can use context to specify words the model can or cannot use, topics to focus on or avoid, or the response format or style</span>
<span class="sd">        examples: List of structured messages to the model to learn how to respond to the conversation.</span>
<span class="sd">            A list of `InputOutputTextPair` objects.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">        message_history: A list of previously sent and received messages.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `ChatSession` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ChatSession</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.ChatSession" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">ChatSession</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">ChatSession</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatModel" href="#vertexai.language_models.ChatModel">ChatModel</a></span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.InputOutputTextPair" href="#vertexai.language_models.InputOutputTextPair">InputOutputTextPair</a></span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatMessage" href="#vertexai.language_models.ChatMessage">ChatMessage</a></span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatSessionBase">_ChatSessionBase</span></code></p>


      <p>ChatSession represents a chat session with a language model.</p>
<p>Within a chat session, the model keeps context and remembers the previous conversation.</p>

                  <details class="quote">
                    <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3332</span>
<span class="normal">3333</span>
<span class="normal">3334</span>
<span class="normal">3335</span>
<span class="normal">3336</span>
<span class="normal">3337</span>
<span class="normal">3338</span>
<span class="normal">3339</span>
<span class="normal">3340</span>
<span class="normal">3341</span>
<span class="normal">3342</span>
<span class="normal">3343</span>
<span class="normal">3344</span>
<span class="normal">3345</span>
<span class="normal">3346</span>
<span class="normal">3347</span>
<span class="normal">3348</span>
<span class="normal">3349</span>
<span class="normal">3350</span>
<span class="normal">3351</span>
<span class="normal">3352</span>
<span class="normal">3353</span>
<span class="normal">3354</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">ChatModel</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">examples</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">InputOutputTextPair</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
        <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.ChatSession.USER_AUTHOR" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">USER_AUTHOR</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">USER_AUTHOR</span> <span class="o">=</span> <span class="s1">&#39;user&#39;</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.ChatSession.MODEL_AUTHOR" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">MODEL_AUTHOR</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">MODEL_AUTHOR</span> <span class="o">=</span> <span class="s1">&#39;bot&#39;</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.ChatSession.message_history" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">message_history</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">message_history</span><span class="p">:</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatMessage" href="#vertexai.language_models.ChatMessage">ChatMessage</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List of previous messages.</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatSession.send_message" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a></span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the language model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>grounding_source</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2988</span>
<span class="normal">2989</span>
<span class="normal">2990</span>
<span class="normal">2991</span>
<span class="normal">2992</span>
<span class="normal">2993</span>
<span class="normal">2994</span>
<span class="normal">2995</span>
<span class="normal">2996</span>
<span class="normal">2997</span>
<span class="normal">2998</span>
<span class="normal">2999</span>
<span class="normal">3000</span>
<span class="normal">3001</span>
<span class="normal">3002</span>
<span class="normal">3003</span>
<span class="normal">3004</span>
<span class="normal">3005</span>
<span class="normal">3006</span>
<span class="normal">3007</span>
<span class="normal">3008</span>
<span class="normal">3009</span>
<span class="normal">3010</span>
<span class="normal">3011</span>
<span class="normal">3012</span>
<span class="normal">3013</span>
<span class="normal">3014</span>
<span class="normal">3015</span>
<span class="normal">3016</span>
<span class="normal">3017</span>
<span class="normal">3018</span>
<span class="normal">3019</span>
<span class="normal">3020</span>
<span class="normal">3021</span>
<span class="normal">3022</span>
<span class="normal">3023</span>
<span class="normal">3024</span>
<span class="normal">3025</span>
<span class="normal">3026</span>
<span class="normal">3027</span>
<span class="normal">3028</span>
<span class="normal">3029</span>
<span class="normal">3030</span>
<span class="normal">3031</span>
<span class="normal">3032</span>
<span class="normal">3033</span>
<span class="normal">3034</span>
<span class="normal">3035</span>
<span class="normal">3036</span>
<span class="normal">3037</span>
<span class="normal">3038</span>
<span class="normal">3039</span>
<span class="normal">3040</span>
<span class="normal">3041</span>
<span class="normal">3042</span>
<span class="normal">3043</span>
<span class="normal">3044</span>
<span class="normal">3045</span>
<span class="normal">3046</span>
<span class="normal">3047</span>
<span class="normal">3048</span>
<span class="normal">3049</span>
<span class="normal">3050</span>
<span class="normal">3051</span>
<span class="normal">3052</span>
<span class="normal">3053</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>
<span class="sd">        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">        text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
        <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">response_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
        <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
    <span class="p">)</span>
    <span class="n">response_text</span> <span class="o">=</span> <span class="n">response_obj</span><span class="o">.</span><span class="n">text</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">response_obj</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatSession.send_message_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a></span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the language model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>grounding_source</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>the text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3055</span>
<span class="normal">3056</span>
<span class="normal">3057</span>
<span class="normal">3058</span>
<span class="normal">3059</span>
<span class="normal">3060</span>
<span class="normal">3061</span>
<span class="normal">3062</span>
<span class="normal">3063</span>
<span class="normal">3064</span>
<span class="normal">3065</span>
<span class="normal">3066</span>
<span class="normal">3067</span>
<span class="normal">3068</span>
<span class="normal">3069</span>
<span class="normal">3070</span>
<span class="normal">3071</span>
<span class="normal">3072</span>
<span class="normal">3073</span>
<span class="normal">3074</span>
<span class="normal">3075</span>
<span class="normal">3076</span>
<span class="normal">3077</span>
<span class="normal">3078</span>
<span class="normal">3079</span>
<span class="normal">3080</span>
<span class="normal">3081</span>
<span class="normal">3082</span>
<span class="normal">3083</span>
<span class="normal">3084</span>
<span class="normal">3085</span>
<span class="normal">3086</span>
<span class="normal">3087</span>
<span class="normal">3088</span>
<span class="normal">3089</span>
<span class="normal">3090</span>
<span class="normal">3091</span>
<span class="normal">3092</span>
<span class="normal">3093</span>
<span class="normal">3094</span>
<span class="normal">3095</span>
<span class="normal">3096</span>
<span class="normal">3097</span>
<span class="normal">3098</span>
<span class="normal">3099</span>
<span class="normal">3100</span>
<span class="normal">3101</span>
<span class="normal">3102</span>
<span class="normal">3103</span>
<span class="normal">3104</span>
<span class="normal">3105</span>
<span class="normal">3106</span>
<span class="normal">3107</span>
<span class="normal">3108</span>
<span class="normal">3109</span>
<span class="normal">3110</span>
<span class="normal">3111</span>
<span class="normal">3112</span>
<span class="normal">3113</span>
<span class="normal">3114</span>
<span class="normal">3115</span>
<span class="normal">3116</span>
<span class="normal">3117</span>
<span class="normal">3118</span>
<span class="normal">3119</span>
<span class="normal">3120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>
<span class="sd">        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains</span>
<span class="sd">        the text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict_async</span><span class="p">(</span>
        <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">response_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
        <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
    <span class="p">)</span>
    <span class="n">response_text</span> <span class="o">=</span> <span class="n">response_obj</span><span class="o">.</span><span class="n">text</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">response_obj</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatSession.send_message_streaming" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.Iterator">Iterator</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">YIELDS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3122</span>
<span class="normal">3123</span>
<span class="normal">3124</span>
<span class="normal">3125</span>
<span class="normal">3126</span>
<span class="normal">3127</span>
<span class="normal">3128</span>
<span class="normal">3129</span>
<span class="normal">3130</span>
<span class="normal">3131</span>
<span class="normal">3132</span>
<span class="normal">3133</span>
<span class="normal">3134</span>
<span class="normal">3135</span>
<span class="normal">3136</span>
<span class="normal">3137</span>
<span class="normal">3138</span>
<span class="normal">3139</span>
<span class="normal">3140</span>
<span class="normal">3141</span>
<span class="normal">3142</span>
<span class="normal">3143</span>
<span class="normal">3144</span>
<span class="normal">3145</span>
<span class="normal">3146</span>
<span class="normal">3147</span>
<span class="normal">3148</span>
<span class="normal">3149</span>
<span class="normal">3150</span>
<span class="normal">3151</span>
<span class="normal">3152</span>
<span class="normal">3153</span>
<span class="normal">3154</span>
<span class="normal">3155</span>
<span class="normal">3156</span>
<span class="normal">3157</span>
<span class="normal">3158</span>
<span class="normal">3159</span>
<span class="normal">3160</span>
<span class="normal">3161</span>
<span class="normal">3162</span>
<span class="normal">3163</span>
<span class="normal">3164</span>
<span class="normal">3165</span>
<span class="normal">3166</span>
<span class="normal">3167</span>
<span class="normal">3168</span>
<span class="normal">3169</span>
<span class="normal">3170</span>
<span class="normal">3171</span>
<span class="normal">3172</span>
<span class="normal">3173</span>
<span class="normal">3174</span>
<span class="normal">3175</span>
<span class="normal">3176</span>
<span class="normal">3177</span>
<span class="normal">3178</span>
<span class="normal">3179</span>
<span class="normal">3180</span>
<span class="normal">3181</span>
<span class="normal">3182</span>
<span class="normal">3183</span>
<span class="normal">3184</span>
<span class="normal">3185</span>
<span class="normal">3186</span>
<span class="normal">3187</span>
<span class="normal">3188</span>
<span class="normal">3189</span>
<span class="normal">3190</span>
<span class="normal">3191</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Yields:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_service_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span>

    <span class="n">full_response_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

    <span class="k">for</span> <span class="p">(</span>
        <span class="n">prediction_dict</span>
    <span class="p">)</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict</span><span class="p">(</span>
        <span class="n">prediction_service_client</span><span class="o">=</span><span class="n">prediction_service_client</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
        <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">prediction_response</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
            <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
            <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">text_generation_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
            <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
        <span class="p">)</span>
        <span class="n">full_response_text</span> <span class="o">+=</span> <span class="n">text_generation_response</span><span class="o">.</span><span class="n">text</span>
        <span class="k">yield</span> <span class="n">text_generation_response</span>

    <span class="c1"># We only add the question and answer to the history if/when the answer</span>
    <span class="c1"># was read fully. Otherwise, the answer would have been truncated.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">full_response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.ChatSession.send_message_streaming_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.AsyncIterator">AsyncIterator</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">YIELDS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3193</span>
<span class="normal">3194</span>
<span class="normal">3195</span>
<span class="normal">3196</span>
<span class="normal">3197</span>
<span class="normal">3198</span>
<span class="normal">3199</span>
<span class="normal">3200</span>
<span class="normal">3201</span>
<span class="normal">3202</span>
<span class="normal">3203</span>
<span class="normal">3204</span>
<span class="normal">3205</span>
<span class="normal">3206</span>
<span class="normal">3207</span>
<span class="normal">3208</span>
<span class="normal">3209</span>
<span class="normal">3210</span>
<span class="normal">3211</span>
<span class="normal">3212</span>
<span class="normal">3213</span>
<span class="normal">3214</span>
<span class="normal">3215</span>
<span class="normal">3216</span>
<span class="normal">3217</span>
<span class="normal">3218</span>
<span class="normal">3219</span>
<span class="normal">3220</span>
<span class="normal">3221</span>
<span class="normal">3222</span>
<span class="normal">3223</span>
<span class="normal">3224</span>
<span class="normal">3225</span>
<span class="normal">3226</span>
<span class="normal">3227</span>
<span class="normal">3228</span>
<span class="normal">3229</span>
<span class="normal">3230</span>
<span class="normal">3231</span>
<span class="normal">3232</span>
<span class="normal">3233</span>
<span class="normal">3234</span>
<span class="normal">3235</span>
<span class="normal">3236</span>
<span class="normal">3237</span>
<span class="normal">3238</span>
<span class="normal">3239</span>
<span class="normal">3240</span>
<span class="normal">3241</span>
<span class="normal">3242</span>
<span class="normal">3243</span>
<span class="normal">3244</span>
<span class="normal">3245</span>
<span class="normal">3246</span>
<span class="normal">3247</span>
<span class="normal">3248</span>
<span class="normal">3249</span>
<span class="normal">3250</span>
<span class="normal">3251</span>
<span class="normal">3252</span>
<span class="normal">3253</span>
<span class="normal">3254</span>
<span class="normal">3255</span>
<span class="normal">3256</span>
<span class="normal">3257</span>
<span class="normal">3258</span>
<span class="normal">3259</span>
<span class="normal">3260</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Yields:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_request</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_service_async_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_async_client</span>

    <span class="n">full_response_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

    <span class="k">async</span> <span class="k">for</span> <span class="n">prediction_dict</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict_async</span><span class="p">(</span>
        <span class="n">prediction_service_async_client</span><span class="o">=</span><span class="n">prediction_service_async_client</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
        <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">prediction_response</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
            <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
            <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">text_generation_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_chat_prediction_response</span><span class="p">(</span>
            <span class="n">prediction_response</span><span class="o">=</span><span class="n">prediction_response</span>
        <span class="p">)</span>
        <span class="n">full_response_text</span> <span class="o">+=</span> <span class="n">text_generation_response</span><span class="o">.</span><span class="n">text</span>
        <span class="k">yield</span> <span class="n">text_generation_response</span>

    <span class="c1"># We only add the question and answer to the history if/when the answer</span>
    <span class="c1"># was read fully. Otherwise, the answer would have been truncated.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">USER_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_message_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">ChatMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">full_response_text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MODEL_AUTHOR</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.CodeChatModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">CodeChatModel</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">CodeChatModel</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatModelBase">_ChatModelBase</span></code>, <code><span title="vertexai.language_models._language_models._TunableChatModelMixin">_TunableChatModelMixin</span></code></p>


      <p>CodeChatModel represents a model that is capable of completing code.</p>


<p><span class="doc-section-title">Examples:</span></p>
    <p>code_chat_model = CodeChatModel.from_pretrained("codechat-bison@001")</p>
<p>code_chat = code_chat_model.start_chat(
    context="I'm writing a large-scale enterprise application.",
    max_output_tokens=128,
    temperature=0.2,
)</p>
<p>code_chat.send_message("Please help write a function to calculate the min of two numbers")</p>

      <p>This constructor should not be called directly.
Use <code class="language-python highlight"><span class="n">LanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="o">=...</span><span class="p">)</span></code> instead.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Identifier of a Vertex LLM. Example: "text-bison@001"</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>endpoint_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vertex Endpoint resource name for the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

                  <details class="quote">
                    <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a LanguageModel.</span>

<span class="sd">    This constructor should not be called directly.</span>
<span class="sd">    Use `LanguageModel.from_pretrained(model_name=...)` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_id: Identifier of a Vertex LLM. Example: &quot;text-bison@001&quot;</span>
<span class="sd">        endpoint_name: Vertex Endpoint resource name for the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatModel.list_tuned_model_names" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">list_tuned_model_names</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">list_tuned_model_names</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.Sequence">Sequence</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Lists the names of tuned models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Sequence">Sequence</span>[str]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of tuned models that can be used with the <code class="language-python highlight"><span class="n">get_tuned_model</span></code> method.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">list_tuned_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lists the names of tuned models.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of tuned models that can be used with the `get_tuned_model` method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)},</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_list_tuned_model_names</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">tuning_model_id</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatModel.get_tuned_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_tuned_model</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">get_tuned_model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">:</span> <span class="n">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModel">_LanguageModel</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Loads the specified tuned language model.</p>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">tuned_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModel&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads the specified tuned language model.&quot;&quot;&quot;</span>

    <span class="n">tuned_vertex_model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">)</span>
    <span class="n">tuned_model_labels</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span>

    <span class="k">if</span> <span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuned_model_labels</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The provided model </span><span class="si">{</span><span class="n">tuned_model_name</span><span class="si">}</span><span class="s2"> does not have a base model ID.&quot;</span>
        <span class="p">)</span>

    <span class="n">tuning_model_id</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span><span class="p">]</span>

    <span class="n">tuned_model_deployments</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">deployed_models</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tuned_model_deployments</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Deploying the model</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">()</span><span class="o">.</span><span class="n">resource_name</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model_deployments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">endpoint</span>

    <span class="n">base_model_id</span> <span class="o">=</span> <span class="n">_get_model_id_from_tuning_model_id</span><span class="p">(</span><span class="n">tuning_model_id</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="bp">cls</span><span class="p">},</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">interface_class</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatModel.tune_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">tune_model</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">tune_model</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="pandas.core.frame.DataFrame">DataFrame</span></span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Tunes a model based on training data.</p>
<p>This method launches and returns an asynchronous model tuning job.
Usage:
<div class="language-text highlight"><pre><span></span><code>tuning_job = model.tune_model(...)
... do some other work
tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete
</code></pre></div></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>training_data</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A Pandas DataFrame or a URI pointing to data in JSON lines format.
The dataset schema is model-specific.
See <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format">https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="pandas.core.frame.DataFrame">DataFrame</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>train_steps</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of training batches to tune on (batch size is 8 samples).</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>learning_rate</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Deprecated. Use learning_rate_multiplier instead.
Learning rate to use in tuning.</p>
              </div>
              <p>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>learning_rate_multiplier</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Learning rate multiplier to use in tuning.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_job_location</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>GCP location where the tuning job should be run.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuned_model_location</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>GCP location where the tuned model should be deployed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>model_display_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom display name for the tuned model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>default_context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The context to use for all training samples by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>accelerator_type</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Type of accelerator to use. Can be "TPU" or "GPU".</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_evaluation_spec</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Specification for the model evaluation during tuning.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">LanguageModelTuningJob</span></code> object that represents the tuning job.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Calling <code class="language-python highlight"><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span></code> blocks until the tuning is complete and returns a <code class="language-python highlight"><span class="n">LanguageModel</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the "tuning_job_location" value is not supported</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the "tuned_model_location" value is not supported</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the model does not support tuning</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>AttributeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If any attribute in the "tuning_evaluation_spec" is not supported</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">    This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">    Usage:</span>
<span class="sd">    ```</span>
<span class="sd">    tuning_job = model.tune_model(...)</span>
<span class="sd">    ... do some other work</span>
<span class="sd">    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">            The dataset schema is model-specific.</span>
<span class="sd">            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">        train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">        learning_rate: Deprecated. Use learning_rate_multiplier instead.</span>
<span class="sd">            Learning rate to use in tuning.</span>
<span class="sd">        learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">        tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">        tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">        model_display_name: Custom display name for the tuned model.</span>
<span class="sd">        default_context: The context to use for all training samples by default.</span>
<span class="sd">        accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">        tuning_evaluation_spec: Specification for the model evaluation during tuning.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">        ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">        RuntimeError: If the model does not support tuning</span>
<span class="sd">        AttributeError: If any attribute in the &quot;tuning_evaluation_spec&quot; is not supported</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">tuning_evaluation_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">unsupported_chat_model_tuning_eval_spec</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;evaluation_data&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_data</span><span class="p">,</span>
            <span class="s2">&quot;evaluation_interval&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_interval</span><span class="p">,</span>
            <span class="s2">&quot;enable_early_stopping&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">enable_early_stopping</span><span class="p">,</span>
            <span class="s2">&quot;enable_checkpoint_selection&quot;</span><span class="p">:</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">enable_checkpoint_selection</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">for</span> <span class="n">att_name</span><span class="p">,</span> <span class="n">att_value</span> <span class="ow">in</span> <span class="n">unsupported_chat_model_tuning_eval_spec</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">att_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                    <span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;ChatModel and CodeChatModel only support tensorboard as attribute for TuningEvaluationSpec&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;found attribute name </span><span class="si">{</span><span class="n">att_name</span><span class="si">}</span><span class="s2"> with value </span><span class="si">{</span><span class="n">att_value</span><span class="si">}</span><span class="s2">, please leave </span><span class="si">{</span><span class="n">att_name</span><span class="si">}</span><span class="s2"> to None&quot;</span>
                    <span class="p">)</span>
                <span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="o">=</span><span class="n">learning_rate_multiplier</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
        <span class="n">default_context</span><span class="o">=</span><span class="n">default_context</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="o">=</span><span class="n">tuning_evaluation_spec</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatModel.start_chat" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">start_chat</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">start_chat</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatMessage" href="#vertexai.language_models.ChatMessage">ChatMessage</a></span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.CodeChatSession" href="#vertexai.language_models.CodeChatSession">CodeChatSession</a></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Starts a chat session with the code chat model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Context shapes how the model responds throughout the conversation.
For example, you can use context to specify words the model can or
cannot use, topics to focus on or avoid, or the response format or style.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.CodeChatSession" href="#vertexai.language_models.CodeChatSession">CodeChatSession</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">ChatSession</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2726</span>
<span class="normal">2727</span>
<span class="normal">2728</span>
<span class="normal">2729</span>
<span class="normal">2730</span>
<span class="normal">2731</span>
<span class="normal">2732</span>
<span class="normal">2733</span>
<span class="normal">2734</span>
<span class="normal">2735</span>
<span class="normal">2736</span>
<span class="normal">2737</span>
<span class="normal">2738</span>
<span class="normal">2739</span>
<span class="normal">2740</span>
<span class="normal">2741</span>
<span class="normal">2742</span>
<span class="normal">2743</span>
<span class="normal">2744</span>
<span class="normal">2745</span>
<span class="normal">2746</span>
<span class="normal">2747</span>
<span class="normal">2748</span>
<span class="normal">2749</span>
<span class="normal">2750</span>
<span class="normal">2751</span>
<span class="normal">2752</span>
<span class="normal">2753</span>
<span class="normal">2754</span>
<span class="normal">2755</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">start_chat</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;CodeChatSession&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Starts a chat session with the code chat model.</span>

<span class="sd">    Args:</span>
<span class="sd">        context: Context shapes how the model responds throughout the conversation.</span>
<span class="sd">            For example, you can use context to specify words the model can or</span>
<span class="sd">            cannot use, topics to focus on or avoid, or the response format or style.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `ChatSession` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">CodeChatSession</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.CodeChatSession" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">CodeChatSession</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">CodeChatSession</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.CodeChatModel" href="#vertexai.language_models.CodeChatModel">CodeChatModel</a></span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatMessage" href="#vertexai.language_models.ChatMessage">ChatMessage</a></span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._ChatSessionBase">_ChatSessionBase</span></code></p>


      <p>CodeChatSession represents a chat session with code chat language model.</p>
<p>Within a code chat session, the model keeps context and remembers the previous converstion.</p>

                  <details class="quote">
                    <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3365</span>
<span class="normal">3366</span>
<span class="normal">3367</span>
<span class="normal">3368</span>
<span class="normal">3369</span>
<span class="normal">3370</span>
<span class="normal">3371</span>
<span class="normal">3372</span>
<span class="normal">3373</span>
<span class="normal">3374</span>
<span class="normal">3375</span>
<span class="normal">3376</span>
<span class="normal">3377</span>
<span class="normal">3378</span>
<span class="normal">3379</span>
<span class="normal">3380</span>
<span class="normal">3381</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">CodeChatModel</span><span class="p">,</span>
    <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">message_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">message_history</span><span class="o">=</span><span class="n">message_history</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.CodeChatSession.USER_AUTHOR" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">USER_AUTHOR</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">USER_AUTHOR</span> <span class="o">=</span> <span class="s1">&#39;user&#39;</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.CodeChatSession.MODEL_AUTHOR" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">MODEL_AUTHOR</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">MODEL_AUTHOR</span> <span class="o">=</span> <span class="s1">&#39;bot&#39;</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.CodeChatSession.message_history" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">message_history</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">message_history</span><span class="p">:</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.ChatMessage" href="#vertexai.language_models.ChatMessage">ChatMessage</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>List of previous messages.</p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatSession.send_message" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the code chat model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].
Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].
 Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3383</span>
<span class="normal">3384</span>
<span class="normal">3385</span>
<span class="normal">3386</span>
<span class="normal">3387</span>
<span class="normal">3388</span>
<span class="normal">3389</span>
<span class="normal">3390</span>
<span class="normal">3391</span>
<span class="normal">3392</span>
<span class="normal">3393</span>
<span class="normal">3394</span>
<span class="normal">3395</span>
<span class="normal">3396</span>
<span class="normal">3397</span>
<span class="normal">3398</span>
<span class="normal">3399</span>
<span class="normal">3400</span>
<span class="normal">3401</span>
<span class="normal">3402</span>
<span class="normal">3403</span>
<span class="normal">3404</span>
<span class="normal">3405</span>
<span class="normal">3406</span>
<span class="normal">3407</span>
<span class="normal">3408</span>
<span class="normal">3409</span>
<span class="normal">3410</span>
<span class="normal">3411</span>
<span class="normal">3412</span>
<span class="normal">3413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the code chat model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">             Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">        text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatSession.send_message_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the code chat model and gets a response.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1000].
Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1].
 Uses the value specified when calling <code class="language-python highlight"><span class="n">CodeChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3415</span>
<span class="normal">3416</span>
<span class="normal">3417</span>
<span class="normal">3418</span>
<span class="normal">3419</span>
<span class="normal">3420</span>
<span class="normal">3421</span>
<span class="normal">3422</span>
<span class="normal">3423</span>
<span class="normal">3424</span>
<span class="normal">3425</span>
<span class="normal">3426</span>
<span class="normal">3427</span>
<span class="normal">3428</span>
<span class="normal">3429</span>
<span class="normal">3430</span>
<span class="normal">3431</span>
<span class="normal">3432</span>
<span class="normal">3433</span>
<span class="normal">3434</span>
<span class="normal">3435</span>
<span class="normal">3436</span>
<span class="normal">3437</span>
<span class="normal">3438</span>
<span class="normal">3439</span>
<span class="normal">3440</span>
<span class="normal">3441</span>
<span class="normal">3442</span>
<span class="normal">3443</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">send_message_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the code chat model and gets a response.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1000].</span>
<span class="sd">            Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1].</span>
<span class="sd">             Uses the value specified when calling `CodeChatModel.start_chat` by default.</span>
<span class="sd">        candidate_count: Number of candidates to return.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the</span>
<span class="sd">        text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_async</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatSession.send_message_streaming" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.Iterator">Iterator</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3445</span>
<span class="normal">3446</span>
<span class="normal">3447</span>
<span class="normal">3448</span>
<span class="normal">3449</span>
<span class="normal">3450</span>
<span class="normal">3451</span>
<span class="normal">3452</span>
<span class="normal">3453</span>
<span class="normal">3454</span>
<span class="normal">3455</span>
<span class="normal">3456</span>
<span class="normal">3457</span>
<span class="normal">3458</span>
<span class="normal">3459</span>
<span class="normal">3460</span>
<span class="normal">3461</span>
<span class="normal">3462</span>
<span class="normal">3463</span>
<span class="normal">3464</span>
<span class="normal">3465</span>
<span class="normal">3466</span>
<span class="normal">3467</span>
<span class="normal">3468</span>
<span class="normal">3469</span>
<span class="normal">3470</span>
<span class="normal">3471</span>
<span class="normal">3472</span>
<span class="normal">3473</span>
<span class="normal">3474</span>
<span class="normal">3475</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_streaming</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.CodeChatSession.send_message_streaming_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">send_message_streaming_async</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="n">message</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.AsyncIterator">AsyncIterator</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously sends message to the language model and gets a streamed response.</p>
<p>The response is only added to the history once it's fully read.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>message</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Message to send to the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.
Uses the value specified when calling <code class="language-python highlight"><span class="n">ChatModel</span><span class="o">.</span><span class="n">start_chat</span></code> by default.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">3477</span>
<span class="normal">3478</span>
<span class="normal">3479</span>
<span class="normal">3480</span>
<span class="normal">3481</span>
<span class="normal">3482</span>
<span class="normal">3483</span>
<span class="normal">3484</span>
<span class="normal">3485</span>
<span class="normal">3486</span>
<span class="normal">3487</span>
<span class="normal">3488</span>
<span class="normal">3489</span>
<span class="normal">3490</span>
<span class="normal">3491</span>
<span class="normal">3492</span>
<span class="normal">3493</span>
<span class="normal">3494</span>
<span class="normal">3495</span>
<span class="normal">3496</span>
<span class="normal">3497</span>
<span class="normal">3498</span>
<span class="normal">3499</span>
<span class="normal">3500</span>
<span class="normal">3501</span>
<span class="normal">3502</span>
<span class="normal">3503</span>
<span class="normal">3504</span>
<span class="normal">3505</span>
<span class="normal">3506</span>
<span class="normal">3507</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">send_message_streaming_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously sends message to the language model and gets a streamed response.</span>

<span class="sd">    The response is only added to the history once it&#39;s fully read.</span>

<span class="sd">    Args:</span>
<span class="sd">        message: Message to send to the model</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">            Uses the value specified when calling `ChatModel.start_chat` by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">send_message_streaming_async</span><span class="p">(</span>
        <span class="n">message</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.CodeGenerationModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">CodeGenerationModel</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">CodeGenerationModel</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._CodeGenerationModel">_CodeGenerationModel</span></code>, <code><span title="vertexai.language_models._language_models._TunableTextModelMixin">_TunableTextModelMixin</span></code>, <code><span title="vertexai.language_models._language_models._ModelWithBatchPredict">_ModelWithBatchPredict</span></code></p>


      <p>This constructor should not be called directly.
Use <code class="language-python highlight"><span class="n">LanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="o">=...</span><span class="p">)</span></code> instead.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Identifier of a Vertex LLM. Example: "text-bison@001"</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>endpoint_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vertex Endpoint resource name for the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

                  <details class="quote">
                    <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a LanguageModel.</span>

<span class="sd">    This constructor should not be called directly.</span>
<span class="sd">    Use `LanguageModel.from_pretrained(model_name=...)` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_id: Identifier of a Vertex LLM. Example: &quot;text-bison@001&quot;</span>
<span class="sd">        endpoint_name: Vertex Endpoint resource name for the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.InputOutputTextPair" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">InputOutputTextPair</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">InputOutputTextPair</span><span class="p">(</span>
    <span class="vm">__module__</span><span class="o">=</span><span class="s2">&quot;vertexai.language_models&quot;</span><span class="p">,</span>
    <span class="n">input_text</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="n">output_text</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">


      <p>InputOutputTextPair represents a pair of input and output texts.</p>




  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.InputOutputTextPair.input_text" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">input_text</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">input_text</span><span class="p">:</span> <span class="n">str</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.InputOutputTextPair.output_text" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">output_text</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">output_text</span><span class="p">:</span> <span class="n">str</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.TextEmbedding" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextEmbedding</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">TextEmbedding</span><span class="p">(</span>
    <span class="vm">__module__</span><span class="o">=</span><span class="s2">&quot;vertexai.language_models&quot;</span><span class="p">,</span>
    <span class="n">values</span><span class="p">:</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">float</span><span class="p">],</span>
    <span class="n">statistics</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="vertexai.language_models._language_models.TextEmbeddingStatistics">TextEmbeddingStatistics</span></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="google.cloud.aiplatform.models.Prediction">Prediction</span></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">


      <p>Text embedding vector and statistics.</p>




  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextEmbedding.values" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">values</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">values</span><span class="p">:</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextEmbedding.statistics" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">statistics</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">statistics</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="vertexai.language_models._language_models.TextEmbeddingStatistics">TextEmbeddingStatistics</span></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.TextEmbeddingInput" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextEmbeddingInput</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">TextEmbeddingInput</span><span class="p">(</span>
    <span class="vm">__module__</span><span class="o">=</span><span class="s2">&quot;vertexai.language_models&quot;</span><span class="p">,</span>
    <span class="n">text</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="n">task_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">title</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">


      <p>Structural text embedding input.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">ATTRIBUTE</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="vertexai.language_models.TextEmbeddingInput.text" href="#vertexai.language_models.TextEmbeddingInput.text">text</a></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The main text content to embed.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="vertexai.language_models.TextEmbeddingInput.task_type" href="#vertexai.language_models.TextEmbeddingInput.task_type">task_type</a></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>The name of the downstream task the embeddings will be used for.
Valid values:
RETRIEVAL_QUERY
    Specifies the given text is a query in a search/retrieval setting.
RETRIEVAL_DOCUMENT
    Specifies the given text is a document from the corpus being searched.
SEMANTIC_SIMILARITY
    Specifies the given text will be used for STS.
CLASSIFICATION
    Specifies that the given text will be classified.
CLUSTERING
    Specifies that the embeddings will be used for clustering.
QUESTION_ANSWERING
    Specifies that the embeddings will be used for question answering.
FACT_VERIFICATION
    Specifies that the embeddings will be used for fact verification.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="vertexai.language_models.TextEmbeddingInput.title" href="#vertexai.language_models.TextEmbeddingInput.title">title</a></code></td>
            <td class="doc-attribute-details">
              <div class="doc-md-description">
                <p>Optional identifier of the text content.</p>
              </div>
              <p>
                  <span class="doc-attribute-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>




  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextEmbeddingInput.text" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">text</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">text</span><span class="p">:</span> <span class="n">str</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextEmbeddingInput.task_type" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">task_type</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">task_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextEmbeddingInput.title" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">title</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">title</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.TextEmbeddingModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextEmbeddingModel</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">TextEmbeddingModel</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._TextEmbeddingModel">_TextEmbeddingModel</span></code>, <code><span title="vertexai.language_models._language_models._TunableTextEmbeddingModelMixin">_TunableTextEmbeddingModelMixin</span></code></p>


      <p>This constructor should not be called directly.
Use <code class="language-python highlight"><span class="n">LanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="o">=...</span><span class="p">)</span></code> instead.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Identifier of a Vertex LLM. Example: "text-bison@001"</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>endpoint_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vertex Endpoint resource name for the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

                  <details class="quote">
                    <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a LanguageModel.</span>

<span class="sd">    This constructor should not be called directly.</span>
<span class="sd">    Use `LanguageModel.from_pretrained(model_name=...)` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_id: Identifier of a Vertex LLM. Example: &quot;text-bison@001&quot;</span>
<span class="sd">        endpoint_name: Vertex Endpoint resource name for the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextEmbeddingModel.list_tuned_model_names" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">list_tuned_model_names</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">list_tuned_model_names</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.Sequence">Sequence</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Lists the names of tuned models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Sequence">Sequence</span>[str]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of tuned models that can be used with the <code class="language-python highlight"><span class="n">get_tuned_model</span></code> method.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">list_tuned_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lists the names of tuned models.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of tuned models that can be used with the `get_tuned_model` method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)},</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_list_tuned_model_names</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">tuning_model_id</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextEmbeddingModel.get_tuned_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_tuned_model</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">get_tuned_model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">del</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>  <span class="c1"># Unused.</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
        <span class="s2">&quot;Use deploy_tuned_model instead to get the tuned model.&quot;</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextEmbeddingModel.tune_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">tune_model</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">tune_model</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">corpus_data</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">queries_data</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">test_data</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">task_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">machine_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._TextEmbeddingModelTuningJob">_TextEmbeddingModelTuningJob</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Tunes a model based on training data.</p>
<p>This method launches and returns an asynchronous model tuning job.
Usage:
```
tuning_job = model.tune_model(...)
... do some other work
tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</p>
<p>Args:
    training_data: URI pointing to training data in TSV format.
    corpus_data: URI pointing to data in JSON lines format.
    queries_data: URI pointing to data in JSON lines format.
    test_data: URI pointing to data in TSV format.
    validation_data: URI pointing to data in TSV format.
    batch_size: The training batch size.
    train_steps: The number of steps to perform model tuning. Must
        be greater than 30.
    tuned_model_location: GCP location where the tuned model should be deployed.
    model_display_name: Custom display name for the tuned model.
    task_type: The task type expected to be used during inference.
        Valid values are <code class="language-python highlight"><span class="n">DEFAULT</span></code>, <code class="language-python highlight"><span class="n">RETRIEVAL_QUERY</span></code>, <code class="language-python highlight"><span class="n">RETRIEVAL_DOCUMENT</span></code>,
        <code class="language-python highlight"><span class="n">SEMANTIC_SIMILARITY</span></code>, <code class="language-python highlight"><span class="n">CLASSIFICATION</span></code>, <code class="language-python highlight"><span class="n">CLUSTERING</span></code>,
        <code class="language-python highlight"><span class="n">FACT_VERIFICATION</span></code>, and <code class="language-python highlight"><span class="n">QUESTION_ANSWERING</span></code>.
    machine_type: The machine type to use for training. For information
        about selecting the machine type that matches the accelerator
        type and count you have selected, see
        <a href="https://cloud.google.com/compute/docs/gpus">https://cloud.google.com/compute/docs/gpus</a>.
    accelerator: The accelerator type to use for tuning, for example
        <code class="language-python highlight"><span class="n">NVIDIA_TESLA_V100</span></code>. For possible values, see
        <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#using-accelerators">https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#using-accelerators</a>.
    accelerator_count: The number of accelerators to use when training.
        Using a greater number of accelerators may make training faster,
        but has no effect on quality.
Returns:
    A <code class="language-python highlight"><span class="n">LanguageModelTuningJob</span></code> object that represents the tuning job.
    Calling <code class="language-python highlight"><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span></code> blocks until the tuning is complete and
    returns a <code class="language-python highlight"><span class="n">LanguageModel</span></code> object.</p>
<p>Raises:
    ValueError: If the provided parameter combinations or values are not
        supported.
    RuntimeError: If the model does not support tuning</p>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2417</span>
<span class="normal">2418</span>
<span class="normal">2419</span>
<span class="normal">2420</span>
<span class="normal">2421</span>
<span class="normal">2422</span>
<span class="normal">2423</span>
<span class="normal">2424</span>
<span class="normal">2425</span>
<span class="normal">2426</span>
<span class="normal">2427</span>
<span class="normal">2428</span>
<span class="normal">2429</span>
<span class="normal">2430</span>
<span class="normal">2431</span>
<span class="normal">2432</span>
<span class="normal">2433</span>
<span class="normal">2434</span>
<span class="normal">2435</span>
<span class="normal">2436</span>
<span class="normal">2437</span>
<span class="normal">2438</span>
<span class="normal">2439</span>
<span class="normal">2440</span>
<span class="normal">2441</span>
<span class="normal">2442</span>
<span class="normal">2443</span>
<span class="normal">2444</span>
<span class="normal">2445</span>
<span class="normal">2446</span>
<span class="normal">2447</span>
<span class="normal">2448</span>
<span class="normal">2449</span>
<span class="normal">2450</span>
<span class="normal">2451</span>
<span class="normal">2452</span>
<span class="normal">2453</span>
<span class="normal">2454</span>
<span class="normal">2455</span>
<span class="normal">2456</span>
<span class="normal">2457</span>
<span class="normal">2458</span>
<span class="normal">2459</span>
<span class="normal">2460</span>
<span class="normal">2461</span>
<span class="normal">2462</span>
<span class="normal">2463</span>
<span class="normal">2464</span>
<span class="normal">2465</span>
<span class="normal">2466</span>
<span class="normal">2467</span>
<span class="normal">2468</span>
<span class="normal">2469</span>
<span class="normal">2470</span>
<span class="normal">2471</span>
<span class="normal">2472</span>
<span class="normal">2473</span>
<span class="normal">2474</span>
<span class="normal">2475</span>
<span class="normal">2476</span>
<span class="normal">2477</span>
<span class="normal">2478</span>
<span class="normal">2479</span>
<span class="normal">2480</span>
<span class="normal">2481</span>
<span class="normal">2482</span>
<span class="normal">2483</span>
<span class="normal">2484</span>
<span class="normal">2485</span>
<span class="normal">2486</span>
<span class="normal">2487</span>
<span class="normal">2488</span>
<span class="normal">2489</span>
<span class="normal">2490</span>
<span class="normal">2491</span>
<span class="normal">2492</span>
<span class="normal">2493</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">corpus_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">queries_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">test_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">task_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">machine_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_TextEmbeddingModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">    This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">    Usage:</span>
<span class="sd">    ```</span>
<span class="sd">    tuning_job = model.tune_model(...)</span>
<span class="sd">    ... do some other work</span>
<span class="sd">    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>

<span class="sd">    Args:</span>
<span class="sd">        training_data: URI pointing to training data in TSV format.</span>
<span class="sd">        corpus_data: URI pointing to data in JSON lines format.</span>
<span class="sd">        queries_data: URI pointing to data in JSON lines format.</span>
<span class="sd">        test_data: URI pointing to data in TSV format.</span>
<span class="sd">        validation_data: URI pointing to data in TSV format.</span>
<span class="sd">        batch_size: The training batch size.</span>
<span class="sd">        train_steps: The number of steps to perform model tuning. Must</span>
<span class="sd">            be greater than 30.</span>
<span class="sd">        tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">        model_display_name: Custom display name for the tuned model.</span>
<span class="sd">        task_type: The task type expected to be used during inference.</span>
<span class="sd">            Valid values are `DEFAULT`, `RETRIEVAL_QUERY`, `RETRIEVAL_DOCUMENT`,</span>
<span class="sd">            `SEMANTIC_SIMILARITY`, `CLASSIFICATION`, `CLUSTERING`,</span>
<span class="sd">            `FACT_VERIFICATION`, and `QUESTION_ANSWERING`.</span>
<span class="sd">        machine_type: The machine type to use for training. For information</span>
<span class="sd">            about selecting the machine type that matches the accelerator</span>
<span class="sd">            type and count you have selected, see</span>
<span class="sd">            https://cloud.google.com/compute/docs/gpus.</span>
<span class="sd">        accelerator: The accelerator type to use for tuning, for example</span>
<span class="sd">            `NVIDIA_TESLA_V100`. For possible values, see</span>
<span class="sd">            https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#using-accelerators.</span>
<span class="sd">        accelerator_count: The number of accelerators to use when training.</span>
<span class="sd">            Using a greater number of accelerators may make training faster,</span>
<span class="sd">            but has no effect on quality.</span>
<span class="sd">    Returns:</span>
<span class="sd">        A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">        Calling `job.result()` blocks until the tuning is complete and</span>
<span class="sd">        returns a `LanguageModel` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the provided parameter combinations or values are not</span>
<span class="sd">            supported.</span>
<span class="sd">        RuntimeError: If the model does not support tuning</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
        <span class="n">corpus_data</span><span class="o">=</span><span class="n">corpus_data</span><span class="p">,</span>
        <span class="n">queries_data</span><span class="o">=</span><span class="n">queries_data</span><span class="p">,</span>
        <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_data</span><span class="p">,</span>
        <span class="n">task_type</span><span class="o">=</span><span class="n">task_type</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
        <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
        <span class="n">accelerator</span><span class="o">=</span><span class="n">accelerator</span><span class="p">,</span>
        <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextEmbeddingModel.deploy_tuned_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">deploy_tuned_model</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">deploy_tuned_model</span><span class="p">(</span>
    <span class="n">tuned_model_name</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="n">machine_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModel">_LanguageModel</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Loads the specified tuned language model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>machine_type</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Machine type. E.g., "a2-highgpu-1g". See also: <a href="https://cloud.google.com/vertex-ai/docs/training/configure-compute">https://cloud.google.com/vertex-ai/docs/training/configure-compute</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>accelerator</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Kind of accelerator. E.g., "NVIDIA_TESLA_A100". See also: <a href="https://cloud.google.com/vertex-ai/docs/training/configure-compute">https://cloud.google.com/vertex-ai/docs/training/configure-compute</a>.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>accelerator_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Count of accelerators.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModel">_LanguageModel</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Tuned <code class="language-python highlight"><span class="n">LanguageModel</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span>
<span class="normal">2375</span>
<span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span>
<span class="normal">2400</span>
<span class="normal">2401</span>
<span class="normal">2402</span>
<span class="normal">2403</span>
<span class="normal">2404</span>
<span class="normal">2405</span>
<span class="normal">2406</span>
<span class="normal">2407</span>
<span class="normal">2408</span>
<span class="normal">2409</span>
<span class="normal">2410</span>
<span class="normal">2411</span>
<span class="normal">2412</span>
<span class="normal">2413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">deploy_tuned_model</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">tuned_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">machine_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModel&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads the specified tuned language model.</span>

<span class="sd">    Args:</span>
<span class="sd">        machine_type: Machine type. E.g., &quot;a2-highgpu-1g&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">        accelerator: Kind of accelerator. E.g., &quot;NVIDIA_TESLA_A100&quot;. See also: https://cloud.google.com/vertex-ai/docs/training/configure-compute.</span>
<span class="sd">        accelerator_count: Count of accelerators.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuned `LanguageModel` object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tuned_vertex_model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">)</span>
    <span class="n">tuned_model_labels</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span>

    <span class="k">if</span> <span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuned_model_labels</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The provided model </span><span class="si">{</span><span class="n">tuned_model_name</span><span class="si">}</span><span class="s2"> does not have a base model ID.&quot;</span>
        <span class="p">)</span>

    <span class="n">tuning_model_id</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span><span class="p">]</span>
    <span class="n">tuned_model_deployments</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">deployed_models</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tuned_model_deployments</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Deploying a model to an endpoint requires a resource quota.</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
            <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
            <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator</span><span class="p">,</span>
            <span class="n">accelerator_count</span><span class="o">=</span><span class="n">accelerator_count</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">resource_name</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model_deployments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">endpoint</span>

    <span class="n">base_model_id</span> <span class="o">=</span> <span class="n">_get_model_id_from_tuning_model_id</span><span class="p">(</span><span class="n">tuning_model_id</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="bp">cls</span><span class="p">},</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">interface_class</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextEmbeddingModel.get_embeddings" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_embeddings</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">get_embeddings</span><span class="p">(</span>
    <span class="n">texts</span><span class="p">:</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbeddingInput" href="#vertexai.language_models.TextEmbeddingInput">TextEmbeddingInput</a></span><span class="p">]],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">auto_truncate</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">output_dimensionality</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbedding" href="#vertexai.language_models.TextEmbedding">TextEmbedding</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Calculates embeddings for the given texts.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>texts</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of texts or <code class="language-python highlight"><span class="n">TextEmbeddingInput</span></code> objects to embed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="typing.Union">Union</span>[str, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbeddingInput" href="#vertexai.language_models.TextEmbeddingInput">TextEmbeddingInput</a>]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>auto_truncate</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to automatically truncate long texts. Default: True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>output_dimensionality</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional dimensions of embeddings. Range: [1, 768]. Default: None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbedding" href="#vertexai.language_models.TextEmbedding">TextEmbedding</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of <code class="language-python highlight"><span class="n">TextEmbedding</span></code> objects.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span>
<span class="normal">2190</span>
<span class="normal">2191</span>
<span class="normal">2192</span>
<span class="normal">2193</span>
<span class="normal">2194</span>
<span class="normal">2195</span>
<span class="normal">2196</span>
<span class="normal">2197</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TextEmbeddingInput</span><span class="p">]],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">auto_truncate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">output_dimensionality</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;TextEmbedding&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculates embeddings for the given texts.</span>

<span class="sd">    Args:</span>
<span class="sd">        texts: A list of texts or `TextEmbeddingInput` objects to embed.</span>
<span class="sd">        auto_truncate: Whether to automatically truncate long texts. Default: True.</span>
<span class="sd">        output_dimensionality: Optional dimensions of embeddings. Range: [1, 768]. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of `TextEmbedding` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_text_embedding_request</span><span class="p">(</span>
        <span class="n">texts</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span>
        <span class="n">auto_truncate</span><span class="o">=</span><span class="n">auto_truncate</span><span class="p">,</span>
        <span class="n">output_dimensionality</span><span class="o">=</span><span class="n">output_dimensionality</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
        <span class="n">instances</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instances</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="n">TextEmbedding</span><span class="o">.</span><span class="n">_parse_text_embedding_response</span><span class="p">(</span>
            <span class="n">prediction_response</span><span class="p">,</span> <span class="n">i_prediction</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i_prediction</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">)</span>
    <span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextEmbeddingModel.get_embeddings_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_embeddings_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">get_embeddings_async</span><span class="p">(</span>
    <span class="n">texts</span><span class="p">:</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbeddingInput" href="#vertexai.language_models.TextEmbeddingInput">TextEmbeddingInput</a></span><span class="p">]],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">auto_truncate</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">output_dimensionality</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbedding" href="#vertexai.language_models.TextEmbedding">TextEmbedding</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously calculates embeddings for the given texts.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>texts</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A list of texts or <code class="language-python highlight"><span class="n">TextEmbeddingInput</span></code> objects to embed.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.List">List</span>[<span title="typing.Union">Union</span>[str, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbeddingInput" href="#vertexai.language_models.TextEmbeddingInput">TextEmbeddingInput</a>]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>auto_truncate</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Whether to automatically truncate long texts. Default: True.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>bool</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>True</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>output_dimensionality</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Optional dimensions of embeddings. Range: [1, 768]. Default: None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.List">List</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextEmbedding" href="#vertexai.language_models.TextEmbedding">TextEmbedding</a>]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of <code class="language-python highlight"><span class="n">TextEmbedding</span></code> objects.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2199</span>
<span class="normal">2200</span>
<span class="normal">2201</span>
<span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">get_embeddings_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TextEmbeddingInput</span><span class="p">]],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">auto_truncate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">output_dimensionality</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="s2">&quot;TextEmbedding&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously calculates embeddings for the given texts.</span>

<span class="sd">    Args:</span>
<span class="sd">        texts: A list of texts or `TextEmbeddingInput` objects to embed.</span>
<span class="sd">        auto_truncate: Whether to automatically truncate long texts. Default: True.</span>
<span class="sd">        output_dimensionality: Optional dimensions of embeddings. Range: [1, 768]. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of `TextEmbedding` objects.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prepare_text_embedding_request</span><span class="p">(</span>
        <span class="n">texts</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span>
        <span class="n">auto_truncate</span><span class="o">=</span><span class="n">auto_truncate</span><span class="p">,</span>
        <span class="n">output_dimensionality</span><span class="o">=</span><span class="n">output_dimensionality</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict_async</span><span class="p">(</span>
        <span class="n">instances</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instances</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span>
        <span class="n">TextEmbedding</span><span class="o">.</span><span class="n">_parse_text_embedding_response</span><span class="p">(</span>
            <span class="n">prediction_response</span><span class="p">,</span> <span class="n">i_prediction</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i_prediction</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prediction_response</span><span class="o">.</span><span class="n">predictions</span><span class="p">)</span>
    <span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.TextGenerationModel" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextGenerationModel</span>


</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">TextGenerationModel</span><span class="p">(</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="vertexai.language_models._language_models._TextGenerationModel">_TextGenerationModel</span></code>, <code><span title="vertexai.language_models._language_models._TunableTextModelMixin">_TunableTextModelMixin</span></code>, <code><span title="vertexai.language_models._language_models._ModelWithBatchPredict">_ModelWithBatchPredict</span></code>, <code><span title="vertexai.language_models._language_models._RlhfTunableModelMixin">_RlhfTunableModelMixin</span></code></p>


      <p>This constructor should not be called directly.
Use <code class="language-python highlight"><span class="n">LanguageModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="o">=...</span><span class="p">)</span></code> instead.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>model_id</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Identifier of a Vertex LLM. Example: "text-bison@001"</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>endpoint_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Vertex Endpoint resource name for the model</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>

                  <details class="quote">
                    <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
                    <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">endpoint_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Creates a LanguageModel.</span>

<span class="sd">    This constructor should not be called directly.</span>
<span class="sd">    Use `LanguageModel.from_pretrained(model_name=...)` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_id: Identifier of a Vertex LLM. Example: &quot;text-bison@001&quot;</span>
<span class="sd">        endpoint_name: Vertex Endpoint resource name for the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.list_tuned_model_names" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">list_tuned_model_names</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">list_tuned_model_names</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.Sequence">Sequence</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Lists the names of tuned models.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="typing.Sequence">Sequence</span>[str]</code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A list of tuned models that can be used with the <code class="language-python highlight"><span class="n">get_tuned_model</span></code> method.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">list_tuned_model_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Lists the names of tuned models.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of tuned models that can be used with the `get_tuned_model` method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)},</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">_list_tuned_model_names</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="n">model_info</span><span class="o">.</span><span class="n">tuning_model_id</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.get_tuned_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_tuned_model</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">get_tuned_model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">:</span> <span class="n">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModel">_LanguageModel</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Loads the specified tuned language model.</p>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">get_tuned_model</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">tuned_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModel&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loads the specified tuned language model.&quot;&quot;&quot;</span>

    <span class="n">tuned_vertex_model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">tuned_model_name</span><span class="p">)</span>
    <span class="n">tuned_model_labels</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span>

    <span class="k">if</span> <span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuned_model_labels</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The provided model </span><span class="si">{</span><span class="n">tuned_model_name</span><span class="si">}</span><span class="s2"> does not have a base model ID.&quot;</span>
        <span class="p">)</span>

    <span class="n">tuning_model_id</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">_TUNING_BASE_MODEL_ID_LABEL_KEY</span><span class="p">]</span>

    <span class="n">tuned_model_deployments</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">gca_resource</span><span class="o">.</span><span class="n">deployed_models</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tuned_model_deployments</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Deploying the model</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_vertex_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">()</span><span class="o">.</span><span class="n">resource_name</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">endpoint_name</span> <span class="o">=</span> <span class="n">tuned_model_deployments</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">endpoint</span>

    <span class="n">base_model_id</span> <span class="o">=</span> <span class="n">_get_model_id_from_tuning_model_id</span><span class="p">(</span><span class="n">tuning_model_id</span><span class="p">)</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">_model_garden_models</span><span class="o">.</span><span class="n">_get_model_info</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">schema_to_class_map</span><span class="o">=</span><span class="p">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">_INSTANCE_SCHEMA_URI</span><span class="p">:</span> <span class="bp">cls</span><span class="p">},</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model_info</span><span class="o">.</span><span class="n">interface_class</span><span class="p">(</span>
        <span class="n">model_id</span><span class="o">=</span><span class="n">base_model_id</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.tune_model_rlhf" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">tune_model_rlhf</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">tune_model_rlhf</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">prompt_data</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="pandas.core.frame.DataFrame">DataFrame</span></span><span class="p">],</span>
    <span class="n">preference_data</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="pandas.core.frame.DataFrame">DataFrame</span></span><span class="p">],</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_sequence_length</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target_sequence_length</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_learning_rate_multiplier</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">float</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_rate_multiplier</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">float</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">int</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kl_coeff</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Tunes a model using reinforcement learning from human feedback.</p>
<p>This method launches and returns an asynchronous model tuning job.
Usage:
<div class="language-text highlight"><pre><span></span><code>tuning_job = model.tune_model_rlhf(...)
... do some other work
tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete
</code></pre></div></p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt_data</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A Pandas DataFrame or a URI pointing to data in JSON lines
format. The dataset schema is model-specific.
See <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset">https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="pandas.core.frame.DataFrame">DataFrame</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>preference_data</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>A Pandas DataFrame or a URI pointing to data in JSON lines
format. The dataset schema is model-specific.
See <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset">https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset</a></p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="pandas.core.frame.DataFrame">DataFrame</span>]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>model_display_name</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Custom display name for the tuned model.
If not provided, a default name will be created.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>prompt_sequence_length</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Maximum tokenized sequence length for input text.
Higher values increase memory overhead.
This value should be at most 8192. Default value is 512.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>target_sequence_length</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Maximum tokenized sequence length for target text.
Higher values increase memory overhead.
This value should be at most 1024. Default value is 64.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reward_model_learning_rate_multiplier</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Constant used to adjust the base
learning rate used when training a reward model. Multiply by a
number &gt; 1 to increase the magnitude of updates applied at each
training step or multiply by a number &lt; 1 to decrease the magnitude
of updates. Default value is 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reinforcement_learning_rate_multiplier</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Constant used to adjust the base
learning rate used during reinforcement learning. Multiply by a
number &gt; 1 to increase the magnitude of updates applied at each
training step or multiply by a number &lt; 1 to decrease the magnitude
of updates. Default value is 1.0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reward_model_train_steps</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of steps to use when training a reward
model. Default value is 1000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>reinforcement_learning_train_steps</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of reinforcement learning steps
to perform when tuning a base model. Default value is 1000.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>kl_coeff</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Coefficient for KL penalty. This regularizes the policy model and
penalizes if it diverges from its initial distribution. If set to 0,
the reference language model is not loaded into memory. Default value
is 0.1.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>default_context</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>This field lets the model know what task to perform.
Base models have been trained over a large set of varied instructions.
You can give a simple and intuitive description of the task and the
model will follow it, e.g. "Classify this movie review as positive or
negative" or "Translate this sentence to Danish". Do not specify this
if your dataset already prepends the instruction to the inputs field.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_job_location</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>GCP location where the tuning job should be run.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[str]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>accelerator_type</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Type of accelerator to use. Can be "TPU" or "GPU".</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>tuning_evaluation_spec</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Evaluation settings to use during tuning.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">LanguageModelTuningJob</span></code> object that represents the tuning job.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>Calling <code class="language-python highlight"><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span></code> blocks until the tuning is complete and returns a <code class="language-python highlight"><span class="n">LanguageModel</span></code> object.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RAISES</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>ValueError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the "tuning_job_location" value is not supported</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
              <span class="doc-raises-annotation">
                  <code>RuntimeError</code>
              </span>
            </td>
            <td class="doc-raises-details">
              <div class="doc-md-description">
                <p>If the model does not support tuning</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tune_model_rlhf</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">prompt_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="n">preference_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prompt_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target_sequence_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reward_model_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reinforcement_learning_train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kl_coeff</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">default_context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tunes a model using reinforcement learning from human feedback.</span>

<span class="sd">    This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">    Usage:</span>
<span class="sd">    ```</span>
<span class="sd">    tuning_job = model.tune_model_rlhf(...)</span>
<span class="sd">    ... do some other work</span>
<span class="sd">    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>
<span class="sd">    ```</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt_data: A Pandas DataFrame or a URI pointing to data in JSON lines</span>
<span class="sd">            format. The dataset schema is model-specific.</span>
<span class="sd">            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#prompt-dataset</span>
<span class="sd">        preference_data: A Pandas DataFrame or a URI pointing to data in JSON lines</span>
<span class="sd">            format. The dataset schema is model-specific.</span>
<span class="sd">            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf#human-preference-dataset</span>
<span class="sd">        model_display_name: Custom display name for the tuned model.</span>
<span class="sd">            If not provided, a default name will be created.</span>
<span class="sd">        prompt_sequence_length: Maximum tokenized sequence length for input text.</span>
<span class="sd">            Higher values increase memory overhead.</span>
<span class="sd">            This value should be at most 8192. Default value is 512.</span>
<span class="sd">        target_sequence_length: Maximum tokenized sequence length for target text.</span>
<span class="sd">            Higher values increase memory overhead.</span>
<span class="sd">            This value should be at most 1024. Default value is 64.</span>
<span class="sd">        reward_model_learning_rate_multiplier: Constant used to adjust the base</span>
<span class="sd">            learning rate used when training a reward model. Multiply by a</span>
<span class="sd">            number &gt; 1 to increase the magnitude of updates applied at each</span>
<span class="sd">            training step or multiply by a number &lt; 1 to decrease the magnitude</span>
<span class="sd">            of updates. Default value is 1.0.</span>
<span class="sd">        reinforcement_learning_rate_multiplier: Constant used to adjust the base</span>
<span class="sd">            learning rate used during reinforcement learning. Multiply by a</span>
<span class="sd">            number &gt; 1 to increase the magnitude of updates applied at each</span>
<span class="sd">            training step or multiply by a number &lt; 1 to decrease the magnitude</span>
<span class="sd">            of updates. Default value is 1.0.</span>
<span class="sd">        reward_model_train_steps: Number of steps to use when training a reward</span>
<span class="sd">            model. Default value is 1000.</span>
<span class="sd">        reinforcement_learning_train_steps: Number of reinforcement learning steps</span>
<span class="sd">            to perform when tuning a base model. Default value is 1000.</span>
<span class="sd">        kl_coeff: Coefficient for KL penalty. This regularizes the policy model and</span>
<span class="sd">            penalizes if it diverges from its initial distribution. If set to 0,</span>
<span class="sd">            the reference language model is not loaded into memory. Default value</span>
<span class="sd">            is 0.1.</span>
<span class="sd">        default_context: This field lets the model know what task to perform.</span>
<span class="sd">            Base models have been trained over a large set of varied instructions.</span>
<span class="sd">            You can give a simple and intuitive description of the task and the</span>
<span class="sd">            model will follow it, e.g. &quot;Classify this movie review as positive or</span>
<span class="sd">            negative&quot; or &quot;Translate this sentence to Danish&quot;. Do not specify this</span>
<span class="sd">            if your dataset already prepends the instruction to the inputs field.</span>
<span class="sd">        tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">        accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">        tuning_evaluation_spec: Evaluation settings to use during tuning.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">        RuntimeError: If the model does not support tuning</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tuning_job_location</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">tuning_job_location</span> <span class="ow">or</span> <span class="n">aiplatform_initializer</span><span class="o">.</span><span class="n">global_config</span><span class="o">.</span><span class="n">location</span>
    <span class="p">)</span>
    <span class="n">eval_dataset</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">tensorboard_resource_id</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">tuning_evaluation_spec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_unused_rlhf_eval_specs</span><span class="p">(</span><span class="n">tuning_evaluation_spec</span><span class="p">)</span>

        <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">tuning_evaluation_spec</span><span class="o">.</span><span class="n">evaluation_data</span>
        <span class="k">if</span> <span class="n">eval_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">eval_dataset</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;evaluation_data must be a GCS URI that starts with gs://&quot;</span>
            <span class="p">)</span>

        <span class="n">tensorboard_resource_id</span> <span class="o">=</span> <span class="n">_get_tensorboard_resource_id_from_evaluation_spec</span><span class="p">(</span>
            <span class="n">tuning_evaluation_spec</span><span class="p">,</span> <span class="n">tuning_job_location</span>
        <span class="p">)</span>
    <span class="n">prompt_dataset_uri</span> <span class="o">=</span> <span class="n">_maybe_upload_training_data</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">prompt_data</span><span class="p">,</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">preference_dataset_uri</span> <span class="o">=</span> <span class="n">_maybe_upload_training_data</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">preference_data</span><span class="p">,</span>
        <span class="n">model_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">accelerator_type</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">accelerator_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_ACCELERATOR_TYPES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unsupported accelerator type: </span><span class="si">{</span><span class="n">accelerator_type</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; Supported types: </span><span class="si">{</span><span class="n">_ACCELERATOR_TYPES</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="n">tuning_parameters</span> <span class="o">=</span> <span class="n">_RlhfTuningParameters</span><span class="p">(</span>
        <span class="n">prompt_dataset</span><span class="o">=</span><span class="n">prompt_dataset_uri</span><span class="p">,</span>
        <span class="n">preference_dataset</span><span class="o">=</span><span class="n">preference_dataset_uri</span><span class="p">,</span>
        <span class="n">large_model_reference</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_id</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
        <span class="n">prompt_sequence_length</span><span class="o">=</span><span class="n">prompt_sequence_length</span><span class="p">,</span>
        <span class="n">target_sequence_length</span><span class="o">=</span><span class="n">target_sequence_length</span><span class="p">,</span>
        <span class="n">reward_model_learning_rate_multiplier</span><span class="o">=</span><span class="n">reward_model_learning_rate_multiplier</span><span class="p">,</span>
        <span class="n">reinforcement_learning_rate_multiplier</span><span class="o">=</span><span class="n">reinforcement_learning_rate_multiplier</span><span class="p">,</span>
        <span class="n">reward_model_train_steps</span><span class="o">=</span><span class="n">reward_model_train_steps</span><span class="p">,</span>
        <span class="n">reinforcement_learning_train_steps</span><span class="o">=</span><span class="n">reinforcement_learning_train_steps</span><span class="p">,</span>
        <span class="n">kl_coeff</span><span class="o">=</span><span class="n">kl_coeff</span><span class="p">,</span>
        <span class="n">instruction</span><span class="o">=</span><span class="n">default_context</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
        <span class="n">tensorboard_resource_id</span><span class="o">=</span><span class="n">tensorboard_resource_id</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tune_model_rlhf</span><span class="p">(</span>
        <span class="n">tuning_parameters</span><span class="o">=</span><span class="n">tuning_parameters</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.batch_predict" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">batch_predict</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">batch_predict</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]],</span>
    <span class="n">destination_uri_prefix</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="n">model_parameters</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.Dict">Dict</span></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="google.cloud.aiplatform.BatchPredictionJob">BatchPredictionJob</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Starts a batch prediction job with the model.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>dataset</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The location of the dataset.
<code class="language-python highlight"><span class="n">gs</span><span class="p">:</span><span class="o">//</span></code> and <code class="language-python highlight"><span class="n">bq</span><span class="p">:</span><span class="o">//</span></code> URIs are supported.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Union">Union</span>[str, <span title="typing.List">List</span>[str]]</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>destination_uri_prefix</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The URI prefix for the prediction.
<code class="language-python highlight"><span class="n">gs</span><span class="p">:</span><span class="o">//</span></code> and <code class="language-python highlight"><span class="n">bq</span><span class="p">:</span><span class="o">//</span></code> URIs are supported.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>model_parameters</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Model-specific parameters to send to the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="google.cloud.aiplatform.BatchPredictionJob">BatchPredictionJob</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">BatchPredictionJob</span></code> object</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
      <p>Raises:
    ValueError: When source or destination URI is not supported.</p>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1856</span>
<span class="normal">1857</span>
<span class="normal">1858</span>
<span class="normal">1859</span>
<span class="normal">1860</span>
<span class="normal">1861</span>
<span class="normal">1862</span>
<span class="normal">1863</span>
<span class="normal">1864</span>
<span class="normal">1865</span>
<span class="normal">1866</span>
<span class="normal">1867</span>
<span class="normal">1868</span>
<span class="normal">1869</span>
<span class="normal">1870</span>
<span class="normal">1871</span>
<span class="normal">1872</span>
<span class="normal">1873</span>
<span class="normal">1874</span>
<span class="normal">1875</span>
<span class="normal">1876</span>
<span class="normal">1877</span>
<span class="normal">1878</span>
<span class="normal">1879</span>
<span class="normal">1880</span>
<span class="normal">1881</span>
<span class="normal">1882</span>
<span class="normal">1883</span>
<span class="normal">1884</span>
<span class="normal">1885</span>
<span class="normal">1886</span>
<span class="normal">1887</span>
<span class="normal">1888</span>
<span class="normal">1889</span>
<span class="normal">1890</span>
<span class="normal">1891</span>
<span class="normal">1892</span>
<span class="normal">1893</span>
<span class="normal">1894</span>
<span class="normal">1895</span>
<span class="normal">1896</span>
<span class="normal">1897</span>
<span class="normal">1898</span>
<span class="normal">1899</span>
<span class="normal">1900</span>
<span class="normal">1901</span>
<span class="normal">1902</span>
<span class="normal">1903</span>
<span class="normal">1904</span>
<span class="normal">1905</span>
<span class="normal">1906</span>
<span class="normal">1907</span>
<span class="normal">1908</span>
<span class="normal">1909</span>
<span class="normal">1910</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">batch_predict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">destination_uri_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model_parameters</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">BatchPredictionJob</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Starts a batch prediction job with the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: The location of the dataset.</span>
<span class="sd">            `gs://` and `bq://` URIs are supported.</span>
<span class="sd">        destination_uri_prefix: The URI prefix for the prediction.</span>
<span class="sd">            `gs://` and `bq://` URIs are supported.</span>
<span class="sd">        model_parameters: Model-specific parameters to send to the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `BatchPredictionJob` object</span>
<span class="sd">    Raises:</span>
<span class="sd">        ValueError: When source or destination URI is not supported.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arguments</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">first_source_uri</span> <span class="o">=</span> <span class="n">dataset</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">first_source_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">uri</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;All URIs in the list must start with &#39;gs://&#39;: </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;gcs_source&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span>
    <span class="k">elif</span> <span class="n">first_source_uri</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;bq://&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only single BigQuery source can be specified: </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;bigquery_source&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported source_uri: </span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">destination_uri_prefix</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;gs://&quot;</span><span class="p">):</span>
        <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;gcs_destination_prefix&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">destination_uri_prefix</span>
    <span class="k">elif</span> <span class="n">destination_uri_prefix</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;bq://&quot;</span><span class="p">):</span>
        <span class="n">arguments</span><span class="p">[</span><span class="s2">&quot;bigquery_destination_prefix&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">destination_uri_prefix</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported destination_uri: </span><span class="si">{</span><span class="n">destination_uri_prefix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">model_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_resource_name</span>

    <span class="n">job</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">BatchPredictionJob</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">job_display_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">arguments</span><span class="p">,</span>
        <span class="n">model_parameters</span><span class="o">=</span><span class="n">model_parameters</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">job</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.tune_model" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">tune_model</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">tune_model</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n"><span title="pandas.core.frame.DataFrame">DataFrame</span></span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TuningEvaluationSpec" href="../preview/language_models/#vertexai.preview.language_models.TuningEvaluationSpec">TuningEvaluationSpec</a></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="vertexai.language_models._language_models._ACCELERATOR_TYPE_TYPE">_ACCELERATOR_TYPE_TYPE</span></span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_context_length</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models._LanguageModelTuningJob">_LanguageModelTuningJob</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Tunes a model based on training data.</p>
<p>This method launches and returns an asynchronous model tuning job.
Usage:
```
tuning_job = model.tune_model(...)
... do some other work
tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</p>
<p>Args:
    training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.
        The dataset schema is model-specific.
        See <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format">https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</a>
    train_steps: Number of training batches to tune on (batch size is 8 samples).
    learning_rate_multiplier: Learning rate multiplier to use in tuning.
    tuning_job_location: GCP location where the tuning job should be run.
    tuned_model_location: GCP location where the tuned model should be deployed.
    model_display_name: Custom display name for the tuned model.
    tuning_evaluation_spec: Specification for the model evaluation during tuning.
    accelerator_type: Type of accelerator to use. Can be "TPU" or "GPU".
    max_context_length: The max context length used for tuning.
        Can be either '8k' or '32k'</p>
<p>Returns:
    A <code class="language-python highlight"><span class="n">LanguageModelTuningJob</span></code> object that represents the tuning job.
    Calling <code class="language-python highlight"><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span></code> blocks until the tuning is complete and returns a <code class="language-python highlight"><span class="n">LanguageModel</span></code> object.</p>
<p>Raises:
    ValueError: If the "tuning_job_location" value is not supported
    ValueError: If the "tuned_model_location" value is not supported
    RuntimeError: If the model does not support tuning</p>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">tune_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">training_data</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;pandas.core.frame.DataFrame&quot;</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">train_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">learning_rate_multiplier</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_job_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuned_model_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_display_name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tuning_evaluation_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;TuningEvaluationSpec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">accelerator_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_ACCELERATOR_TYPE_TYPE</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_context_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;_LanguageModelTuningJob&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tunes a model based on training data.</span>

<span class="sd">    This method launches and returns an asynchronous model tuning job.</span>
<span class="sd">    Usage:</span>
<span class="sd">    ```</span>
<span class="sd">    tuning_job = model.tune_model(...)</span>
<span class="sd">    ... do some other work</span>
<span class="sd">    tuned_model = tuning_job.get_tuned_model()  # Blocks until tuning is complete</span>

<span class="sd">    Args:</span>
<span class="sd">        training_data: A Pandas DataFrame or a URI pointing to data in JSON lines format.</span>
<span class="sd">            The dataset schema is model-specific.</span>
<span class="sd">            See https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models#dataset_format</span>
<span class="sd">        train_steps: Number of training batches to tune on (batch size is 8 samples).</span>
<span class="sd">        learning_rate_multiplier: Learning rate multiplier to use in tuning.</span>
<span class="sd">        tuning_job_location: GCP location where the tuning job should be run.</span>
<span class="sd">        tuned_model_location: GCP location where the tuned model should be deployed.</span>
<span class="sd">        model_display_name: Custom display name for the tuned model.</span>
<span class="sd">        tuning_evaluation_spec: Specification for the model evaluation during tuning.</span>
<span class="sd">        accelerator_type: Type of accelerator to use. Can be &quot;TPU&quot; or &quot;GPU&quot;.</span>
<span class="sd">        max_context_length: The max context length used for tuning.</span>
<span class="sd">            Can be either &#39;8k&#39; or &#39;32k&#39;</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `LanguageModelTuningJob` object that represents the tuning job.</span>
<span class="sd">        Calling `job.result()` blocks until the tuning is complete and returns a `LanguageModel` object.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the &quot;tuning_job_location&quot; value is not supported</span>
<span class="sd">        ValueError: If the &quot;tuned_model_location&quot; value is not supported</span>
<span class="sd">        RuntimeError: If the model does not support tuning</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Note: Chat models do not support default_context</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">tune_model</span><span class="p">(</span>
        <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span>
        <span class="n">train_steps</span><span class="o">=</span><span class="n">train_steps</span><span class="p">,</span>
        <span class="n">learning_rate_multiplier</span><span class="o">=</span><span class="n">learning_rate_multiplier</span><span class="p">,</span>
        <span class="n">tuning_job_location</span><span class="o">=</span><span class="n">tuning_job_location</span><span class="p">,</span>
        <span class="n">tuned_model_location</span><span class="o">=</span><span class="n">tuned_model_location</span><span class="p">,</span>
        <span class="n">model_display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
        <span class="n">tuning_evaluation_spec</span><span class="o">=</span><span class="n">tuning_evaluation_spec</span><span class="p">,</span>
        <span class="n">accelerator_type</span><span class="o">=</span><span class="n">accelerator_type</span><span class="p">,</span>
        <span class="n">max_context_length</span><span class="o">=</span><span class="n">max_context_length</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.predict" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">predict</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">predict</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">int</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a></span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.Dict">Dict</span></span><span class="p">[</span><span class="n">int</span><span class="p">,</span> <span class="n">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Gets model response for a single prompt.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Question to ask the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code><span title="vertexai.language_models._language_models._TextGenerationModel._DEFAULT_MAX_OUTPUT_TOKENS">_DEFAULT_MAX_OUTPUT_TOKENS</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of response candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>grounding_source</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logprobs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Returns the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidate tokens with their log probabilities
at each generation step. The chosen tokens and their log probabilities at each step are always
returned. The chosen token may or may not be in the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidates.
The minimum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 0, which means only the chosen tokens and their log
probabilities are returned.
The maximum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 5.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>presence_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that have appeared in the generated text,
thus increasing the possibility of generating more diversed topics.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>frequency_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that repeatedly appear in the generated
text, thus decreasing the possibility of repeating the same content.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logit_bias</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mapping from token IDs (integers) to their bias values (floats).
The bias values are added to the logits before sampling.
Larger positive bias increases the probability of choosing the token.
Smaller negative bias decreases the probability of choosing the token.
Range: [-100.0, 100.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[int, float]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>seed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to
logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates
the same output with the same seed. If seed is not set, the seed used in decoder will not be
deterministic, thus the generated random noise will not be deterministic. If seed is set, the
generated random noise will be deterministic.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets model response for a single prompt.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt: Question to ask the model.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of response candidates to return.</span>
<span class="sd">        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>
<span class="sd">        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">            at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">            probabilities are returned.</span>
<span class="sd">            The maximum value for `logprobs` is 5.</span>
<span class="sd">        presence_penalty:</span>
<span class="sd">            Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">            thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        frequency_penalty:</span>
<span class="sd">            Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">            text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        logit_bias:</span>
<span class="sd">            Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">            The bias values are added to the logits before sampling.</span>
<span class="sd">            Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">            Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">            Range: [-100.0, 100.0]</span>
<span class="sd">        seed:</span>
<span class="sd">            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to</span>
<span class="sd">            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates</span>
<span class="sd">            the same output with the same seed. If seed is not set, the seed used in decoder will not be</span>
<span class="sd">            deterministic, thus the generated random noise will not be deterministic. If seed is set, the</span>
<span class="sd">            generated random noise will be deterministic.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
        <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">_parse_text_generation_model_multi_candidate_response</span><span class="p">(</span>
        <span class="n">prediction_response</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.predict_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">predict_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">predict_async</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n">int</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span>
        <span class="n"><span title="typing.Union">Union</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a></span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.Dict">Dict</span></span><span class="p">[</span><span class="n">int</span><span class="p">,</span> <span class="n">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously gets model response for a single prompt.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Question to ask the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code><span title="vertexai.language_models._language_models._TextGenerationModel._DEFAULT_MAX_OUTPUT_TOKENS">_DEFAULT_MAX_OUTPUT_TOKENS</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>candidate_count</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Number of response candidates to return.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>grounding_source</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>If specified, grounding feature will be enabled using the grounding source. Default: None.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a>, <a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a>]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logprobs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Returns the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidate tokens with their log probabilities
at each generation step. The chosen tokens and their log probabilities at each step are always
returned. The chosen token may or may not be in the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidates.
The minimum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 0, which means only the chosen tokens and their log
probabilities are returned.
The maximum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 5.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>presence_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that have appeared in the generated text,
thus increasing the possibility of generating more diversed topics.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>frequency_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that repeatedly appear in the generated
text, thus decreasing the possibility of repeating the same content.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logit_bias</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mapping from token IDs (integers) to their bias values (floats).
The bias values are added to the logits before sampling.
Larger positive bias increases the probability of choosing the token.
Smaller negative bias decreases the probability of choosing the token.
Range: [-100.0, 100.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[int, float]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>seed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to
logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates
the same output with the same seed. If seed is not set, the seed used in decoder will not be
deterministic, thus the generated random noise will not be deterministic. If seed is set, the
generated random noise will be deterministic.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">RETURNS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-returns-annotation">
                    <code><span title="vertexai.language_models._language_models.MultiCandidateTextGenerationResponse">MultiCandidateTextGenerationResponse</span></code>
                </span>
            </td>
            <td class="doc-returns-details">
              <div class="doc-md-description">
                <p>A <code class="language-python highlight"><span class="n">MultiCandidateTextGenerationResponse</span></code> object that contains the text produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span>
<span class="normal">1434</span>
<span class="normal">1435</span>
<span class="normal">1436</span>
<span class="normal">1437</span>
<span class="normal">1438</span>
<span class="normal">1439</span>
<span class="normal">1440</span>
<span class="normal">1441</span>
<span class="normal">1442</span>
<span class="normal">1443</span>
<span class="normal">1444</span>
<span class="normal">1445</span>
<span class="normal">1446</span>
<span class="normal">1447</span>
<span class="normal">1448</span>
<span class="normal">1449</span>
<span class="normal">1450</span>
<span class="normal">1451</span>
<span class="normal">1452</span>
<span class="normal">1453</span>
<span class="normal">1454</span>
<span class="normal">1455</span>
<span class="normal">1456</span>
<span class="normal">1457</span>
<span class="normal">1458</span>
<span class="normal">1459</span>
<span class="normal">1460</span>
<span class="normal">1461</span>
<span class="normal">1462</span>
<span class="normal">1463</span>
<span class="normal">1464</span>
<span class="normal">1465</span>
<span class="normal">1466</span>
<span class="normal">1467</span>
<span class="normal">1468</span>
<span class="normal">1469</span>
<span class="normal">1470</span>
<span class="normal">1471</span>
<span class="normal">1472</span>
<span class="normal">1473</span>
<span class="normal">1474</span>
<span class="normal">1475</span>
<span class="normal">1476</span>
<span class="normal">1477</span>
<span class="normal">1478</span>
<span class="normal">1479</span>
<span class="normal">1480</span>
<span class="normal">1481</span>
<span class="normal">1482</span>
<span class="normal">1483</span>
<span class="normal">1484</span>
<span class="normal">1485</span>
<span class="normal">1486</span>
<span class="normal">1487</span>
<span class="normal">1488</span>
<span class="normal">1489</span>
<span class="normal">1490</span>
<span class="normal">1491</span>
<span class="normal">1492</span>
<span class="normal">1493</span>
<span class="normal">1494</span>
<span class="normal">1495</span>
<span class="normal">1496</span>
<span class="normal">1497</span>
<span class="normal">1498</span>
<span class="normal">1499</span>
<span class="normal">1500</span>
<span class="normal">1501</span>
<span class="normal">1502</span>
<span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">predict_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">candidate_count</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">grounding_source</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">WebSearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">VertexAISearch</span><span class="p">,</span>
            <span class="n">GroundingSource</span><span class="o">.</span><span class="n">InlineContext</span><span class="p">,</span>
        <span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;MultiCandidateTextGenerationResponse&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously gets model response for a single prompt.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt: Question to ask the model.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        candidate_count: Number of response candidates to return.</span>
<span class="sd">        grounding_source: If specified, grounding feature will be enabled using the grounding source. Default: None.</span>
<span class="sd">        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">            at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">            probabilities are returned.</span>
<span class="sd">            The maximum value for `logprobs` is 5.</span>
<span class="sd">        presence_penalty:</span>
<span class="sd">            Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">            thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        frequency_penalty:</span>
<span class="sd">            Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">            text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        logit_bias:</span>
<span class="sd">            Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">            The bias values are added to the logits before sampling.</span>
<span class="sd">            Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">            Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">            Range: [-100.0, 100.0]</span>
<span class="sd">        seed:</span>
<span class="sd">            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to</span>
<span class="sd">            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates</span>
<span class="sd">            the same output with the same seed. If seed is not set, the seed used in decoder will not be</span>
<span class="sd">            deterministic, thus the generated random noise will not be deterministic. If seed is set, the</span>
<span class="sd">            generated random noise will be deterministic.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">candidate_count</span><span class="o">=</span><span class="n">candidate_count</span><span class="p">,</span>
        <span class="n">grounding_source</span><span class="o">=</span><span class="n">grounding_source</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">predict_async</span><span class="p">(</span>
        <span class="n">instances</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">],</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">_parse_text_generation_model_multi_candidate_response</span><span class="p">(</span>
        <span class="n">prediction_response</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.predict_streaming" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">predict_streaming</span>


</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">predict_streaming</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.Dict">Dict</span></span><span class="p">[</span><span class="n">int</span><span class="p">,</span> <span class="n">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.Iterator">Iterator</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Gets a streaming model response for a single prompt.</p>
<p>The result is a stream (generator) of partial responses.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Question to ask the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code><span title="vertexai.language_models._language_models._TextGenerationModel._DEFAULT_MAX_OUTPUT_TOKENS">_DEFAULT_MAX_OUTPUT_TOKENS</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logprobs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Returns the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidate tokens with their log probabilities
at each generation step. The chosen tokens and their log probabilities at each step are always
returned. The chosen token may or may not be in the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidates.
The minimum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 0, which means only the chosen tokens and their log
probabilities are returned.
The maximum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 5.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>presence_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that have appeared in the generated text,
thus increasing the possibility of generating more diversed topics.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>frequency_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that repeatedly appear in the generated
text, thus decreasing the possibility of repeating the same content.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logit_bias</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mapping from token IDs (integers) to their bias values (floats).
The bias values are added to the logits before sampling.
Larger positive bias increases the probability of choosing the token.
Smaller negative bias decreases the probability of choosing the token.
Range: [-100.0, 100.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[int, float]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>seed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to
logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates
the same output with the same seed. If seed is not set, the seed used in decoder will not be
deterministic, thus the generated random noise will not be deterministic. If seed is set, the
generated random noise will be deterministic.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">YIELDS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span>
<span class="normal">1598</span>
<span class="normal">1599</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict_streaming</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets a streaming model response for a single prompt.</span>

<span class="sd">    The result is a stream (generator) of partial responses.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt: Question to ask the model.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">            at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">            probabilities are returned.</span>
<span class="sd">            The maximum value for `logprobs` is 5.</span>
<span class="sd">        presence_penalty:</span>
<span class="sd">            Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">            thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        frequency_penalty:</span>
<span class="sd">            Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">            text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        logit_bias:</span>
<span class="sd">            Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">            The bias values are added to the logits before sampling.</span>
<span class="sd">            Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">            Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">            Range: [-100.0, 100.0]</span>
<span class="sd">        seed:</span>
<span class="sd">            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to</span>
<span class="sd">            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates</span>
<span class="sd">            the same output with the same seed. If seed is not set, the seed used in decoder will not be</span>
<span class="sd">            deterministic, thus the generated random noise will not be deterministic. If seed is set, the</span>
<span class="sd">            generated random noise will be deterministic.</span>

<span class="sd">    Yields:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_service_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_client</span>
    <span class="k">for</span> <span class="p">(</span>
        <span class="n">prediction_dict</span>
    <span class="p">)</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict</span><span class="p">(</span>
        <span class="n">prediction_service_client</span><span class="o">=</span><span class="n">prediction_service_client</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
        <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">prediction_obj</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
            <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
            <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">yield</span> <span class="n">_parse_text_generation_model_response</span><span class="p">(</span><span class="n">prediction_obj</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="vertexai.language_models.TextGenerationModel.predict_streaming_async" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">predict_streaming_async</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">predict_streaming_async</span><span class="p">(</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.List">List</span></span><span class="p">[</span><span class="n">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="typing.Dict">Dict</span></span><span class="p">[</span><span class="n">int</span><span class="p">,</span> <span class="n">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="typing.AsyncIterator">AsyncIterator</span></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Asynchronously gets a streaming model response for a single prompt.</p>
<p>The result is a stream (generator) of partial responses.</p>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">PARAMETER</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>prompt</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Question to ask the model.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>str</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_output_tokens</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Max length of the output text in tokens. Range: [1, 1024].</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code>int</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code><span title="vertexai.language_models._language_models._TextGenerationModel._DEFAULT_MAX_OUTPUT_TOKENS">_DEFAULT_MAX_OUTPUT_TOKENS</span></code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>temperature</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Controls the randomness of predictions. Range: [0, 1]. Default: 0.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_k</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>top_p</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>stop_sequences</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Customized stop sequences to stop the decoding process.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[str]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logprobs</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Returns the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidate tokens with their log probabilities
at each generation step. The chosen tokens and their log probabilities at each step are always
returned. The chosen token may or may not be in the top <code class="language-python highlight"><span class="n">logprobs</span></code> most likely candidates.
The minimum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 0, which means only the chosen tokens and their log
probabilities are returned.
The maximum value for <code class="language-python highlight"><span class="n">logprobs</span></code> is 5.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>presence_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that have appeared in the generated text,
thus increasing the possibility of generating more diversed topics.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>frequency_penalty</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Positive values penalize tokens that repeatedly appear in the generated
text, thus decreasing the possibility of repeating the same content.
Range: [-2.0, 2.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[float]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>logit_bias</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Mapping from token IDs (integers) to their bias values (floats).
The bias values are added to the logits before sampling.
Larger positive bias increases the probability of choosing the token.
Smaller negative bias decreases the probability of choosing the token.
Range: [-100.0, 100.0]</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[int, float]]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>seed</code></td>
            <td class="doc-param-details">
              <div class="doc-md-description">
                <p>Decoder generates random noise with a pseudo random number generator, temperature * noise is added to
logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates
the same output with the same seed. If seed is not set, the seed used in decoder will not be
deterministic, thus the generated random noise will not be deterministic. If seed is set, the
generated random noise will be deterministic.</p>
              </div>
              <p>
                  <span class="doc-param-annotation">
                    <b>TYPE:</b>
                      <code><span title="typing.Optional">Optional</span>[int]</code>
                  </span>
                  <span class="doc-param-default">
                    <b>DEFAULT:</b>
                      <code>None</code>
                  </span>
              </p>
            </td>
          </tr>
      </tbody>
    </table>


<table>
      <thead>
        <tr>
          <th><span class="doc-section-title">YIELDS</span></th>
          <th><span>DESCRIPTION</span></th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>A stream of <code class="language-python highlight"><span class="n">TextGenerationResponse</span></code> objects that contain partial</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <span class="doc-yields-annotation">
                    <code><span title="typing.AsyncIterator">AsyncIterator</span>[<a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.TextGenerationResponse" href="#vertexai.language_models.TextGenerationResponse">TextGenerationResponse</a>]</code>
                </span>
            </td>
            <td class="doc-yields-details">
              <div class="doc-md-description">
                <p>responses produced by the model.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>vertexai\language_models\_language_models.py</code></summary>
              <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span>
<span class="normal">1606</span>
<span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span>
<span class="normal">1638</span>
<span class="normal">1639</span>
<span class="normal">1640</span>
<span class="normal">1641</span>
<span class="normal">1642</span>
<span class="normal">1643</span>
<span class="normal">1644</span>
<span class="normal">1645</span>
<span class="normal">1646</span>
<span class="normal">1647</span>
<span class="normal">1648</span>
<span class="normal">1649</span>
<span class="normal">1650</span>
<span class="normal">1651</span>
<span class="normal">1652</span>
<span class="normal">1653</span>
<span class="normal">1654</span>
<span class="normal">1655</span>
<span class="normal">1656</span>
<span class="normal">1657</span>
<span class="normal">1658</span>
<span class="normal">1659</span>
<span class="normal">1660</span>
<span class="normal">1661</span>
<span class="normal">1662</span>
<span class="normal">1663</span>
<span class="normal">1664</span>
<span class="normal">1665</span>
<span class="normal">1666</span>
<span class="normal">1667</span>
<span class="normal">1668</span>
<span class="normal">1669</span>
<span class="normal">1670</span>
<span class="normal">1671</span>
<span class="normal">1672</span>
<span class="normal">1673</span>
<span class="normal">1674</span>
<span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">predict_streaming_async</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">max_output_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_DEFAULT_MAX_OUTPUT_TOKENS</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncIterator</span><span class="p">[</span><span class="n">TextGenerationResponse</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Asynchronously gets a streaming model response for a single prompt.</span>

<span class="sd">    The result is a stream (generator) of partial responses.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt: Question to ask the model.</span>
<span class="sd">        max_output_tokens: Max length of the output text in tokens. Range: [1, 1024].</span>
<span class="sd">        temperature: Controls the randomness of predictions. Range: [0, 1]. Default: 0.</span>
<span class="sd">        top_k: The number of highest probability vocabulary tokens to keep for top-k-filtering. Range: [1, 40]. Default: 40.</span>
<span class="sd">        top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Range: [0, 1]. Default: 0.95.</span>
<span class="sd">        stop_sequences: Customized stop sequences to stop the decoding process.</span>
<span class="sd">        logprobs: Returns the top `logprobs` most likely candidate tokens with their log probabilities</span>
<span class="sd">            at each generation step. The chosen tokens and their log probabilities at each step are always</span>
<span class="sd">            returned. The chosen token may or may not be in the top `logprobs` most likely candidates.</span>
<span class="sd">            The minimum value for `logprobs` is 0, which means only the chosen tokens and their log</span>
<span class="sd">            probabilities are returned.</span>
<span class="sd">            The maximum value for `logprobs` is 5.</span>
<span class="sd">        presence_penalty:</span>
<span class="sd">            Positive values penalize tokens that have appeared in the generated text,</span>
<span class="sd">            thus increasing the possibility of generating more diversed topics.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        frequency_penalty:</span>
<span class="sd">            Positive values penalize tokens that repeatedly appear in the generated</span>
<span class="sd">            text, thus decreasing the possibility of repeating the same content.</span>
<span class="sd">            Range: [-2.0, 2.0]</span>
<span class="sd">        logit_bias:</span>
<span class="sd">            Mapping from token IDs (integers) to their bias values (floats).</span>
<span class="sd">            The bias values are added to the logits before sampling.</span>
<span class="sd">            Larger positive bias increases the probability of choosing the token.</span>
<span class="sd">            Smaller negative bias decreases the probability of choosing the token.</span>
<span class="sd">            Range: [-100.0, 100.0]</span>
<span class="sd">        seed:</span>
<span class="sd">            Decoder generates random noise with a pseudo random number generator, temperature * noise is added to</span>
<span class="sd">            logits before sampling. The pseudo random number generator (prng) takes a seed as input, it generates</span>
<span class="sd">            the same output with the same seed. If seed is not set, the seed used in decoder will not be</span>
<span class="sd">            deterministic, thus the generated random noise will not be deterministic. If seed is set, the</span>
<span class="sd">            generated random noise will be deterministic.</span>

<span class="sd">    Yields:</span>
<span class="sd">        A stream of `TextGenerationResponse` objects that contain partial</span>
<span class="sd">        responses produced by the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction_request</span> <span class="o">=</span> <span class="n">_create_text_generation_prediction_request</span><span class="p">(</span>
        <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
        <span class="n">max_output_tokens</span><span class="o">=</span><span class="n">max_output_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stop_sequences</span><span class="o">=</span><span class="n">stop_sequences</span><span class="p">,</span>
        <span class="n">logprobs</span><span class="o">=</span><span class="n">logprobs</span><span class="p">,</span>
        <span class="n">presence_penalty</span><span class="o">=</span><span class="n">presence_penalty</span><span class="p">,</span>
        <span class="n">frequency_penalty</span><span class="o">=</span><span class="n">frequency_penalty</span><span class="p">,</span>
        <span class="n">logit_bias</span><span class="o">=</span><span class="n">logit_bias</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">prediction_service_async_client</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_endpoint</span><span class="o">.</span><span class="n">_prediction_async_client</span>
    <span class="k">async</span> <span class="k">for</span> <span class="n">prediction_dict</span> <span class="ow">in</span> <span class="n">_streaming_prediction</span><span class="o">.</span><span class="n">predict_stream_of_dicts_from_single_dict_async</span><span class="p">(</span>
        <span class="n">prediction_service_async_client</span><span class="o">=</span><span class="n">prediction_service_async_client</span><span class="p">,</span>
        <span class="n">endpoint_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_endpoint_name</span><span class="p">,</span>
        <span class="n">instance</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">instance</span><span class="p">,</span>
        <span class="n">parameters</span><span class="o">=</span><span class="n">prediction_request</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">prediction_obj</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Prediction</span><span class="p">(</span>
            <span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">prediction_dict</span><span class="p">],</span>
            <span class="n">deployed_model_id</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">yield</span> <span class="n">_parse_text_generation_model_response</span><span class="p">(</span><span class="n">prediction_obj</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.TextGenerationResponse" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">TextGenerationResponse</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">TextGenerationResponse</span><span class="p">(</span>
    <span class="vm">__module__</span><span class="o">=</span><span class="s2">&quot;vertexai.language_models&quot;</span><span class="p">,</span>
    <span class="n">text</span><span class="p">:</span> <span class="n">str</span><span class="p">,</span>
    <span class="n">_prediction_response</span><span class="p">:</span> <span class="n"><span title="typing.Any">Any</span></span><span class="p">,</span>
    <span class="n">is_blocked</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">errors</span><span class="p">:</span> <span class="n"><span title="typing.Tuple">Tuple</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(),</span>
    <span class="n">safety_attributes</span><span class="p">:</span> <span class="n"><span title="typing.Dict">Dict</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(),</span>
    <span class="n">grounding_metadata</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="vertexai.language_models._language_models.GroundingMetadata">GroundingMetadata</span></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">


      <p>TextGenerationResponse represents a response of a language model.
Attributes:
    text: The generated text
    is_blocked: Whether the the request was blocked.
    errors: The error codes indicate why the response was blocked.
        Learn more information about safety errors here:
        this documentation <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors">https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_errors</a>
    safety_attributes: Scores for safety attributes.
        Learn more about the safety attributes here:
        <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions">https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</a>
    grounding_metadata: Metadata for grounding.</p>




  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextGenerationResponse.text" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">text</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">text</span><span class="p">:</span> <span class="n">str</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextGenerationResponse.is_blocked" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">is_blocked</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">is_blocked</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextGenerationResponse.errors" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">errors</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">errors</span><span class="p">:</span> <span class="n"><span title="typing.Tuple">Tuple</span></span><span class="p">[</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">tuple</span><span class="p">()</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextGenerationResponse.safety_attributes" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">safety_attributes</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">safety_attributes</span><span class="p">:</span> <span class="n"><span title="typing.Dict">Dict</span></span><span class="p">[</span><span class="n">str</span><span class="p">,</span> <span class="n">float</span><span class="p">]</span> <span class="o">=</span> <span class="n"><span title="dataclasses.field">field</span></span><span class="p">(</span>
    <span class="n">default_factory</span><span class="o">=</span><span class="n">dict</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextGenerationResponse.grounding_metadata" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">grounding_metadata</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">grounding_metadata</span><span class="p">:</span> <span class="n"><span title="typing.Optional">Optional</span></span><span class="p">[</span><span class="n"><span title="vertexai.language_models._language_models.GroundingMetadata">GroundingMetadata</span></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.TextGenerationResponse.raw_prediction_response" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">raw_prediction_response</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">raw_prediction_response</span><span class="p">:</span> <span class="n"><span title="google.cloud.aiplatform.models.Prediction">Prediction</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

      <p>Raw prediction response.</p>
    </div>

</div>





  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="vertexai.language_models.GroundingSource" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">GroundingSource</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="nf">GroundingSource</span><span class="p">(</span>
    <span class="n">WebSearch</span><span class="o">=</span><span class="n">WebSearch</span><span class="p">,</span>
    <span class="n">VertexAISearch</span><span class="o">=</span><span class="n">VertexAISearch</span><span class="p">,</span>
    <span class="n">InlineContext</span><span class="o">=</span><span class="n">InlineContext</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">





  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.GroundingSource.WebSearch" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">WebSearch</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">WebSearch</span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.WebSearch" href="#vertexai.language_models.GroundingSource.WebSearch">WebSearch</a></span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.GroundingSource.VertexAISearch" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">VertexAISearch</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">VertexAISearch</span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.VertexAISearch" href="#vertexai.language_models.GroundingSource.VertexAISearch">VertexAISearch</a></span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h4 id="vertexai.language_models.GroundingSource.InlineContext" class="doc doc-heading">
            <span class="doc doc-object-name doc-attribute-name">InlineContext</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-class-attribute"><code>class-attribute</code></small>
      <small class="doc doc-label doc-label-instance-attribute"><code>instance-attribute</code></small>
  </span>

</h4>
<div class="language-python doc-signature highlight"><pre><span></span><code><span class="n">InlineContext</span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="vertexai.language_models._language_models.GroundingSource.InlineContext" href="#vertexai.language_models.GroundingSource.InlineContext">InlineContext</a></span>
</code></pre></div>

    <div class="doc doc-contents ">
    </div>

</div>





  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.tabs"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.081f42fc.min.js"></script>
      
    
  </body>
</html>